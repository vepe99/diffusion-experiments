{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009b6adf",
   "metadata": {},
   "source": "# Visualising Score & Schedules of the Diffusion Model\n"
  },
  {
   "cell_type": "code",
   "id": "d5f88a59",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "from bayesflow.utils import optimal_transport\n",
    "\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "else:\n",
    "    print(f\"Using '{os.environ['KERAS_BACKEND']}' backend\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "import timeit\n",
    "\n",
    "import keras\n",
    "import bayesflow as bf\n",
    "from bayesflow.networks.diffusion_model import EDMNoiseSchedule\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import norm\n",
    "sech = lambda x: 1 / np.cosh(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simulator<a class=\"anchor\" id=\"simulator\"></a>",
   "id": "c63b26ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def theta_prior():\n",
    "    theta = np.random.uniform(-1, 1, 2)\n",
    "    return dict(theta=theta)\n",
    "\n",
    "def forward_model(theta):\n",
    "    alpha = np.random.uniform(-np.pi / 2, np.pi / 2)\n",
    "    r = np.random.normal(0.1, 0.01)\n",
    "    x1 = -np.abs(theta[0] + theta[1]) / np.sqrt(2) + r * np.cos(alpha) + 0.25\n",
    "    x2 = (-theta[0] + theta[1]) / np.sqrt(2) + r * np.sin(alpha)\n",
    "    return dict(x=np.array([x1, x2]))"
   ],
   "id": "f761b142a0e1da66",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4b89c861527c13b8",
   "metadata": {},
   "source": [
    "simulator = bf.make_simulator([theta_prior, forward_model])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c9c2dc70f53d103",
   "metadata": {},
   "source": [
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .to_array()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .rename(\"theta\", \"inference_variables\")\n",
    "    .rename(\"x\", \"inference_conditions\")\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "35291b6d847c2fb1"
  },
  {
   "cell_type": "code",
   "id": "39cb5a1c9824246f",
   "metadata": {},
   "source": [
    "num_training_batches = 256\n",
    "num_validation_sets = 300\n",
    "batch_size = 128\n",
    "epochs = 1000"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9dee7252ef99affa",
   "metadata": {},
   "source": [
    "training_data = simulator.sample(num_training_batches * batch_size)\n",
    "validation_data = simulator.sample(num_validation_sets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96ca6ffa",
   "metadata": {},
   "source": [
    "workflows = {}\n",
    "\n",
    "for noise_schedule in ['cosine', 'edm', 'edm_ve']:\n",
    "    diffusion_model_workflow = bf.BasicWorkflow(\n",
    "        simulator=simulator,\n",
    "        adapter=adapter,\n",
    "        inference_network = bf.networks.DiffusionModel(\n",
    "            noise_schedule=noise_schedule if noise_schedule != 'edm_ve' else EDMNoiseSchedule(variance_type='exploding'),\n",
    "            prediction_type='velocity' if noise_schedule == 'cosine' else 'F',\n",
    "        ),\n",
    "    )\n",
    "    workflows['diffusion_model_'+noise_schedule] = diffusion_model_workflow\n",
    "\n",
    "for ot in [False, True]:\n",
    "    name = \"flow_matching_ot\" if ot else \"flow_matching\"\n",
    "    for time_sampling_alpha in [0, -0.6]:\n",
    "        if time_sampling_alpha != 0:\n",
    "            continue\n",
    "            name += '_pl'\n",
    "        flow_matching_workflow = bf.BasicWorkflow(\n",
    "            simulator=simulator,\n",
    "            adapter=adapter,\n",
    "            inference_network = bf.networks.FlowMatching(\n",
    "                use_optimal_transport=ot,\n",
    "                time_power_law_alpha=-0.6,\n",
    "                optimal_transport_kwargs={'conditional_ot_ratio': 0.5}  # no conditional OT for straighter paths\n",
    "            ),\n",
    "        )\n",
    "        workflows[name] = flow_matching_workflow\n",
    "\n",
    "stable_consistency_workflow = bf.BasicWorkflow(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    inference_network=bf.experimental.StableConsistencyModel(),\n",
    ")\n",
    "workflows['stable_consistency'] = stable_consistency_workflow"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f496bda",
   "metadata": {},
   "source": [
    "for name, workflow in workflows.items():\n",
    "    if os.path.exists('models/two_moons_'+name+'.keras'):\n",
    "        print(f\"Loading {name} from disk\")\n",
    "        workflow.fit_offline(  # otherwise some building steps are not executed\n",
    "            training_data,\n",
    "            epochs=1,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=validation_data,\n",
    "            verbose=0\n",
    "        )\n",
    "        workflow.approximator = keras.saving.load_model('models/two_moons_'+name+'.keras')\n",
    "        continue\n",
    "\n",
    "    print(f\"Training {name}...\")\n",
    "    start = timeit.default_timer()\n",
    "    workflow.fit_offline(\n",
    "        training_data,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=validation_data,\n",
    "        verbose=2\n",
    "    )\n",
    "    workflow.approximator.save('models/two_moons_'+name+'.keras')\n",
    "    end = timeit.default_timer()\n",
    "    print(f\"Trained {name} in {end - start:.2f} seconds\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2fbe42f-b6e8-45f3-a53a-4015fb84e78f",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(ncols=len(workflows), figsize=(14, 5), layout='constrained', sharey=True, sharex=True)\n",
    "if len(workflows) == 1:\n",
    "    ax = [ax]\n",
    "for i, (name, workflow) in enumerate(workflows.items()):\n",
    "    samples = workflow.sample(num_samples=1000, conditions={\"x\":np.array([[0.0, 0.0]], dtype=np.float32)})\n",
    "    ax[i].scatter(samples[\"theta\"][0, :, 0], samples[\"theta\"][0, :, 1], alpha=0.75, s=0.5, label=\"ODE\")\n",
    "\n",
    "    if name.split('_')[0] == 'diffusion':\n",
    "        samples = workflow.sample(num_samples=1000, method=\"euler_maruyama\",\n",
    "                                  conditions={\"x\":np.array([[0.0, 0.0]], dtype=np.float32)})\n",
    "        ax[i].scatter(samples[\"theta\"][0, :, 0], samples[\"theta\"][0, :, 1], alpha=0.75, s=0.5, label=\"Euler Maruyama\")\n",
    "        ax[i].legend(loc=\"upper right\")\n",
    "\n",
    "    ax[i].set_title(name)\n",
    "    ax[i].set_aspect('equal')\n",
    "ax[-1].set_xlim([-0.5, 0.5])\n",
    "ax[-1].set_ylim([-0.5, 0.5])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualizing the Trajectory",
   "id": "6ba22a899ba3a5e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def euler_backward_like(workflow, conditions, x0=None, steps=200, stochastic_solver=False):\n",
    "    num_samples = 1  # not sure if the code would work with more, but not needed\n",
    "    # conditions must always have shape (batch_size, ..., dims)\n",
    "    conditions_prep = workflow.approximator._prepare_data(conditions)['inference_conditions']\n",
    "    batch_size = keras.ops.shape(conditions_prep)[0]\n",
    "    inference_conditions = keras.ops.expand_dims(conditions_prep, axis=1)\n",
    "    inference_conditions = keras.ops.broadcast_to(\n",
    "                    inference_conditions, (batch_size, num_samples, *keras.ops.shape(inference_conditions)[2:])\n",
    "    )\n",
    "\n",
    "    if workflow.approximator.inference_network.name.split('_')[0] == 'flow':\n",
    "        t_start, t_end = 0.0, 1.0\n",
    "    elif workflow.approximator.inference_network.name.split('_')[0] == 'diffusion':\n",
    "        t_start, t_end = 1.0, 0.0\n",
    "    else:\n",
    "        raise ValueError(\"Unknown inference network type\")\n",
    "\n",
    "    dt = (t_end - t_start) / steps  # negative if integrating toward 0\n",
    "    if x0 is not None:\n",
    "        x = x0\n",
    "    else:\n",
    "        # sample from the base distribution\n",
    "        x = workflow.approximator.inference_network.base_distribution.sample((1, num_samples))\n",
    "    t = float(t_start)\n",
    "\n",
    "    traj = []\n",
    "    vels = []\n",
    "    for k in range(steps):\n",
    "        traj.append(keras.ops.convert_to_numpy(x))\n",
    "        if workflow.inference_network.name.split('_')[0] != 'diffusion':\n",
    "            v_curr = workflow.approximator.inference_network.velocity(\n",
    "                xz=x, time=t, conditions=inference_conditions, training=False\n",
    "            )\n",
    "        else:\n",
    "            # for diffusion models, we can use a stochastic solver\n",
    "            v_curr = workflow.approximator.inference_network.velocity(\n",
    "                xz=x, time=t, conditions=inference_conditions, stochastic_solver=stochastic_solver, training=False\n",
    "            )\n",
    "            if stochastic_solver:\n",
    "                diff_curr = workflow.approximator.inference_network.diffusion_term(\n",
    "                    xz=x, time=t, training=False\n",
    "                )\n",
    "                noise = keras.random.normal(keras.ops.shape(x), dtype=keras.ops.dtype(x)) * np.sqrt(np.abs(dt))\n",
    "                x = x + diff_curr * noise\n",
    "\n",
    "        x = x + dt * v_curr\n",
    "        t = t + dt\n",
    "        vels.append(keras.ops.convert_to_numpy(v_curr))\n",
    "\n",
    "    traj = np.stack(traj, axis=0)      # shape [steps+1, batch, num_samples, dims]\n",
    "    vels = np.stack(vels, axis=0)      # shape [steps,   batch, num_samples, dims]\n",
    "    times = np.linspace(t_start, t_end, steps+1, dtype=np.float32)\n",
    "\n",
    "    traj =  workflow.approximator.standardize_layers[\"inference_variables\"](traj, forward=False)\n",
    "    return traj, vels, times"
   ],
   "id": "5252221790198aee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x0 = workflows[list(workflows.keys())[0]].approximator.inference_network.base_distribution.sample((1, 1)) * 0\n",
    "conditions = {\"x\":np.array([[0.0, 0.0]], dtype=np.float32)}\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=len(workflows), figsize=(3*len(workflows), 4),\n",
    "                       layout='constrained', sharey='row', sharex='row')\n",
    "for i, (name, workflow) in enumerate(workflows.items()):\n",
    "    if name != 'stable_consistency':\n",
    "        traj, vels, times = euler_backward_like(\n",
    "            workflow, x0=x0, conditions=conditions, steps=200, stochastic_solver=False\n",
    "        )\n",
    "        traj, vels = traj[:, 0, 0], vels[:, 0, 0]  # take first batch item\n",
    "        vel_norm = np.linalg.norm(vels, axis=-1)\n",
    "        ax[0, i].plot(traj[:, 0], traj[:, 1], color='black', label=\"Trajectory\" if i == 0 else None)\n",
    "        ax[0, i].scatter(traj[0, 0], traj[0, 1], s=30, marker='o', label='start' if i == 0 else None, color='black') # start\n",
    "        #ax[0, i].scatter(traj[-1, 0], traj[-1, 1], s=30, marker='x', label='end' if i == 0 else None, color='black') # end\n",
    "        ax[1, i].plot(times[:-1], vel_norm)\n",
    "        #ax[1, i].set_title(\"Velocity norm over time\")\n",
    "        ax[1, i].set_xlabel(\"time\")\n",
    "\n",
    "    samples = workflow.sample(num_samples=1000, conditions=conditions)\n",
    "    ax[0, i].scatter(samples[\"theta\"][0, :, 0], samples[\"theta\"][0, :, 1], alpha=0.75, s=0.75,\n",
    "                     color='darkviolet', label=\"Posterior Samples\" if i == 0 else None)\n",
    "\n",
    "\n",
    "\n",
    "    ax[0, i].set_title(name)\n",
    "    ax[0, i].set_aspect('equal')\n",
    "    ax[0, i].set_xlabel(\"x\")\n",
    "fig.legend(bbox_to_anchor=(1.1, 0.85))\n",
    "ax[0, 0].set_ylabel(\"y\")\n",
    "ax[0, -1].set_xlim([-0.5, 0.5])\n",
    "ax[0, -1].set_ylim([-0.5, 0.5])\n",
    "ax[1, 0].set_ylabel(\"||v||\")\n",
    "plt.show()"
   ],
   "id": "9097a167e6044ebb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_trajectories = 15\n",
    "n_sde_averages = 1\n",
    "colors = [\n",
    "    \"#E7298A\",  # magenta pink\n",
    "    \"#7570B3\",  # muted purple\n",
    "    \"#1B9E77\",  # teal green\n",
    "    \"#D95F02\",  # deep orange\n",
    "    '#E6AB02'\n",
    "]"
   ],
   "id": "e49dee10b80ece2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.ndimage import gaussian_filter, gaussian_filter1d\n",
    "from scipy.stats import qmc\n",
    "\n",
    "def title_and_color(name, is_sde, colors):\n",
    "    if name.split('_')[0] == 'diffusion':\n",
    "        kind = name.split('diffusion_model_')[1]\n",
    "        if kind == 'edm':\n",
    "            title = 'EDM VP'\n",
    "            _color = colors[0]\n",
    "        elif kind == 'edm_ve':\n",
    "            title = 'EDM VE'\n",
    "            _color = colors[0]\n",
    "        else:\n",
    "            title = kind.title()\n",
    "            _color = colors[1]\n",
    "    else:\n",
    "        if name == 'flow_matching_ot':\n",
    "            title = 'Flow Matching OT'\n",
    "            _color = colors[2]\n",
    "        elif name == 'stable_consistency':\n",
    "            title = 'Continous Consistency'\n",
    "            _color = colors[4]\n",
    "        else:\n",
    "            title = 'Flow Matching'\n",
    "            _color = colors[2]\n",
    "    if is_sde:\n",
    "        title = f\"{title} (SDE)\"\n",
    "    return title, _color\n",
    "\n",
    "\n",
    "def plot_density(ax, samples, xlim, ylim, bins=250, smooth_sigma=1):\n",
    "    if samples is None or len(samples) == 0:\n",
    "        return\n",
    "    H, xedges, yedges = np.histogram2d(\n",
    "        samples[:, 0], samples[:, 1],\n",
    "        bins=bins, range=[xlim, ylim]\n",
    "    )\n",
    "     # smooth counts\n",
    "    H = gaussian_filter(H, sigma=smooth_sigma)\n",
    "    H = np.rot90(H)\n",
    "    H = np.flipud(H)\n",
    "    ax.imshow(\n",
    "        H,\n",
    "        extent=[xlim[0], xlim[1], ylim[0], ylim[1]],\n",
    "        origin=\"lower\",\n",
    "        aspect=\"equal\",\n",
    "        interpolation=\"nearest\",\n",
    "        cmap='Reds',\n",
    "        zorder=0,\n",
    "        vmax=0.1 * np.max(H)\n",
    "    )\n",
    "\n",
    "def moving_average(x, sigma):\n",
    "    \"\"\"\n",
    "    Gaussian smoothing - weights decay smoothly.\n",
    "    sigma controls smoothness (larger = more smoothing).\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        return gaussian_filter1d(x, sigma, mode='nearest')\n",
    "    elif x.ndim == 2:\n",
    "        y = np.zeros_like(x)\n",
    "        for dim in range(x.shape[1]):\n",
    "            y[:, dim] = gaussian_filter1d(x[:, dim], sigma, mode='nearest')\n",
    "        return y"
   ],
   "id": "2fee64998aaf66fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# initial points\n",
    "sampler = qmc.LatinHypercube(d=2)\n",
    "x0 = sampler.random(n=n_trajectories).reshape(n_trajectories, 1, 2).astype(np.float32)\n",
    "x0 = (x0 - 0.5) * 2.0\n",
    "\n",
    "# storage\n",
    "sim_det = {i: [] for i in range(len(workflows))}                 # per workflow index: list of traj arrays for all j\n",
    "vel_det = {i: [] for i in range(len(workflows))}\n",
    "post_samples_det = {i: None for i in range(len(workflows))}      # one sample cloud per workflow\n",
    "\n",
    "diffusion_idxs = []\n",
    "for idx, (name, _) in enumerate(workflows.items()):\n",
    "    if name.split('_')[0] == 'diffusion':\n",
    "        diffusion_idxs.append(idx)\n",
    "\n",
    "n_trajectories_sde = n_trajectories #int(n_trajectories * 0.25)\n",
    "\n",
    "sim_sde = {k: [] for k in diffusion_idxs}                        # per diffusion workflow index: list of traj arrays for j in SDE subset\n",
    "post_samples_sde = {k: None for k in diffusion_idxs}\n",
    "\n",
    "# run deterministic trajectories and collect samples\n",
    "for j in tqdm(range(n_trajectories)):\n",
    "    x0_i = x0[j][None]\n",
    "    for i, (name, workflow) in enumerate(workflows.items()):\n",
    "        if name != 'stable_consistency':\n",
    "            traj, vels, times = euler_backward_like(\n",
    "                workflow, x0=x0_i, conditions=conditions, steps=200, stochastic_solver=False\n",
    "            )\n",
    "            traj = traj[:, 0, 0]\n",
    "            vels = vels[:, 0, 0]\n",
    "            sim_det[i].append((traj, vels))\n",
    "        if j == (n_trajectories - 1):\n",
    "            samples = workflow.sample(num_samples=1000, conditions=conditions)\n",
    "            post_samples_det[i] = samples[\"theta\"][0]\n",
    "\n",
    "# run SDE trajectories and collect samples for diffusion workflows\n",
    "for j in tqdm(range(n_trajectories_sde)):\n",
    "    x0_i = x0[j][None]\n",
    "    for i, (name, workflow) in enumerate(workflows.items()):\n",
    "        if i not in diffusion_idxs:\n",
    "            continue\n",
    "        traj_average = []\n",
    "        for k in range(n_sde_averages):\n",
    "            traj, vels, times = euler_backward_like(\n",
    "                workflow, x0=x0_i, conditions=conditions, steps=200, stochastic_solver=True\n",
    "            )\n",
    "            traj = traj[:, 0, 0]\n",
    "            traj_average.append(traj)\n",
    "        traj_average = np.mean(np.stack(traj_average, axis=0), axis=0)\n",
    "        sim_sde[i].append(traj_average)\n",
    "        if j == (n_trajectories_sde - 1):\n",
    "            samples = workflow.sample(num_samples=1000, conditions=conditions, method=\"euler_maruyama\")\n",
    "            post_samples_sde[i] = samples[\"theta\"][0]"
   ],
   "id": "d77b6c8c40dd710",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plotting\n",
    "ncols = len(workflows) + len(diffusion_idxs) - 1  # consistency model\n",
    "fig, ax = plt.subplots(ncols=ncols, figsize=(10, 3), layout='constrained', sharey='row', sharex='row')\n",
    "\n",
    "# map sde column positions\n",
    "sde_col_start = len(workflows)\n",
    "sde_col_map = {wf_i: sde_col_start + k for k, wf_i in enumerate(diffusion_idxs)}\n",
    "init_marker = '*', 10\n",
    "end_marker = 'o', 10\n",
    "\n",
    "xlim = [-0.6, 0.6]\n",
    "ylim = [-0.6, 0.6]\n",
    "\n",
    "# deterministic panels\n",
    "i = 0\n",
    "for _i, (name, workflow) in enumerate(workflows.items()):\n",
    "    if name == 'stable_consistency':\n",
    "        continue\n",
    "    title, col = title_and_color(name, is_sde=False, colors=colors)\n",
    "    ax[i].set_title(title)\n",
    "    for j, (traj, vels) in enumerate(sim_det[i]):\n",
    "        ax[i].plot(traj[:, 0], traj[:, 1], color=col, alpha=0.75, linewidth=0.75)\n",
    "        ax[i].scatter(traj[0, 0], traj[0, 1], s=init_marker[1], marker=init_marker[0], color=col)\n",
    "        ax[i].scatter(traj[-1, 0], traj[-1, 1], s=end_marker[1], marker=end_marker[0], color=col, alpha=0.25)\n",
    "\n",
    "        # add small arrows along trajectory\n",
    "        mid = len(traj) // 5 * 3\n",
    "        ax[i].annotate(\n",
    "            '',\n",
    "            xy=(traj[mid + 1, 0], traj[mid + 1, 1]),\n",
    "            xytext=(traj[mid, 0], traj[mid, 1]),\n",
    "            arrowprops=dict(arrowstyle='->', color=col, lw=1),\n",
    "        )\n",
    "\n",
    "    if post_samples_det[i] is not None:\n",
    "        ps = post_samples_det[i]\n",
    "        ax[i].scatter(ps[:, 0], ps[:, 1], s=0.6, marker='.', alpha=0.5,\n",
    "                      color='#1C1B1A', label=\"Posterior Samples\" if i == 0 else None, zorder=0)\n",
    "    ax[i].set_aspect('equal')\n",
    "    ax[i].set_xlabel(r\"$\\theta_1$\")\n",
    "    i += 1\n",
    "\n",
    "# SDE panels for diffusion workflows\n",
    "for wf_i in diffusion_idxs:\n",
    "    col_idx = sde_col_map[wf_i] - 1  # -1 for consistency model skip\n",
    "    name = list(workflows.keys())[wf_i]\n",
    "    title, col = title_and_color(name, is_sde=True, colors=colors)\n",
    "    ax[col_idx].set_title(title)\n",
    "    for j, traj in enumerate(sim_sde[wf_i]):\n",
    "        xy = moving_average(traj, sigma=100/4)  # windows / 4\n",
    "        ax[col_idx].plot(xy[:, 0], xy[:, 1], color=col, alpha=0.75, linewidth=0.75)\n",
    "        ax[col_idx].scatter(xy[0, 0], xy[0, 1], s=init_marker[1], marker=init_marker[0], color=col)\n",
    "        ax[col_idx].scatter(xy[-1, 0], xy[-1, 1], s=end_marker[1], marker=end_marker[0], color=col, alpha=0.25)\n",
    "\n",
    "        # add small arrows along trajectory\n",
    "        mid = len(xy) // 5 * 3\n",
    "        ax[col_idx].annotate(\n",
    "            '',\n",
    "            xy=(xy[mid + 1, 0], xy[mid + 1, 1]),\n",
    "            xytext=(xy[mid, 0], xy[mid, 1]),\n",
    "            arrowprops=dict(arrowstyle='->', color=col, lw=0.7),\n",
    "        )\n",
    "    if post_samples_sde[wf_i] is not None:\n",
    "        ps = post_samples_sde[wf_i]\n",
    "        ax[col_idx].scatter(ps[:, 0], ps[:, 1], alpha=0.5, marker='.', s=0.6, color='#1C1B1A', zorder=0)\n",
    "    ax[col_idx].set_aspect('equal')\n",
    "    ax[col_idx].set_xlabel(r\"$\\theta_1$\")\n",
    "\n",
    "# legend and formatting\n",
    "handles = [\n",
    "    plt.scatter([], [], color='black', marker=init_marker[0], s=30, label='Initial Point (t=1)'),\n",
    "    Line2D([0], [0], color='black', marker='>', markersize=6, markevery=(1, 1),\n",
    "           label='Trajectory'),\n",
    "    plt.scatter([], [], color='#1C1B1A', marker='.', s=50, label='Posterior Samples (t=0)'),\n",
    "]\n",
    "\n",
    "ax[0].set_ylabel(r\"$\\theta_2$\")\n",
    "ax[-1].set_xlim([-0.6, 0.6])\n",
    "ax[-1].set_ylim([-0.6, 0.6])\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])\n",
    "    for spine in a.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    a.set_facecolor('#FFE5CC')\n",
    "\n",
    "for a in ax:\n",
    "    for item in ([a.title, a.xaxis.label, a.yaxis.label] + a.get_xticklabels() + a.get_yticklabels()):\n",
    "        item.set_fontsize(10)\n",
    "\n",
    "#plt.savefig('trajectories.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "11c69345f11cc81d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plotting\n",
    "ncols = 3\n",
    "fig, ax = plt.subplots(ncols=ncols, figsize=(10, 3), layout='constrained', sharey='row', sharex='row')\n",
    "ax = [ax] if ncols == 1 else ax.flatten()\n",
    "\n",
    "# map sde column positions\n",
    "sde_col_start = len(workflows)\n",
    "sde_col_map = {wf_i: sde_col_start + k for k, wf_i in enumerate(diffusion_idxs)}\n",
    "init_marker = '*', 10\n",
    "end_marker = 'o', 10\n",
    "\n",
    "xlim = [-0.6, 0.6]\n",
    "ylim = [-0.6, 0.6]\n",
    "\n",
    "# deterministic panels\n",
    "p_ids = [0,1,1,1,2,2]\n",
    "for i, (name, workflow) in enumerate(workflows.items()):\n",
    "    if not name in ['diffusion_model_cosine', 'flow_matching',  'stable_consistency']:\n",
    "        continue\n",
    "    if post_samples_det[i] is not None:\n",
    "        ps = post_samples_det[i]\n",
    "        ps_x = ps[ps[:, 0] < 0, 0]\n",
    "        ps_y = ps[ps[:, 0] < 0, 1]\n",
    "        ax[p_ids[i]].scatter(ps_x, ps_y, s=2, marker='.', alpha=0.5,\n",
    "                         color='#1C1B1A', label=\"Posterior Samples\" if i == 0 else None, zorder=0)\n",
    "    if name != 'flow_matching':\n",
    "        continue\n",
    "    for p_id in range(3):\n",
    "        title, col = title_and_color(name, is_sde=False, colors=colors)\n",
    "        for j, (traj, vels) in enumerate(sim_det[i]):\n",
    "            if traj[-1, 0] > 0:\n",
    "                continue\n",
    "            ax[p_id].plot(traj[:, 0], traj[:, 1], color=col, alpha=0.75, linewidth=0.75)\n",
    "            ax[p_id].scatter(traj[0, 0], traj[0, 1], s=init_marker[1], marker=init_marker[0], color=col)\n",
    "            ax[p_id].scatter(traj[-1, 0], traj[-1, 1], s=end_marker[1], marker=end_marker[0], color=col, alpha=0.25)\n",
    "\n",
    "            # add small arrows along trajectory\n",
    "            mid = len(traj) // 5 * 3\n",
    "            ax[p_id].annotate(\n",
    "               '',\n",
    "               xy=(traj[mid + 1, 0], traj[mid + 1, 1]),\n",
    "               xytext=(traj[mid, 0], traj[mid, 1]),\n",
    "               arrowprops=dict(arrowstyle='->', color=col, lw=1),\n",
    "            )\n",
    "        ax[p_id].set_aspect('equal')\n",
    "        ax[p_id].set_xlabel(r\"$\\theta_1$\")\n",
    "\n",
    "\n",
    "# legend and formatting\n",
    "handles = [\n",
    "    plt.scatter([], [], color='black', marker=init_marker[0], s=30, label='Initial Point (t=1)'),\n",
    "    Line2D([0], [0], color='black', marker='>', markersize=6, markevery=(1, 1),\n",
    "           label='Trajectory'),\n",
    "    plt.scatter([], [], color='#1C1B1A', marker='.', s=50, label='Posterior Samples (t=0)'),\n",
    "]\n",
    "\n",
    "ax[0].set_ylabel(r\"$\\theta_2$\")\n",
    "#ax[-1].set_xlim([-0.6, 0.2])\n",
    "#ax[-1].set_ylim([-0.6, 0.2])\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])\n",
    "    for spine in a.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    a.set_facecolor('#FFE5CC')\n",
    "\n",
    "for a in ax:\n",
    "    for item in ([a.title, a.xaxis.label, a.yaxis.label] + a.get_xticklabels() + a.get_yticklabels()):\n",
    "        item.set_fontsize(10)\n",
    "\n",
    "#plt.savefig('trajectories_fm.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "c1990a336fa662bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualizing the Schedules",
   "id": "fa85e1cb9eef5b94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from bayesflow.networks.diffusion_model.schedules import EDMNoiseSchedule, CosineNoiseSchedule, NoiseSchedule",
   "id": "ef6ddb2da13bf8b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FlowMatching(NoiseSchedule):\n",
    "    def __init__(self, name=\"Flow Matching Schedule\"):\n",
    "        super().__init__(name=name, variance_type=\"preserving\", weighting=None)\n",
    "\n",
    "    def get_log_snr(self, t, training):\n",
    "        \"\"\"Get the log signal-to-noise ratio (lambda) for a given diffusion time.\"\"\"\n",
    "        return 2 * keras.ops.log((1-t)/t)\n",
    "\n",
    "    def get_alpha_sigma(self, t):\n",
    "        alpha_t = 1 - t\n",
    "        sigma_t = 1 - alpha_t\n",
    "        return alpha_t, sigma_t\n",
    "\n",
    "    def get_t_from_log_snr(self, log_snr_t, training: bool):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def derivative_log_snr(self, log_snr_t, training):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_weights_for_snr(self, log_snr_t):\n",
    "        return 1 + keras.ops.exp(-log_snr_t) + 2*keras.ops.exp(-log_snr_t / 2)"
   ],
   "id": "2428482878b57571",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sample_powerlaw(alpha, size=1):\n",
    "    \"\"\"\n",
    "    Sample from distribution with CDF p(t) ∝ t^(1/(1+α)) on [0,1]\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    alpha : float\n",
    "        Shape parameter. α = 0 gives uniform distribution\n",
    "    size : int or tuple\n",
    "        Number of samples to generate\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    samples : ndarray\n",
    "        Samples from the distribution\n",
    "    \"\"\"\n",
    "    # Generate uniform random samples\n",
    "    u = np.random.uniform(0, 1, size)\n",
    "\n",
    "    # Since p(t) ∝ t^(1/(1+α)) is the CDF, we need the inverse\n",
    "    # Normalized CDF: F(t) = t^(1/(1+α))\n",
    "    # Inverse CDF: F^(-1)(u) = u^(1+α)\n",
    "    return 1 - u ** (1 + alpha)"
   ],
   "id": "cf3867346f3efe58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "edm = EDMNoiseSchedule()\n",
    "edm_ve = EDMNoiseSchedule(variance_type='exploding')\n",
    "cosine = CosineNoiseSchedule(weighting=None)\n",
    "cosine_ve = CosineNoiseSchedule(weighting=None)\n",
    "cosine_ve._variance_type = 'exploding'\n",
    "edm.name = \"EDM Schedule\"# VP\"\n",
    "edm_ve.name = \"EDM Schedule VE\"\n",
    "cosine.name = \"Cosine Schedule\"\n",
    "fm = FlowMatching()\n",
    "fm_pl = FlowMatching(r\"Flow Matching Schedule $\\rho=-0.6$\")\n",
    "\n",
    "time = keras.ops.linspace(0.0, 1, 10000)\n",
    "# gebhard: t^(1/4) sim U(0,1)\n",
    "#fm_time = (1-time) ** 4\n",
    "# power law with alpha=-0.6\n",
    "fm_time = sample_powerlaw(alpha=-0.6, size=time.shape[0])\n",
    "fm_time = np.sort(fm_time)\n",
    "fm_time = keras.ops.convert_to_tensor(fm_time)\n",
    "colors = [\n",
    "    \"#E7298A\",  # magenta pink, EDM\n",
    "    \"#7570B3\",  # muted purple, cosine\n",
    "    \"#1B9E77\",  # teal green, flow matching\n",
    "    \"#D95F02\",  # deep orange, flow matching pl\n",
    "]\n",
    "schedules = [edm, #edm_ve,\n",
    "             cosine, fm, fm_pl]"
   ],
   "id": "fc4614be04485301",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axis = plt.subplots(1, 4, figsize=(10,2), layout='constrained') #, gridspec_kw={'width_ratios': [1.75, 1]})\n",
    "ax = axis[2]\n",
    "for i, schedule in enumerate(schedules):\n",
    "    training_schedule = keras.ops.convert_to_numpy(schedule.get_log_snr(time, training=True))\n",
    "    inference_schedule = keras.ops.convert_to_numpy(schedule.get_log_snr(time, training=False))\n",
    "\n",
    "    if (training_schedule != inference_schedule).all():\n",
    "        ax.plot(time, training_schedule, label=f\"{schedule.name} Training\", color=colors[i])\n",
    "        ax.plot(time, inference_schedule, label=f\"{schedule.name} Inference\", linestyle=\"--\", color=colors[i])\n",
    "    else:\n",
    "        ax.plot(time, training_schedule, label=f\"{schedule.name}\", color=colors[i])\n",
    "ax.set_ylabel(r\"log SNR $\\lambda_t$\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "\n",
    "ax = axis[0:2]\n",
    "for i, schedule in enumerate([edm, edm_ve]):\n",
    "    training_schedule = schedule.get_log_snr(time, training=True)\n",
    "    alpha_t, sigma_t = schedule.get_alpha_sigma(training_schedule)\n",
    "    if i == 0:\n",
    "        ax[0].plot(time, keras.ops.convert_to_numpy(alpha_t), color=colors[0])\n",
    "        ax[0].plot(time, keras.ops.convert_to_numpy(sigma_t), color=colors[0])\n",
    "        ax[0].text(0.9, 0.75, r'$\\sigma_t$', color=colors[0], fontsize=12)\n",
    "        ax[0].text(0.85, 0.25, r'$\\alpha_t$', color=colors[0], fontsize=12)\n",
    "    else:\n",
    "        ax[1].plot(time, keras.ops.convert_to_numpy(alpha_t), color=colors[0])\n",
    "        ax[1].plot(time, keras.ops.convert_to_numpy(sigma_t), color=colors[0])\n",
    "        print(max(sigma_t))\n",
    "        ax[1].set_ylim(0, 5)\n",
    "        ax[1].text(0.4, 1.1, r'$\\alpha_t$', color=colors[0], fontsize=12)\n",
    "        ax[1].text(0.85, 2.65, r'$\\sigma_t$', color=colors[0], fontsize=12)\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time\")\n",
    "    #a.set_ylabel(\"Noise Level\")\n",
    "ax[0].set_ylabel(r'VP Noise Level')\n",
    "ax[1].set_ylabel(r'VE Noise Level')\n",
    "\n",
    "ax = axis[3]\n",
    "for i, schedule in enumerate(schedules):\n",
    "    if schedule.name == r'Flow Matching Schedule $\\rho=-0.6$':\n",
    "        training_schedule = schedule.get_log_snr(fm_time[1:-1], training=True)\n",
    "    else:\n",
    "        training_schedule = schedule.get_log_snr(time[1:-1], training=True)\n",
    "    training_weights = keras.ops.convert_to_numpy(schedule.get_weights_for_snr(training_schedule))\n",
    "    log_snr = np.random.choice(training_schedule, p=training_weights / training_weights.sum(), replace=True, size=10000)\n",
    "    ax.hist(log_snr, density=True, color=colors[i], label=f\"{schedule.name}\", alpha=0.7, bins=20)\n",
    "\n",
    "ax.set_xlabel(r\"$\\log$ SNR $\\lambda_t$\")\n",
    "ax.set_ylabel(\"Weighting Density\")\n",
    "handles = [\n",
    "    Patch(color=colors[0], label='EDM Schedule', alpha=0.7),\n",
    "    Patch(color=colors[1], label='Cosine Schedule', alpha=0.7),\n",
    "    Line2D([0], [0], color=colors[0], label='EDM Training', alpha=0.7),\n",
    "    Patch(color=colors[2], label='Flow Matching Schedule', alpha=0.7),\n",
    "    Line2D([0], [0], color=colors[0], linestyle='--', label='EDM Inference', alpha=0.7),\n",
    "    Patch(color=colors[3], label=r'Flow Matching Schedule $\\rho=-0.6$', alpha=0.7),\n",
    "]\n",
    "fig.legend(handles=handles, loc=\"lower center\", bbox_to_anchor=(0.5, -0.4), ncols=3, frameon=False, fontsize=12)\n",
    "for a in axis:\n",
    "    a.spines['top'].set_visible(False)\n",
    "    a.spines['right'].set_visible(False)\n",
    "for ax in axis:\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(12)\n",
    "plt.savefig('schedules.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "e348df0a230f5b38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "from typing import Literal\n",
    "from keras import ops\n",
    "from bayesflow.types import Tensor\n",
    "\n",
    "class EDMNoiseScheduleNoWeight(NoiseSchedule):\n",
    "    \"\"\"EDM noise schedule for diffusion models. This schedule is based on the EDM paper [1].\n",
    "    This should be used with the F-prediction type in the diffusion model.\n",
    "\n",
    "    [1] Karras, T., Aittala, M., Aila, T., & Laine, S. (2022). Elucidating the design space of diffusion-based\n",
    "    generative models. Advances in Neural Information Processing Systems, 35, 26565-26577.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        weighting,\n",
    "        sigma_data: float = 1.0,\n",
    "        sigma_min: float = 1e-4,\n",
    "        sigma_max: float = 80.0,\n",
    "        variance_type: Literal[\"preserving\", \"exploding\"] = \"preserving\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the EDM noise schedule.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sigma_data : float, optional\n",
    "            The standard deviation of the output distribution. Input of the network is scaled by this factor and\n",
    "            the weighting function is scaled by this factor as well. Default is 1.0.\n",
    "        sigma_min : float, optional\n",
    "            The minimum noise level. Only relevant for sampling. Default is 1e-4.\n",
    "        sigma_max : float, optional\n",
    "            The maximum noise level. Only relevant for sampling. Default is 80.0.\n",
    "        variance_type : Literal[\"preserving\", \"exploding\"], optional\n",
    "            The type of variance to use. Default is \"preserving\". Original EDM paper uses \"exploding\".\n",
    "        \"\"\"\n",
    "        super().__init__(name=\"edm_noise_schedule\", variance_type=variance_type, weighting=weighting)\n",
    "        self.sigma_data = sigma_data\n",
    "        # training settings\n",
    "        self.p_mean = -1.2\n",
    "        self.p_std = 1.2\n",
    "        # sampling settings\n",
    "        self.sigma_max = sigma_max\n",
    "        self.sigma_min = sigma_min\n",
    "        self.rho = 7\n",
    "\n",
    "        # convert EDM parameters to signal-to-noise ratio formulation\n",
    "        self.log_snr_min = -2 * ops.log(sigma_max)\n",
    "        self.log_snr_max = -2 * ops.log(sigma_min)\n",
    "        # t is not truncated for EDM by definition of the sampling schedule\n",
    "        # training bounds should be set to avoid numerical issues\n",
    "        self._log_snr_min_training = self.log_snr_min - 1  # one is never sampler during training\n",
    "        self._log_snr_max_training = self.log_snr_max + 1  # 0 is almost surely never sampled during training\n",
    "\n",
    "    def get_log_snr(self, t: float | Tensor, training: bool) -> Tensor:\n",
    "        \"\"\"Get the log signal-to-noise ratio (lambda) for a given diffusion time.\"\"\"\n",
    "        if training:\n",
    "            # SNR = dist.icdf(1-t)  # Kingma paper wrote -F(t) but this seems to be wrong\n",
    "            loc = -2 * self.p_mean\n",
    "            scale = 2 * self.p_std\n",
    "            snr = loc + scale * ops.erfinv(2 * (1 - t) - 1) * math.sqrt(2)\n",
    "            snr = ops.clip(snr, x_min=self._log_snr_min_training, x_max=self._log_snr_max_training)\n",
    "        else:\n",
    "            sigma_min_rho = self.sigma_min ** (1 / self.rho)\n",
    "            sigma_max_rho = self.sigma_max ** (1 / self.rho)\n",
    "            snr = -2 * self.rho * ops.log(sigma_max_rho + (1 - t) * (sigma_min_rho - sigma_max_rho))\n",
    "        return snr\n",
    "\n",
    "    def get_t_from_log_snr(self, log_snr_t: float | Tensor, training: bool) -> Tensor:\n",
    "        \"\"\"Get the diffusion time (t) from the log signal-to-noise ratio (lambda).\"\"\"\n",
    "        if training:\n",
    "            # SNR = dist.icdf(1-t) => t = 1-dist.cdf(snr)  # Kingma paper wrote -F(t) but this seems to be wrong\n",
    "            loc = -2 * self.p_mean\n",
    "            scale = 2 * self.p_std\n",
    "            x = log_snr_t\n",
    "            t = 1 - 0.5 * (1 + ops.erf((x - loc) / (scale * math.sqrt(2.0))))\n",
    "        else:  # sampling\n",
    "            # SNR = -2 * rho * log(sigma_max ** (1/rho) + (1 - t) * (sigma_min ** (1/rho) - sigma_max ** (1/rho)))\n",
    "            # => t = 1 - ((exp(-snr/(2*rho)) - sigma_max ** (1/rho)) / (sigma_min ** (1/rho) - sigma_max ** (1/rho)))\n",
    "            sigma_min_rho = self.sigma_min ** (1 / self.rho)\n",
    "            sigma_max_rho = self.sigma_max ** (1 / self.rho)\n",
    "            t = 1 - ((ops.exp(-log_snr_t / (2 * self.rho)) - sigma_max_rho) / (sigma_min_rho - sigma_max_rho))\n",
    "        return t\n",
    "\n",
    "    def derivative_log_snr(self, log_snr_t: Tensor, training: bool = False) -> Tensor:\n",
    "        \"\"\"Compute d/dt log(1 + e^(-snr(t))), which is used for the reverse SDE.\"\"\"\n",
    "        if training:\n",
    "            raise NotImplementedError(\"Derivative of log SNR is not implemented for training mode.\")\n",
    "        # sampling mode\n",
    "        t = self.get_t_from_log_snr(log_snr_t=log_snr_t, training=training)\n",
    "\n",
    "        # SNR = -2*rho*log(s_max + (1 - x)*(s_min - s_max))\n",
    "        s_max = self.sigma_max ** (1 / self.rho)\n",
    "        s_min = self.sigma_min ** (1 / self.rho)\n",
    "        u = s_max + (1 - t) * (s_min - s_max)\n",
    "        # d/dx snr = 2*rho*(s_min - s_max) / u\n",
    "        dsnr_dx = 2 * self.rho * (s_min - s_max) / u\n",
    "\n",
    "        # Using the chain rule on f(t) = log(1 + e^(-snr(t))):\n",
    "        # f'(t) = - (e^{-snr(t)} / (1 + e^{-snr(t)})) * dsnr_dt\n",
    "        factor = ops.exp(-log_snr_t) / (1 + ops.exp(-log_snr_t))\n",
    "        return -factor * dsnr_dx\n",
    "\n",
    "    # def get_weights_for_snr(self, log_snr_t: Tensor) -> Tensor:\n",
    "    #     \"\"\"Get weights for the signal-to-noise ratio (snr) for a given log signal-to-noise ratio (lambda).\"\"\"\n",
    "    #     # for F-loss: w = (ops.exp(-log_snr_t) + sigma_data^2) / (ops.exp(-log_snr_t)*sigma_data^2)\n",
    "    #     return 1 + ops.exp(-log_snr_t) / ops.square(self.sigma_data)"
   ],
   "id": "af81958eed8f0908",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot:\n",
    "# - EDM schedule with different weighting functions, F parameterization\n",
    "# - EDM schedule with different weighting functions, e parameterization\n",
    "# - Cosine schedule with different weighting functions, e parameterization\n",
    "edm = EDMNoiseSchedule()\n",
    "cosine = CosineNoiseSchedule(weighting=None)\n",
    "schedules = [edm, cosine]\n",
    "time = keras.ops.linspace(0.0, 1, 10000)\n",
    "\n",
    "fig, axis = plt.subplots(3, 3, figsize=(10,6), sharey='row', sharex=True, layout='constrained')\n",
    "ax = axis[0]\n",
    "max_w = 700\n",
    "for i, schedule in enumerate(schedules):\n",
    "    training_schedule = schedule.get_log_snr(time, training=True)\n",
    "    no_weight_schedule = CosineNoiseSchedule(weighting=None)\n",
    "    training_weights = keras.ops.convert_to_numpy(no_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    ax[0].plot(training_schedule, training_weights/ training_weights.max(), color=colors[i], label=f\"{schedule.name}\", alpha=0.7)\n",
    "    ax[0].set_title('No weighting')\n",
    "\n",
    "\n",
    "    training_schedule = schedule.get_log_snr(time, training=True)\n",
    "    sigmoid_weight_schedule = CosineNoiseSchedule(weighting='sigmoid')\n",
    "    training_weights = keras.ops.convert_to_numpy(sigmoid_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    ax[1].plot(training_schedule, training_weights, color=colors[i], label=f\"{schedule.name}\", alpha=0.7)\n",
    "    ax[1].set_title('Sigmoid weighting')\n",
    "\n",
    "    training_schedule = schedule.get_log_snr(time, training=True)\n",
    "    if i == 0:\n",
    "        llh_weight_schedule = EDMNoiseScheduleNoWeight(weighting='likelihood_weighting')\n",
    "    else:\n",
    "        llh_weight_schedule = CosineNoiseSchedule(weighting='likelihood_weighting')\n",
    "    training_weights = keras.ops.convert_to_numpy(llh_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    ax[2].plot(training_schedule, training_weights/ training_weights.max(), color=colors[i], label=f\"{schedule.name}\", alpha=0.7)\n",
    "    ax[2].set_title('Likelihood weighting')\n",
    "ax[0].set_ylabel(\"Weighting Function\\n\"+r\"($\\boldsymbol{\\epsilon}$-loss)\")\n",
    "ax[0].set_ylim(-0.1, 1.1)\n",
    "\n",
    "ax = axis[1]\n",
    "for i, schedule in enumerate(schedules):\n",
    "    training_schedule = schedule.get_log_snr(time, training=True)\n",
    "    no_weight_schedule = CosineNoiseSchedule(weighting=None)\n",
    "    training_weights = keras.ops.convert_to_numpy(no_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    log_snr = np.random.choice(training_schedule, p=training_weights / training_weights.sum(), replace=True, size=10000)\n",
    "    ax[0].hist(log_snr, density=True, color=colors[i], label=f\"{schedule.name}\", alpha=0.7, bins=20)\n",
    "\n",
    "    training_schedule = schedule.get_log_snr(time, training=True)\n",
    "    sigmoid_weight_schedule = CosineNoiseSchedule(weighting='sigmoid')\n",
    "    training_weights = keras.ops.convert_to_numpy(sigmoid_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    log_snr = np.random.choice(training_schedule, p=training_weights / training_weights.sum(), replace=True, size=10000)\n",
    "    ax[1].hist(log_snr, density=True, color=colors[i], label=f\"{schedule.name}\", alpha=0.7, bins=20)\n",
    "\n",
    "    training_schedule = schedule.get_log_snr(time, training=True)\n",
    "    log_snr = np.random.uniform(np.min(training_schedule), np.max(training_schedule), size=training_schedule.shape)\n",
    "    ax[2].hist(log_snr, density=True, color=colors[i], label=f\"{schedule.name}\", alpha=0.7, bins=20)\n",
    "ax[0].set_ylabel(\"Weighting Density\\n\"+r\"($\\boldsymbol{\\epsilon}$-loss)\")\n",
    "\n",
    "ax = axis[2]\n",
    "for i, schedule in enumerate(schedules):\n",
    "    training_schedule = schedule.get_log_snr(time[1:-1], training=True)\n",
    "    no_weight_schedule = CosineNoiseSchedule(weighting=None)\n",
    "    training_weights = keras.ops.convert_to_numpy(no_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    edm_weight_schedule = EDMNoiseSchedule()\n",
    "    edm_weights = keras.ops.convert_to_numpy(edm_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    training_weights = training_weights * edm_weights\n",
    "    log_snr = np.random.choice(training_schedule, p=training_weights / training_weights.sum(), replace=True, size=10000)\n",
    "    ax[0].hist(log_snr, density=True, color=colors[i], label=f\"{schedule.name}\", alpha=0.7, bins=20)\n",
    "    ax[0].set_xlabel(r\"log SNR $\\lambda_t$\")\n",
    "\n",
    "    training_schedule = schedule.get_log_snr(time[1:-1], training=True)\n",
    "    sigmoid_weight_schedule = CosineNoiseSchedule(weighting='sigmoid')\n",
    "    training_weights = keras.ops.convert_to_numpy(sigmoid_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    edm_weight_schedule = EDMNoiseSchedule()\n",
    "    edm_weights = keras.ops.convert_to_numpy(edm_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    training_weights = training_weights * edm_weights\n",
    "    log_snr = np.random.choice(training_schedule, p=training_weights / training_weights.sum(), replace=True, size=10000)\n",
    "    ax[1].hist(log_snr, density=True, color=colors[i], label=f\"{schedule.name}\", alpha=0.7, bins=20)\n",
    "    ax[1].set_xlabel(r\"log SNR $\\lambda_t$\")\n",
    "\n",
    "    training_schedule = schedule.get_log_snr(time[1:-1], training=True)\n",
    "    training_schedule = np.random.uniform(np.min(training_schedule), np.max(training_schedule), size=training_schedule.shape)\n",
    "    edm_weight_schedule = EDMNoiseSchedule()\n",
    "    edm_weights = keras.ops.convert_to_numpy(edm_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    training_weights = edm_weights\n",
    "    log_snr = np.random.choice(training_schedule, p=training_weights / training_weights.sum(), replace=True, size=10000)\n",
    "    ax[2].hist(log_snr, density=True, color=colors[i], label=f\"{schedule.name}\", alpha=0.7, bins=20)\n",
    "    ax[2].set_xlabel(r\"log SNR $\\lambda_t$\")\n",
    "\n",
    "handles = [\n",
    "    Patch(color=colors[0], label='EDM', alpha=0.7),\n",
    "    Patch(color=colors[1], label='Cosine', alpha=0.7),\n",
    "]\n",
    "#fig.legend(handles=handles, labels=['EDM', 'Cosine'], loc=\"lower center\", bbox_to_anchor=(0.5, -0.05), ncol=2, frameon=False)\n",
    "fig.legend(handles=handles, labels=['EDM', 'Cosine'], loc=\"center right\", bbox_to_anchor=(1.13, 0.5),\n",
    "           ncol=1, frameon=False, fontsize=12)\n",
    "ax[0].set_ylabel(\"Weighting Density\\n\"+r\"($\\boldsymbol{F}$-loss)\")\n",
    "fig.align_ylabels(axis[:, 0])\n",
    "for a in axis.flatten():\n",
    "    a.spines['top'].set_visible(False)\n",
    "    a.spines['right'].set_visible(False)\n",
    "for ax in axis.flatten():\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(12)\n",
    "plt.savefig('weightings.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "af05f0d7fccad660",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(12,3), layout='constrained') #, sharey=True)\n",
    "for i, schedule in enumerate(schedules):\n",
    "    if schedule.name == 'Flow Matching Schedule':\n",
    "        alpha_t_training, sigma_t_training = schedule.get_alpha_sigma(time)\n",
    "        alpha_t_inference, sigma_t_inference = alpha_t_training, sigma_t_training\n",
    "    else:\n",
    "        training_schedule = schedule.get_log_snr(time, training=True)\n",
    "        inference_schedule = schedule.get_log_snr(time, training=False)\n",
    "        alpha_t_training, sigma_t_training = schedule.get_alpha_sigma(training_schedule)\n",
    "        alpha_t_inference, sigma_t_inference = schedule.get_alpha_sigma(inference_schedule)\n",
    "    alpha_t_training = keras.ops.convert_to_numpy(alpha_t_training)\n",
    "    sigma_t_training = keras.ops.convert_to_numpy(sigma_t_training)\n",
    "    alpha_t_inference = keras.ops.convert_to_numpy(alpha_t_inference)\n",
    "    sigma_t_inference = keras.ops.convert_to_numpy(sigma_t_inference)\n",
    "\n",
    "    if (training_schedule != inference_schedule).all():\n",
    "        ax[0].plot(time, alpha_t_training, label=f\"{schedule.name} Training\", color=colors[i])\n",
    "        ax[0].plot(time, alpha_t_inference, label=f\"{schedule.name} Inference\", linestyle=\"--\", color=colors[i])\n",
    "        ax[1].plot(time, sigma_t_training, label=f\"{schedule.name} Training\", color=colors[i])\n",
    "        ax[1].plot(time, sigma_t_inference, label=f\"{schedule.name} Inference\", linestyle=\"--\", color=colors[i])\n",
    "        ax[2].plot(time, alpha_t_training**2+sigma_t_training**2, label=f\"{schedule.name} Training\", color=colors[i])\n",
    "        ax[2].plot(time, alpha_t_inference**2+sigma_t_inference**2, label=f\"{schedule.name} Inference\", linestyle=\"--\", color=colors[i])\n",
    "    else:\n",
    "        ax[0].plot(time, alpha_t_training, label=f\"{schedule.name} Training & Inference\", color=colors[i])\n",
    "        ax[1].plot(time, sigma_t_training, label=f\"{schedule.name} Training & Inference\", color=colors[i])\n",
    "        ax[2].plot(time, alpha_t_training**2+sigma_t_training**2, label=f\"{schedule.name} Training & Inference\", color=colors[i])\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"time\")\n",
    "ax[0].set_ylabel(r\"$\\alpha_t$\")\n",
    "ax[1].set_ylabel(r\"$\\sigma_t$\")\n",
    "ax[1].set_yscale(\"log\")\n",
    "ax[2].set_yscale(\"log\")\n",
    "ax[2].set_ylabel(r\"$\\alpha_t^2+\\sigma_t^2$\")\n",
    "fig.legend(loc=\"lower center\", ncol=3, bbox_to_anchor=(0.5, -0.35))\n",
    "plt.show()"
   ],
   "id": "37884c5f6fe8578c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# theoretical densities from Kingma Paper\n",
    "lambda_t = np.linspace(-15, 15, 100)\n",
    "edm_w = norm.pdf(lambda_t, loc=2.4, scale=2.4) * (np.exp(-lambda_t) + 1**2)\n",
    "cosine_w = sech(lambda_t / 2)\n",
    "fm_w =  np.exp(-lambda_t/2)\n",
    "\n",
    "plt.plot(lambda_t, edm_w / sum(edm_w), label='edm', color=colors[0])\n",
    "plt.plot(lambda_t, cosine_w / sum(cosine_w), label='cosine', color=colors[1])\n",
    "plt.plot(lambda_t, fm_w / sum(fm_w), label='flow matching', color=colors[2])\n",
    "plt.xlabel(r\"log SNR $\\lambda$\")\n",
    "plt.ylabel(\"Implied Weighting Function\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "e829c47f0990389f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Variance Types\n",
    "\n",
    "- Variance preserving (VP): $\\alpha_t = \\sqrt{1 - \\sigma_t^2}$. The total variance remains constant over $t$. This setting maintains a balance between signal and noise at each step.\n",
    "- Variance exploding (VE): $\\alpha_t = 1$ with $\\sigma_t$ growing large. Here the signal is constant but the noise variance increases.\n",
    "- Sub-variance preserving (sub-VP)}: $\\alpha_t = \\sqrt{1 - \\sigma_t}$.\n",
    "- Flow matching: $\\alpha_t = 1 - \\sigma_t$."
   ],
   "id": "96dc58cfce298946"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "t = np.linspace(1,0, 1000)\n",
    "\n",
    "vp_sigma_t = t\n",
    "vp_alpha_t = np.sqrt(1 - vp_sigma_t ** 2)\n",
    "\n",
    "ve_sigma_t = t\n",
    "ve_alpha_t = np.ones_like(ve_sigma_t)\n",
    "\n",
    "sub_vp_sigma_t = t\n",
    "sub_vp_alpha_t = np.sqrt(1 - sub_vp_sigma_t)\n",
    "\n",
    "fm_sigma_t = t\n",
    "fm_alpha_t = 1 - t"
   ],
   "id": "fe5116ebff5165f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = plt.figure(figsize=(5,3), layout='constrained')\n",
    "plt.plot(t, vp_sigma_t**2+vp_alpha_t**2, label='Variance Preserving')\n",
    "plt.plot(t, ve_sigma_t**2+ve_alpha_t**2, label='Variance Exploding')\n",
    "plt.plot(t, sub_vp_sigma_t**2+sub_vp_alpha_t**2, label='Sub-Variance Preserving')\n",
    "plt.plot(t, fm_sigma_t**2+fm_alpha_t**2, label='Flow Matching')\n",
    "plt.ylabel('Variance')\n",
    "plt.xlabel('time')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "6d61f9ff414c9f21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "57d098cee7ad0a8e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
