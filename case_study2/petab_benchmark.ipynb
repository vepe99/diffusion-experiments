{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# PEtab benchmark model with BayesFlow",
   "id": "f2db3aaca617dd83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pip install git+https://github.com/Benchmarking-Initiative/Benchmark-Models-PEtab.git@master#subdirectory=src/python\n",
    "# pypesto, amici, petab, fides, joblib"
   ],
   "id": "c4c3a5bf38839491",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "else:\n",
    "    print(f\"Using '{os.environ['KERAS_BACKEND']}' backend\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "from typing import Union\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import benchmark_models_petab as benchmark_models\n",
    "import petab\n",
    "import pypesto.optimize as optimize\n",
    "import pypesto.sample as sample\n",
    "import pypesto.petab\n",
    "import pypesto.visualize as visualize\n",
    "from pypesto.visualize.model_fit import visualize_optimized_model_fit\n",
    "from scipy import stats\n",
    "\n",
    "import bayesflow as bf\n",
    "\n",
    "import amici\n",
    "import logging\n",
    "amici.swig_wrappers.logger.setLevel(logging.CRITICAL)\n",
    "pypesto.logging.log(level=logging.ERROR, name=\"pypesto.petab\", console=True)\n",
    "\n",
    "from petab_helper import scale_values, values_to_linear_scale, amici_df_to_array, amici_pred_to_df, create_pypesto_problem\n",
    "\n",
    "USE_MALA = False\n",
    "\n",
    "# print all model names\n",
    "print(benchmark_models.MODELS)"
   ],
   "id": "f7d1d9565944bee9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# generate petab problem\n",
    "#job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0))\n",
    "n_cpus = 10 #int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",
    "problem_name = \"Beer_MolBioSystems2014\" #\"Beer_MolBioSystems2014\", \"Boehm_JProteomeRes2014\"\n",
    "storage = '' # f'plots/{problem_name}/'\n",
    "petab_problem = benchmark_models.get_problem(problem_name)\n",
    "\n",
    "# decrease upper bounds for offset, scaling and noise parameters\n",
    "scale_params_id = [name for name in petab_problem.parameter_df.index.values if name[:6] == 'offset' or name[:5] == 'scale']\n",
    "petab_problem.parameter_df.loc[scale_params_id, 'upperBound'] = 100  # instead of 1000\n",
    "sd_params_id = [name for name in petab_problem.parameter_df.index.values if name[:3] == 'sd_']\n",
    "petab_problem.parameter_df.loc[sd_params_id, 'upperBound'] = 10  # instead of 1000\n",
    "\n",
    "\n",
    "# add normal prior (on scale) around real parameters values\n",
    "real_data_params = petab_problem.parameter_df.nominalValue\n",
    "std = 0.5\n",
    "for i in real_data_params.index:\n",
    "    if petab_problem.parameter_df.loc[i, 'estimate'] == 0:\n",
    "        continue\n",
    "    # set prior mean depending on scale\n",
    "    mean = scale_values(real_data_params.loc[i], petab_problem.parameter_df.loc[i, 'parameterScale'])\n",
    "    if not 'objectivePriorType' in petab_problem.parameter_df or pd.isna(petab_problem.parameter_df.loc[i, 'objectivePriorType']):\n",
    "        petab_problem.parameter_df.loc[i, 'objectivePriorType'] = \"parameterScaleNormal\"\n",
    "        petab_problem.parameter_df.loc[i, 'objectivePriorParameters'] = f\"{mean};{std}\"\n",
    "\n",
    "for i, row in petab_problem.parameter_df.iterrows():\n",
    "    if 'objectivePriorType' in row and not pd.isna(row['objectivePriorType']):\n",
    "        if row['estimate'] == 0:\n",
    "            print(f\"Parameter {i} has a {row['objectivePriorType']} prior but is not estimated, setting to nan\")\n",
    "            petab_problem.parameter_df.loc[i, 'objectivePriorType'] = np.nan\n",
    "        # validate petab problem, if scale for parameter is defined, prior must be on the same scale\n",
    "        if row['parameterScale'] != 'lin' and not row['objectivePriorType'].startswith('parameterScale'):\n",
    "            raise ValueError(f\"Parameter {i} has parameterScale {row['parameterScale']} but {row['objectivePriorType']} prior\")\n",
    "\n",
    "\n",
    "importer = pypesto.petab.PetabImporter(petab_problem, simulator_type=\"amici\")\n",
    "factory = importer.create_objective_creator()\n",
    "\n",
    "model = factory.create_model(verbose=False)\n",
    "amici_predictor = factory.create_predictor()\n",
    "amici_predictor.amici_objective.amici_solver.setAbsoluteTolerance(1e-8)\n",
    "\n",
    "# Creating the pypesto problem from PEtab\n",
    "pypesto_problem = importer.create_problem()\n",
    "pypesto_problem.print_parameter_summary()"
   ],
   "id": "6bc21d5dc9ed9ad7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_samples_from_dict(samples_dict):\n",
    "    samples = np.stack([samples_dict[name][..., 0] for name in pypesto_problem.x_names], axis=-1)\n",
    "    return samples\n",
    "\n",
    "def prior():\n",
    "    lb = petab_problem.parameter_df['lowerBound'].values\n",
    "    ub = petab_problem.parameter_df['upperBound'].values\n",
    "    param_names_id = petab_problem.parameter_df.index.values\n",
    "    param_scale = petab_problem.parameter_df['parameterScale'].values\n",
    "    if 'objectivePriorType' in petab_problem.parameter_df.columns:\n",
    "        prior_type = petab_problem.parameter_df['objectivePriorType'].values\n",
    "    else:\n",
    "        prior_type = [np.nan] * len(param_names_id)\n",
    "    estimate_param = petab_problem.parameter_df['estimate'].values\n",
    "\n",
    "    prior_dict = {}\n",
    "    for i, name in enumerate(param_names_id):\n",
    "        if estimate_param[i] == 0:\n",
    "            prior_dict[name] = petab_problem.parameter_df['nominalValue'].values[i]  # linear space\n",
    "        elif prior_type[i] == 'uniform':  # linear space\n",
    "            prior_dict[name] = np.random.uniform(low=lb[i], high=ub[i])\n",
    "        elif prior_type[i] == 'parameterScaleUniform' or pd.isna(prior_type[i]):\n",
    "            # scale bounds to scaled space\n",
    "            lb_scaled_i = scale_values(lb[i], param_scale[i])\n",
    "            ub_scaled_i = scale_values(ub[i], param_scale[i])\n",
    "            val = np.random.uniform(low=lb_scaled_i, high=ub_scaled_i)\n",
    "            # scale to linear space\n",
    "            prior_dict[name] = values_to_linear_scale(val, param_scale[i])\n",
    "        elif prior_type[i] == 'parameterScaleNormal':\n",
    "            mean, std = petab_problem.parameter_df['objectivePriorParameters'].values[i].split(';')\n",
    "            lb_scaled_i = scale_values(lb[i], param_scale[i])\n",
    "            ub_scaled_i = scale_values(ub[i], param_scale[i])\n",
    "            a, b = (lb_scaled_i - float(mean)) / float(std), (ub_scaled_i - float(mean)) / float(std)\n",
    "            rv = stats.truncnorm.rvs(loc=float(mean), scale=float(std), a=a, b=b)\n",
    "            # scale to linear space\n",
    "            prior_dict[name] = values_to_linear_scale(rv, param_scale[i])\n",
    "        elif prior_type[i] == 'normal':\n",
    "            mean, std = petab_problem.parameter_df['objectivePriorParameters'].values[i].split(';')\n",
    "            a, b = (lb[i] - float(mean)) / float(std), (ub[i] - float(mean)) / float(std)\n",
    "            rv = stats.truncnorm.rvs(loc=float(mean), scale=float(std), a=a, b=b)\n",
    "            prior_dict[name] = rv\n",
    "        elif prior_type[i] == 'laplace':\n",
    "            loc, scale = petab_problem.parameter_df['objectivePriorParameters'].values[i].split(';')\n",
    "            for t in range(10):\n",
    "                rv = np.random.laplace(loc=float(loc), scale=float(scale))\n",
    "                if lb[i] <= rv <= ub[i]:  # sample from truncated laplace\n",
    "                    break\n",
    "            prior_dict[name] = rv\n",
    "        else:\n",
    "            raise ValueError(\"Unknown prior type:\", prior_type[i])\n",
    "        # scale params and make list\n",
    "        prior_dict[name] = np.array([scale_values(prior_dict[name], param_scale[i])])\n",
    "\n",
    "    # prepare variables for simulation\n",
    "    x = get_samples_from_dict(prior_dict)\n",
    "    prior_dict['amici_params'] = x  # scaled parameters for amici\n",
    "    return prior_dict\n",
    "\n",
    "def simulator_amici(amici_params, return_df=False):\n",
    "    pred = amici_predictor(amici_params)  # expect amici_params to be scaled\n",
    "    sim_df, failed = amici_pred_to_df(pred, amici_params,\n",
    "                                      factory=factory, petab_problem=petab_problem, pypesto_problem=pypesto_problem)\n",
    "    sim = amici_df_to_array(sim_df)\n",
    "    if failed:\n",
    "        sim = sim * np.nan  # set all to nan if simulation failed\n",
    "    if return_df:\n",
    "        return dict(sim_data=sim, sim_failed=failed, sim_data_df=sim_df)\n",
    "    return dict(sim_data=sim, sim_failed=failed)"
   ],
   "id": "d478fd3d90b5da5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior_sample = prior()\n",
    "test = simulator_amici(prior_sample['amici_params'], return_df=True)\n",
    "test['sim_data'].shape, prior_sample['amici_params'].shape, np.nansum(test['sim_data'])"
   ],
   "id": "815a2fa50947e5f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#test = simulator_amici(prior_sample['amici_params'], return_df=True)\n",
    "#compute_objective(petab_problem, test['sim_data_df'], prior_sample['amici_params'])"
   ],
   "id": "b8a009d17082137a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # plot prior\n",
    "# n_rows = len(pypesto_problem.x_names) // 6\n",
    "# n_cols = int(np.ceil(len(pypesto_problem.x_names) / n_rows))\n",
    "# fig, axs = plt.subplots(n_rows, n_cols, figsize=(2*n_rows, 2*n_cols), layout='constrained')\n",
    "# axs = axs.flatten()\n",
    "# samples = [prior() for i in range(1000)]\n",
    "# for i, name in enumerate(pypesto_problem.x_names):\n",
    "#     samples_i = np.array([s[name] for s in samples]).flatten()\n",
    "#     axs[i].hist(samples_i, density=True)\n",
    "#     axs[i].set_title(name)\n",
    "#     # axs[i].axvline(scale_values(petab_problem.parameter_df['nominalValue'][i],\n",
    "#     #                              petab_problem.parameter_df['parameterScale'][i]), color='red', linestyle='--')\n",
    "#     # axs[i].axvline(scale_values(petab_problem.parameter_df['lowerBound'][i],\n",
    "#     #                             petab_problem.parameter_df['parameterScale'][i]), color='blue', linestyle='--')\n",
    "#     # axs[i].axvline(scale_values(petab_problem.parameter_df['upperBound'][i],\n",
    "#     #                             petab_problem.parameter_df['parameterScale'][i]), color='blue', linestyle='--')\n",
    "# plt.show()"
   ],
   "id": "8b4598746c13ef4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_mcmc(petab_problem, data_df=None, n_optimization_starts=0, n_chains=10, n_samples=10000,\n",
    "             n_procs=10, use_mala=True, verbose=False, mix_start=0.75) -> Union[pypesto.result.Result, tuple[pypesto.result.Result, petab.Problem, pypesto.Problem]]:\n",
    "    if data_df is None:\n",
    "        # use true data\n",
    "        _pypesto_problem = create_pypesto_problem(petab_problem)\n",
    "        _petab_problem = None\n",
    "    else:\n",
    "        _measurement_df = data_df\n",
    "        if not 'measurement' in _measurement_df.columns:\n",
    "            _measurement_df['measurement'] = _measurement_df['simulation']  # pypesto expects measurement column\n",
    "        _pypesto_problem, _petab_problem = create_pypesto_problem(petab_problem, _measurement_df)\n",
    "\n",
    "    if n_optimization_starts == 0:\n",
    "        print(\"Skipping optimization, sample start points for chains from prior\")\n",
    "        _result = None\n",
    "        #x0 = [_pypesto_problem.get_reduced_vector(prior()['amici_params']) for _ in range(n_chains)]\n",
    "    else:\n",
    "        # do the optimization\n",
    "        _result = optimize.minimize(\n",
    "            problem=_pypesto_problem,\n",
    "            optimizer=optimize.FidesOptimizer(verbose=0),\n",
    "            #optimizer=optimize.ScipyOptimizer(method='L-BFGS-B'),\n",
    "            n_starts=n_optimization_starts,\n",
    "            engine=pypesto.engine.MultiProcessEngine(n_procs=n_procs) if n_procs > 1 else None,\n",
    "            progress_bar=verbose\n",
    "        )\n",
    "        #x0 = _pypesto_problem.get_reduced_vector(_result.optimize_result.x[0])\n",
    "        #x0 = [mix_start*np.array(x0[0]) + (1-mix_start)*_pypesto_problem.get_reduced_vector(prior()['amici_params'])]\n",
    "        #if x0[0] is None:\n",
    "        #    print(\"Warning: x0 contains nan, replace with prior sample\")\n",
    "        #    x0[0] = _pypesto_problem.get_reduced_vector(prior()['amici_params'])\n",
    "        #x0 += [_pypesto_problem.get_reduced_vector(prior()['amici_params']) for _ in range(n_chains - 1)]\n",
    "\n",
    "    _sampler = sample.AdaptiveParallelTemperingSampler(\n",
    "        #internal_sampler=sample.MalaSampler() if use_mala else sample.AdaptiveMetropolisSampler(),\n",
    "        internal_sampler=sample.AdaptiveMetropolisSampler(),\n",
    "        n_chains=n_chains,\n",
    "        options=dict(show_progress=verbose)\n",
    "    )\n",
    "\n",
    "    _result = sample.sample(\n",
    "        problem=_pypesto_problem,\n",
    "        n_samples=n_samples,\n",
    "        sampler=_sampler,\n",
    "        result=_result,\n",
    "        #x0=x0,\n",
    "    )\n",
    "    sample.geweke_test(_result)\n",
    "\n",
    "    if data_df is None:\n",
    "        return _result\n",
    "    return _result, _petab_problem, _pypesto_problem"
   ],
   "id": "f36f242366de68e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_mcmc_posterior_samples(res):\n",
    "    burn_in = sample.geweke_test(res)\n",
    "    if burn_in == res.sample_result.trace_x.shape[1]:\n",
    "        print(\"Warning: All samples are considered burn-in.\")\n",
    "        _samples = res.sample_result.trace_x[0]  # only use first chain\n",
    "    else:\n",
    "        _samples = res.sample_result.trace_x[0, burn_in:]  # only use first chain\n",
    "    #_samples = pypesto_problem.get_full_vector(_samples)\n",
    "    #scales = petab_problem.parameter_df.loc[res.problem.x_names, 'parameterScale'].values\n",
    "    #_samples = values_to_linear_scale(_samples, scales)\n",
    "    return _samples"
   ],
   "id": "74f951d5b884ee3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_optimization_starts = 1\n",
    "test_params = prior()\n",
    "test = simulator_amici(prior_sample['amici_params'], return_df=True)\n",
    "#print(test_params)\n",
    "new_result, new_petab_problem, new_pypesto_problem = run_mcmc(\n",
    "    petab_problem=petab_problem,\n",
    "    data_df=test['sim_data_df'],\n",
    "    n_optimization_starts=n_optimization_starts,\n",
    "    n_samples=1e2,\n",
    "    n_procs=n_cpus,\n",
    "    n_chains=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if n_optimization_starts > 0:\n",
    "    visualize.waterfall(new_result, size=(6, 4))\n",
    "    visualize.parameters(new_result, size=(6, 25))\n",
    "    sim_dict = visualize_optimized_model_fit(\n",
    "        petab_problem=new_petab_problem,\n",
    "        result=new_result,\n",
    "        pypesto_problem=new_pypesto_problem,\n",
    "        return_dict=True\n",
    "    )\n",
    "    print(test_params['amici_params']-new_result.optimize_result.x[0])\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=False, figsize=(10, 3), layout='constrained')\n",
    "    obs_name = ['Bac', 'Ind']\n",
    "    for i, obs_id in enumerate(sim_dict['simulation_df']['observableId'].unique()):\n",
    "        df_obs = sim_dict['simulation_df'][sim_dict['simulation_df']['observableId'] == obs_id]\n",
    "        cmap = plt.get_cmap('tab20', len(df_obs['simulationConditionId'].unique()))\n",
    "        for j, sim_con in enumerate(df_obs['simulationConditionId'].unique()):\n",
    "            color = cmap(j)\n",
    "            df = df_obs[df_obs['simulationConditionId'] == sim_con]\n",
    "            ax[i].plot(df['time'].values, df['simulation'].values[:, 0], 'o', color=color,\n",
    "                       markersize=0.7, label=f'Condition {j}')\n",
    "        ax[i].set_ylabel(f'{obs_name[i]} [a.u.]', fontsize=14)\n",
    "        ax[i].set_xlabel('Time [min]', fontsize=14)\n",
    "        ax[i].tick_params(axis='x', labelsize=12)\n",
    "        ax[i].spines['top'].set_visible(False)\n",
    "        ax[i].spines['right'].set_visible(False)\n",
    "    plt.savefig(f'plots/petab_benchmark_model_{problem_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "visualize.sampling_fval_traces(new_result)\n",
    "ax = visualize.sampling_parameter_traces(new_result)\n",
    "ax = ax.flatten()\n",
    "test_val = pypesto_problem.get_reduced_vector(test_params['amici_params'])\n",
    "for i in range(new_pypesto_problem.dim):\n",
    "    ax[i].axhline(test_val[i], color='red', linestyle='--')\n",
    "print(np.mean((test_val-np.median(get_mcmc_posterior_samples(new_result), axis=0))**2))"
   ],
   "id": "62ba1a556342abed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# n_optimization_starts = 20\n",
    "# result = run_mcmc(\n",
    "#     petab_problem=petab_problem,\n",
    "#     pypesto_problem=pypesto_problem,\n",
    "#     n_optimization_starts=n_optimization_starts,\n",
    "#     n_samples=1e3\n",
    "# )\n",
    "#\n",
    "# if n_optimization_starts > 0:\n",
    "#     visualize.waterfall(result, size=(6, 4))\n",
    "#     visualize.parameters(result, size=(6, 25))\n",
    "#     visualize_optimized_model_fit(petab_problem=petab_problem, result=result, pypesto_problem=pypesto_problem);"
   ],
   "id": "82b4a81a42d5475c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#ax = visualize.sampling_parameter_traces(result, size=(20, 20), full_trace=False, use_problem_bounds=False);\n",
    "#visualize.sampling_scatter(result, size=(13, 6));"
   ],
   "id": "8e7d158c84562a1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate training and validation data",
   "id": "9f8137a572fafeaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from model_settings import MODELS, NUM_SAMPLES_INFERENCE, load_model\n",
    "\n",
    "num_training_sets = 512 * 64\n",
    "num_validation_sets = 1000"
   ],
   "id": "2b3638846c2c2d4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@delayed\n",
    "def sample_and_simulate(return_df=False):\n",
    "    \"\"\"Single iteration of sampling and simulation\"\"\"\n",
    "    prior_sample = prior()\n",
    "    test = simulator_amici(prior_sample['amici_params'], return_df=return_df)\n",
    "\n",
    "    # Combine both dictionaries\n",
    "    result = {**prior_sample, **test}\n",
    "    return result\n",
    "\n",
    "\n",
    "def simulate_parallel(n_samples, return_df=False):\n",
    "    \"\"\"Parallel sampling and simulation\"\"\"\n",
    "    results = Parallel(n_jobs=n_cpus, verbose=100)(\n",
    "        sample_and_simulate(return_df) for _ in range(n_samples)\n",
    "    )\n",
    "    results_dict = defaultdict(list)\n",
    "\n",
    "    for r in results:\n",
    "        for key, value in r.items():\n",
    "            results_dict[key].append(value)\n",
    "    for key, value_list in results_dict.items():\n",
    "        if isinstance(value_list[0], pd.DataFrame):\n",
    "            pass\n",
    "        else:\n",
    "           results_dict[key] = np.array(value_list)\n",
    "    return results_dict"
   ],
   "id": "f83743725aca6688",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MCMC workflow",
   "id": "d5a018bb92514dd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.exists(f\"{storage}validation_data_petab_{problem_name}.pkl\"):\n",
    "    #with open(f'{storage}validation_data_petab_{problem_name}.pkl', 'rb') as f:\n",
    "    #todo:    validation_data = pickle.load(f)\n",
    "    with open(f'{storage}validation_data_petab_{problem_name}_100.pkl', 'rb') as f:\n",
    "        validation_data = pickle.load(f)\n",
    "    try:\n",
    "        with open(f'{storage}training_data_petab_{problem_name}.pkl', 'rb') as f:\n",
    "            training_data = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        training_data = None\n",
    "        print(\"Training data not found\")\n",
    "else:\n",
    "    training_data = simulate_parallel(num_training_sets)\n",
    "    validation_data = simulate_parallel(num_validation_sets, return_df=True)\n",
    "\n",
    "    with open(f'{storage}training_data_petab_{problem_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(training_data, f)\n",
    "    with open(f'{storage}validation_data_petab_{problem_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(validation_data, f)\n",
    "\n",
    "# # remove failed simulations\n",
    "# if not training_data is None:\n",
    "#     train_mask = ~training_data['sim_failed']\n",
    "#     for key in training_data.keys():\n",
    "#         training_data[key] = training_data[key][train_mask]\n",
    "#     print(f\"Failed Training data: {np.sum(~train_mask)} / {len(train_mask)}\")\n",
    "# val_mask = ~validation_data['sim_failed']\n",
    "# for key in validation_data.keys():\n",
    "#     if key == 'sim_data_df':\n",
    "#         continue\n",
    "#     validation_data[key] = validation_data[key][val_mask]\n",
    "# print(f\"Failed Validation data: {np.sum(~val_mask)} / {len(val_mask)}\")\n",
    "\n",
    "test_mean = np.nanmean(np.log(validation_data['sim_data'] + 1), axis=(0, 1), keepdims=True)\n",
    "test_std = np.nanstd(np.log(validation_data['sim_data'] + 1), axis=(0, 1), keepdims=True)\n",
    "print(validation_data['sim_data'].shape)\n",
    "\n",
    "param_names = [name for i, name in enumerate(pypesto_problem.x_names) if i in pypesto_problem.x_free_indices]\n",
    "lbs = np.array([lb for i, lb in enumerate(petab_problem.lb_scaled) if i in pypesto_problem.x_free_indices])\n",
    "ubs = np.array([ub for i, ub in enumerate(petab_problem.ub_scaled) if i in pypesto_problem.x_free_indices])"
   ],
   "id": "54b5536ddb5de956",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_mcmc_single(petab_prob, pypesto_prob, sim_data_df, n_starts,\n",
    "                    n_mcmc_samples, n_final_samples, n_chains, use_mala):\n",
    "    import amici\n",
    "    import logging\n",
    "    amici.swig_wrappers.logger.setLevel(logging.CRITICAL)\n",
    "    pypesto.logging.log(level=logging.ERROR, name=\"pypesto.petab\", console=True)\n",
    "\n",
    "    if all(np.isnan(sim_data_df['simulation'])):\n",
    "        return np.full((n_final_samples, len(pypesto_prob.x_free_indices)), np.nan)\n",
    "\n",
    "    r, _, _ = run_mcmc(\n",
    "        petab_problem=petab_prob,\n",
    "        data_df=sim_data_df,\n",
    "        n_optimization_starts=n_starts,\n",
    "        n_samples=n_mcmc_samples,\n",
    "        n_chains=n_chains,\n",
    "        n_procs=1,\n",
    "        use_mala=use_mala\n",
    "    )\n",
    "\n",
    "    if r is None:\n",
    "        return np.full((n_final_samples, len(pypesto_prob.x_free_indices)), np.nan)\n",
    "\n",
    "    ps = get_mcmc_posterior_samples(r)\n",
    "    # num_samples random samples from posterior\n",
    "    idx = np.random.choice(ps.shape[0], size=n_final_samples)\n",
    "    return ps[idx]"
   ],
   "id": "3dceab2c5591867",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mcmc_path = f'{storage}mcmc_samples_{problem_name}{\"_mala\" if USE_MALA else \"\"}.pkl'\n",
    "if os.path.exists(mcmc_path):\n",
    "    with open(mcmc_path, 'rb') as f:\n",
    "        mcmc_posterior_samples = pickle.load(f)\n",
    "else:\n",
    "    mcmc_posterior_samples = Parallel(n_jobs=n_cpus, verbose=10)(\n",
    "        delayed(run_mcmc_single)(\n",
    "            petab_prob=petab_problem,\n",
    "            pypesto_prob=pypesto_problem,\n",
    "            sim_data_df=sim_data_df,\n",
    "            n_starts=10,\n",
    "            n_mcmc_samples=1e5,\n",
    "            n_final_samples=1000,\n",
    "            n_chains=5,\n",
    "            use_mala=USE_MALA\n",
    "        ) for sim_data_df in validation_data['sim_data_df']\n",
    "    )\n",
    "    mcmc_posterior_samples = np.array(mcmc_posterior_samples)\n",
    "\n",
    "    with open(mcmc_path, 'wb') as f:\n",
    "        pickle.dump(mcmc_posterior_samples, f)\n",
    "mcmc_mask = ~np.isnan(mcmc_posterior_samples.sum(axis=(1, 2)))\n",
    "mcmc_posterior_samples_test = mcmc_posterior_samples"
   ],
   "id": "4b1c5b1e5a1797a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.recovery(\n",
    "    estimates=mcmc_posterior_samples_test[mcmc_mask],\n",
    "    targets=pypesto_problem.get_reduced_vector(validation_data['amici_params'].T).T[mcmc_mask],\n",
    "    variable_names=param_names,\n",
    ")\n",
    "#fig.savefig(f\"{storage}petab_benchmark_{problem_name}_mcmc_recovery.png\")\n",
    "\n",
    "fig = bf.diagnostics.calibration_ecdf(\n",
    "    estimates=mcmc_posterior_samples_test[mcmc_mask],\n",
    "    targets=pypesto_problem.get_reduced_vector(validation_data['amici_params'].T).T[mcmc_mask],\n",
    "    variable_names=param_names,\n",
    "    difference=True,\n",
    "    stacked=True\n",
    ")\n",
    "#fig.savefig(f\"{storage}petab_benchmark_{problem_name}_mcmc_calibration.png\")"
   ],
   "id": "4ec482955602ce23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BayesFlow workflow",
   "id": "2471c34692a6d37c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .drop('amici_params')  # only used for simulation\n",
    "    .drop('sim_data_df')\n",
    "    .to_array()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .concatenate(param_names, into=\"inference_variables\")\n",
    "    .constrain(\"inference_variables\", lower=lbs, upper=ubs, inclusive='both')  # after concatenate such that we can apply an array as constraint\n",
    "\n",
    "    .as_time_series(\"sim_data\")\n",
    "    .log(\"sim_data\", p1=True)\n",
    "    #.standardize(\"sim_data\", mean=test_mean, std=test_std)\n",
    "    #.nan_to_num(\"sim_data\", default_value=-3.0)\n",
    "    .rename(\"sim_data\", \"summary_variables\")\n",
    ")"
   ],
   "id": "84f74d0e7c75651a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # check how the distributions look like\n",
    "# test_params = adapter.forward(validation_data)['inference_variables']\n",
    "#\n",
    "# n_rows = len(param_names) // 6\n",
    "# n_cols = int(np.ceil(len(param_names) / n_rows))\n",
    "# fig, ax = plt.subplots(n_rows, n_cols, figsize=(2*n_rows, 2*n_cols), layout='constrained')\n",
    "# ax = ax.flatten()\n",
    "# for i, name in enumerate(param_names):\n",
    "#     samples = test_params[:, i]\n",
    "#     ax[i].hist(samples, density=True)\n",
    "#     ax[i].set_title(name)\n",
    "# plt.show()"
   ],
   "id": "6d79fc2eac8185eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check how the data distribution looks like (disable nan_to_num in adapter to see nans)\n",
    "_test_data = adapter.forward(validation_data)['summary_variables']\n",
    "# n_features = _test_data.shape[-1]\n",
    "#\n",
    "# n_rows = n_features // 5\n",
    "# n_cols = int(np.ceil(n_features / n_rows))\n",
    "# fig, ax = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(2*n_rows, 2*n_cols))\n",
    "# ax = ax.flatten()\n",
    "# for i in range(n_features):\n",
    "#     ax[i].hist(_test_data[:, :, i].flatten(), density=True)\n",
    "# plt.show()"
   ],
   "id": "d13f79cb2d51a9d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print some stats about the data\n",
    "print('Minimum', np.min(_test_data))\n",
    "print('Maximum', np.max(_test_data))\n",
    "print('Mean', np.mean(_test_data))\n",
    "print('Standard Deviation', np.std(_test_data))\n",
    "print('Nan Values', np.isnan(_test_data).sum())"
   ],
   "id": "db94aa20d5ed36c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_name = list(MODELS.keys())[4]\n",
    "conf_tuple = MODELS[model_name]\n",
    "print(model_name)"
   ],
   "id": "6a0293b5f73232bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "workflow = load_model(adapter=adapter, conf_tuple=conf_tuple, param_names=param_names,\n",
    "                      training_data=validation_data, #todo: training_data,\n",
    "                      #use_ema=True,\n",
    "                      validation_data=validation_data, storage=f'{storage}models/',\n",
    "                      problem_name=problem_name, model_name=model_name)"
   ],
   "id": "8567adc22e0fbc09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics_plots = workflow.plot_default_diagnostics(test_data=validation_data,\n",
    "                                                      num_samples=10, #NUM_SAMPLES_INFERENCE,\n",
    "                                                      approximator_kwargs=dict(rho=7, steps=30),\n",
    "                                                      calibration_ecdf_kwargs={\"difference\": True, 'stacked': True})\n",
    "#for k in diagnostics_plots.keys():\n",
    "#    diagnostics_plots[k].savefig(f\"{storage}petab_benchmark_{problem_name}_{model_name}_{k}.png\")"
   ],
   "id": "3894672c0c7ed821",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics_test = workflow.compute_default_diagnostics(test_data=validation_data,\n",
    "                                     num_samples=100, #NUM_SAMPLES_INFERENCE,\n",
    "                                     approximator_kwargs=dict(rho=7, steps=30)\n",
    "                                    )\n",
    "metrics_test.mean(axis=1)"
   ],
   "id": "50cd33188215d2ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics_test = workflow.compute_default_diagnostics(test_data=validation_data,\n",
    "                                     num_samples=100, #NUM_SAMPLES_INFERENCE,\n",
    "                                     approximator_kwargs=dict(rho=3.5, steps=15)\n",
    "                                    )\n",
    "metrics_test.mean(axis=1)"
   ],
   "id": "ccd86f552480a590",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compare MCMC and Diffusion models",
   "id": "ebb1651065f52dac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from bayesflow.diagnostics.metrics import root_mean_squared_error, posterior_contraction, calibration_error, classifier_two_sample_test\n",
    "from visualize_results import plot_model_comparison_grid\n",
    "from petab_helper import compute_likelihood_parallel\n",
    "\n",
    "from scipy.stats import median_abs_deviation"
   ],
   "id": "823a2e0f8a48fbe4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_data = {}\n",
    "for key, values in validation_data.items():\n",
    "    if key == 'sim_data_df':\n",
    "        test_data[key] = [v for i, v in enumerate(values) if mcmc_mask[i]]\n",
    "    else:\n",
    "        test_data[key] = values[mcmc_mask]\n",
    "len(test_data['sim_data_df'])"
   ],
   "id": "29eec708cd99c486",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "no_ema = False  # set to True, to get the noema model to validate training",
   "id": "e5ff1ba2d8e89bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# add mcmc as baseline\n",
    "if os.path.exists(f'{storage}metrics/petab_benchmark_{problem_name}_mcmc_{\"_mala\" if USE_MALA else \"\"}metrics.csv'):\n",
    "    with open(f'{storage}metrics/petab_benchmark_{problem_name}_mcmc_{\"_mala\" if USE_MALA else \"\"}metrics.csv', 'rb') as f:\n",
    "        mcmc_df = pd.read_csv(f, index_col=0)\n",
    "else:\n",
    "    print('Adding MCMC metrics')\n",
    "    test_targets = get_samples_from_dict(test_data)\n",
    "\n",
    "    rand_idx = np.random.choice(mcmc_posterior_samples_test.shape[1])\n",
    "    workflow_samples_aug = compute_likelihood_parallel(petab_problem, mcmc_posterior_samples_test[mcmc_mask, rand_idx],\n",
    "                                                       test_data, n_jobs=n_cpus)\n",
    "\n",
    "    # augment test data\n",
    "    test_data_aug = compute_likelihood_parallel(petab_problem, test_data['amici_params'], test_data,\n",
    "                                                n_jobs=n_cpus)\n",
    "\n",
    "    workflow_samples_aug = workflow_samples_aug[~np.isnan(workflow_samples_aug).any(axis=1)]\n",
    "    test_data_aug = test_data_aug[~np.isnan(test_data_aug).any(axis=1)]\n",
    "    print(f\"{workflow_samples_aug.shape[0]} workflow samples and {test_data_aug.shape[0]} test data samples.\")\n",
    "\n",
    "    mcmc_df = pd.DataFrame([{\n",
    "        'model': 'MCMC',\n",
    "        'sampler': 'MCMC',\n",
    "        'nrmse': root_mean_squared_error(\n",
    "            mcmc_posterior_samples_test[mcmc_mask], test_targets, aggregation=np.nanmedian\n",
    "        )['values'].mean(),\n",
    "        'nrmse_mad': root_mean_squared_error(\n",
    "            mcmc_posterior_samples_test[mcmc_mask], test_targets, aggregation=median_abs_deviation\n",
    "        )['values'].mean(),\n",
    "        'posterior_contraction': posterior_contraction(\n",
    "            mcmc_posterior_samples_test[mcmc_mask], test_targets, aggregation=np.nanmedian\n",
    "        )['values'].mean(),\n",
    "        'posterior_contraction_mad': posterior_contraction(\n",
    "            mcmc_posterior_samples_test[mcmc_mask], test_targets, aggregation=median_abs_deviation\n",
    "        )['values'].mean(),\n",
    "        'posterior_calibration_error': calibration_error(\n",
    "            mcmc_posterior_samples_test[mcmc_mask], test_targets, aggregation=np.nanmedian\n",
    "        )['values'].mean(),\n",
    "        'posterior_calibration_error_mad': calibration_error(\n",
    "            mcmc_posterior_samples_test[mcmc_mask], test_targets, aggregation=median_abs_deviation\n",
    "        )['values'].mean(),\n",
    "        'c2st': classifier_two_sample_test(workflow_samples_aug, test_data_aug, mlp_widths=(128, 128, 128),\n",
    "                                           validation_split=0.25)\n",
    "    }], index=[0])\n",
    "    with open(f'{storage}metrics/petab_benchmark_{problem_name}_mcmc_{\"_mala\" if USE_MALA else \"\"}metrics.csv', 'wb') as f:\n",
    "        mcmc_df.to_csv(f)"
   ],
   "id": "586c1f924e40b7a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for fusion_transformer_summary in [False, True]:\n",
    "    metrics = []\n",
    "    for i in range(len(MODELS)):\n",
    "        if fusion_transformer_summary and not 'ft' in list(MODELS.keys())[i]:\n",
    "            continue\n",
    "        elif not fusion_transformer_summary and 'ft' in list(MODELS.keys())[i]:\n",
    "            continue\n",
    "        if os.path.exists(f'{storage}metrics/petab_benchmark_{problem_name}_metrics_{i}.pkl'):\n",
    "            if no_ema and 'ema' in list(MODELS.keys())[i]:\n",
    "                with open(f'{storage}metrics/petab_benchmark_{problem_name}_metrics_{i}_noema.pkl', 'rb') as f:\n",
    "                    metric = pickle.load(f)\n",
    "            else:\n",
    "                with open(f'{storage}metrics/petab_benchmark_{problem_name}_metrics_{i}.pkl', 'rb') as f:\n",
    "                    metric = pickle.load(f)\n",
    "            metrics += metric\n",
    "        else:\n",
    "            print(f\"Metrics for model {list(MODELS.keys())[i]} not found\")\n",
    "\n",
    "    # df, all columns to float beside model and sampler\n",
    "    metrics_df = pd.DataFrame(metrics, index=None)\n",
    "    metrics_df.index.name = None\n",
    "    for col in ['nrmse', 'posterior_contraction', 'posterior_calibration_error', 'c2st']:\n",
    "        metrics_df[col] = metrics_df[col].astype(float)\n",
    "        metrics_df.loc[metrics_df[col].isna(), col] = 1\n",
    "    metrics_df = pd.concat([metrics_df, mcmc_df], ignore_index=True)\n",
    "\n",
    "    metrics_df.loc[metrics_df['nrmse'] == 1.0, 'posterior_contraction'] = np.nan  # samples were nan\n",
    "    metrics_df.loc[metrics_df['nrmse'] == 1.0, 'posterior_contraction_mad'] = np.nan  # samples were nan\n",
    "    metrics_df.loc[metrics_df['nrmse'] == 1.0, 'nrmse'] = np.nan  # samples were nan\n",
    "    metrics_df.loc[metrics_df['nrmse'] == 1.0, 'nrmse_mad'] = np.nan  # samples were nan\n",
    "    metrics_df = metrics_df[metrics_df['sampler'] != 'sde-pc']\n",
    "    metrics_df['c2st'] = 0.5+np.abs(metrics_df['c2st']-0.5)\n",
    "    metrics_df['rank'] = metrics_df['nrmse'] + metrics_df['posterior_calibration_error']\n",
    "\n",
    "    if fusion_transformer_summary:\n",
    "        metrics_df_2 = metrics_df.copy()\n",
    "    else:\n",
    "        metrics_df_1 = metrics_df.copy()\n",
    "        metrics_df_joint = metrics_df.copy()\n",
    "\n",
    "    display(metrics_df.sort_values(by='rank', inplace=False))"
   ],
   "id": "316dd05f86c816ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# order from df1\n",
    "order_map = (\n",
    "    metrics_df_1[[\"model\", \"sampler\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .assign(order=lambda x: range(len(x)))\n",
    "    .rename(columns={\"model\": \"model_key\"})\n",
    ")\n",
    "\n",
    "# add model_key\n",
    "df1 = metrics_df_1.copy()\n",
    "df1[\"model_key\"] = df1[\"model\"]\n",
    "\n",
    "df2 = metrics_df_2.copy()\n",
    "df2[\"model_key\"] = (\n",
    "    df2[\"model\"]\n",
    "    .str.replace(r\"_ft_ema$\", \"_ema\", regex=True)\n",
    "    .str.replace(r\"_ft$\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# combine\n",
    "merged = pd.concat([df1, df2], ignore_index=True, sort=False)\n",
    "\n",
    "# aggregate means ignoring NaN\n",
    "agg = (\n",
    "    merged.groupby([\"model_key\", \"sampler\"], dropna=False, as_index=False)\n",
    "    .mean(numeric_only=True)\n",
    ")\n",
    "\n",
    "# choose display model name\n",
    "display_names = df1[[\"model_key\", \"sampler\", \"model\"]].drop_duplicates()\n",
    "metrics_df_joint = agg.merge(display_names, on=[\"model_key\", \"sampler\"], how=\"left\")\n",
    "metrics_df_joint[\"model\"] = metrics_df_joint[\"model\"].fillna(metrics_df_joint[\"model_key\"])\n",
    "\n",
    "# apply df1 order, unseen pairs go last\n",
    "metrics_df_joint = (\n",
    "    metrics_df_joint.merge(order_map, on=[\"model_key\", \"sampler\"], how=\"left\")\n",
    "          .sort_values([\"order\"], na_position=\"last\", kind=\"stable\")\n",
    "          .drop(columns=[\"order\", \"model_key\"])\n",
    "          .reset_index(drop=True)\n",
    ")\n",
    "# reorder columns so model and sampler come first\n",
    "cols = [\"model\", \"sampler\"] + [c for c in metrics_df_joint.columns if c not in [\"model\", \"sampler\"]]\n",
    "metrics_df_joint = metrics_df_joint[cols]\n",
    "metrics_df_joint['rank'] = metrics_df_joint['nrmse'] + metrics_df_joint['posterior_calibration_error']\n",
    "metrics_df_joint.sort_values(by='rank', inplace=False, ignore_index=True)"
   ],
   "id": "8d528bb702b7b76e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.corrcoef(metrics_df_joint[['posterior_calibration_error', 'c2st']].values.T)",
   "id": "741f911b02660093",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#plot_model_comparison_radar(metrics_df_joint, group_by_sampler=True, save_path=f'plots')\n",
    "#plot_model_comparison_radar(metrics_df_joint, group_by_method='diffusion', save_path=f'plots')"
   ],
   "id": "f7dac60ca021a570",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_model_comparison_grid(metrics_df_joint, save_path='plots')",
   "id": "4064b8612f0403c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "de98938133d31bf1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
