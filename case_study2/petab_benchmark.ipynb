{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# PEtab benchmark model with BayesFlow",
   "id": "f2db3aaca617dd83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pip install git+https://github.com/Benchmarking-Initiative/Benchmark-Models-PEtab.git@master#subdirectory=src/python\n",
    "# pypesto, amici, petab, fides, joblib"
   ],
   "id": "c4c3a5bf38839491",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "else:\n",
    "    print(f\"Using '{os.environ['KERAS_BACKEND']}' backend\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "from typing import Union\n",
    "from collections import defaultdict\n",
    "\n",
    "import benchmark_models_petab as benchmark_models\n",
    "import petab\n",
    "import pypesto.optimize as optimize\n",
    "import pypesto.sample as sample\n",
    "import pypesto.petab\n",
    "import pypesto.visualize as visualize\n",
    "from pypesto.visualize.model_fit import visualize_optimized_model_fit\n",
    "from scipy import stats\n",
    "\n",
    "import keras\n",
    "import bayesflow as bf\n",
    "\n",
    "import amici\n",
    "import logging\n",
    "amici.swig_wrappers.logger.setLevel(logging.CRITICAL)\n",
    "pypesto.logging.log(level=logging.ERROR, name=\"pypesto.petab\", console=True)\n",
    "\n",
    "from petab_helper import scale_values, values_to_linear_scale, amici_pred_to_array, apply_noise_to_data\n",
    "\n",
    "# print all model names\n",
    "print(benchmark_models.MODELS)"
   ],
   "id": "f7d1d9565944bee9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# generate petab problem\n",
    "#job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0))\n",
    "n_cpus = 10 #int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",
    "problem_name = \"Beer_MolBioSystems2014\" #\"Raimundez_PCB2020\", \"Beer_MolBioSystems2014\", \"Boehm_JProteomeRes2014\"\n",
    "storage = '' # f'plots/{problem_name}/'\n",
    "petab_problem = benchmark_models.get_problem(problem_name)\n",
    "\n",
    "# decrease upper bounds for offset, scaling and noise parameters\n",
    "scale_params_id = [name for name in petab_problem.parameter_df.index.values if name[:6] == 'offset' or name[:5] == 'scale']\n",
    "petab_problem.parameter_df.loc[scale_params_id, 'upperBound'] = 100  # instead of 1000\n",
    "sd_params_id = [name for name in petab_problem.parameter_df.index.values if name[:3] == 'sd_']\n",
    "petab_problem.parameter_df.loc[sd_params_id, 'upperBound'] = 10  # instead of 1000\n",
    "\n",
    "if problem_name == \"Raimundez_PCB2020\":\n",
    "    # Elba added normal priors for the scaling params\n",
    "    scale_params_id = [name for name in petab_problem.parameter_df.index.values if name[:2] == 's_']\n",
    "    petab_problem.parameter_df.loc[scale_params_id, 'objectivePriorType'] = \"normal\"\n",
    "    petab_problem.parameter_df.loc[scale_params_id, 'objectivePriorParameters'] = \"1;10\"\n",
    "    petab_problem.parameter_df.loc[scale_params_id, 'parameterScale'] = \"lin\"\n",
    "\n",
    "\n",
    "for i, row in petab_problem.parameter_df.iterrows():\n",
    "    if 'objectivePriorType' in row and not pd.isna(row['objectivePriorType']):\n",
    "        if row['estimate'] == 0:\n",
    "            print(f\"Parameter {i} has a {row['objectivePriorType']} prior but is not estimated, setting to nan\")\n",
    "            petab_problem.parameter_df.loc[i, 'objectivePriorType'] = np.nan\n",
    "        # validate petab problem, if scale for parameter is defined, prior must be on the same scale\n",
    "        if row['parameterScale'] != 'lin' and not row['objectivePriorType'].startswith('parameterScale'):\n",
    "            raise ValueError(f\"Parameter {i} has parameterScale {row['parameterScale']} but {row['objectivePriorType']} prior\")\n",
    "\n",
    "# load problem\n",
    "importer = pypesto.petab.PetabImporter(petab_problem, simulator_type=\"amici\")\n",
    "factory = importer.create_objective_creator()\n",
    "\n",
    "model = factory.create_model(verbose=False)\n",
    "amici_predictor = factory.create_predictor()\n",
    "amici_predictor.amici_objective.amici_solver.setAbsoluteTolerance(1e-8)\n",
    "\n",
    "# Creating the pypesto problem from PEtab\n",
    "pypesto_problem = importer.create_problem(\n",
    "    startpoint_kwargs={\"check_fval\": True, \"check_grad\": True}\n",
    ")"
   ],
   "id": "6bc21d5dc9ed9ad7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prior():\n",
    "    lb = petab_problem.parameter_df['lowerBound'].values\n",
    "    ub = petab_problem.parameter_df['upperBound'].values\n",
    "    param_names_id = petab_problem.parameter_df.index.values\n",
    "    param_scale = petab_problem.parameter_df['parameterScale'].values\n",
    "    if 'objectivePriorType' in petab_problem.parameter_df.columns:\n",
    "        prior_type = petab_problem.parameter_df['objectivePriorType'].values\n",
    "    else:\n",
    "        prior_type = [np.nan] * len(param_names_id)\n",
    "    estimate_param = petab_problem.parameter_df['estimate'].values\n",
    "\n",
    "    prior_dict = {}\n",
    "    for i, name in enumerate(param_names_id):\n",
    "        if estimate_param[i] == 0:\n",
    "            prior_dict[name] = petab_problem.parameter_df['nominalValue'].values[i]  # linear space\n",
    "        elif prior_type[i] == 'uniform':  # linear space\n",
    "            prior_dict[name] = np.random.uniform(low=lb[i], high=ub[i])\n",
    "        elif prior_type[i] == 'parameterScaleUniform' or pd.isna(prior_type[i]):\n",
    "            # scale bounds to scaled space\n",
    "            val = np.random.uniform(low=scale_values(lb[i], param_scale[i]), high=scale_values(ub[i], param_scale[i]))\n",
    "            # scale to linear space\n",
    "            prior_dict[name] = values_to_linear_scale(val, param_scale[i])\n",
    "        elif prior_type[i] == 'parameterScaleNormal':\n",
    "            mean, std = petab_problem.parameter_df['objectivePriorParameters'].values[i].split(';')\n",
    "            val = np.random.normal(float(mean), float(std))\n",
    "            # scale to linear space\n",
    "            prior_dict[name] = values_to_linear_scale(val, param_scale[i])\n",
    "        elif prior_type[i] == 'normal':\n",
    "            mean, std = petab_problem.parameter_df['objectivePriorParameters'].values[i].split(';')\n",
    "            a, b = (lb[i] - float(mean)) / float(std), (ub[i] - float(mean)) / float(std)\n",
    "            for t in range(10):\n",
    "                rv = stats.truncnorm.rvs(a=a, b=b)\n",
    "                if lb[i] <= rv <= ub[i]:  # sample from truncated normal (sometimes it fails and then we try again)\n",
    "                    break\n",
    "            prior_dict[name] = rv\n",
    "        elif prior_type[i] == 'laplace':\n",
    "            loc, scale = petab_problem.parameter_df['objectivePriorParameters'].values[i].split(';')\n",
    "            for t in range(10):\n",
    "                rv = np.random.laplace(loc=float(loc), scale=float(scale))\n",
    "                if lb[i] <= rv <= ub[i]:  # sample from truncated laplace\n",
    "                    break\n",
    "            prior_dict[name] = rv\n",
    "        else:\n",
    "            raise ValueError(\"Unknown prior type:\", prior_type[i])\n",
    "        # scale params and make list\n",
    "        prior_dict[name] = np.array([scale_values(prior_dict[name], param_scale[i])])\n",
    "\n",
    "    # prepare variables for simulation\n",
    "    x = np.array([prior_dict[name][0] for name in pypesto_problem.x_names])\n",
    "    prior_dict['amici_params'] = x  # scaled parameters for amici\n",
    "    return prior_dict\n",
    "\n",
    "def simulator_amici(amici_params):\n",
    "    pred = amici_predictor(amici_params)  # expect amici_params to be scaled\n",
    "    sim, failed = amici_pred_to_array(pred, amici_params,\n",
    "                                      factory=factory, petab_problem=petab_problem, pypesto_problem=pypesto_problem)\n",
    "    return dict(sim_data=sim, sim_failed=failed)"
   ],
   "id": "d478fd3d90b5da5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior_sample = prior()\n",
    "test = simulator_amici(prior_sample['amici_params'])\n",
    "test['sim_data'].shape, prior_sample['amici_params'].shape, np.nansum(test['sim_data'])"
   ],
   "id": "815a2fa50947e5f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # plot prior\n",
    "# n_rows = len(pypesto_problem.x_names) // 2\n",
    "# n_cols = int(np.ceil(len(pypesto_problem.x_names) / n_rows))\n",
    "# fig, axs = plt.subplots(n_rows, n_cols, figsize=(2*n_rows, 2*n_cols), layout='constrained')\n",
    "# axs = axs.flatten()\n",
    "# samples = [prior() for i in range(1000)]\n",
    "# for i, name in enumerate(pypesto_problem.x_names):\n",
    "#     samples_i = np.array([s[name] for s in samples]).flatten()\n",
    "#     axs[i].hist(samples_i, density=True)\n",
    "#     axs[i].set_title(name)\n",
    "# plt.show()"
   ],
   "id": "8b4598746c13ef4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_mcmc(petab_problem, pypesto_problem, true_params=None, n_optimization_starts=0, n_chains=10, n_samples=10000,\n",
    "             n_procs=10, verbose=False) -> Union[pypesto.result.Result, tuple[pypesto.result.Result, petab.Problem, pypesto.Problem]]:\n",
    "    _petab_problem = deepcopy(petab_problem)\n",
    "    if true_params is None:\n",
    "        # use true data\n",
    "        pass\n",
    "    else:\n",
    "        # this is needed to create a new measurement df and recompile the problem for amici\n",
    "        pred = amici_predictor(true_params)\n",
    "        _, failed = amici_pred_to_array(pred, true_params,\n",
    "                                      factory=factory, petab_problem=petab_problem, pypesto_problem=pypesto_problem)\n",
    "        if failed:\n",
    "            print(\"Simulation failed for true parameters\")\n",
    "            return None, None, None\n",
    "        _measurement_df = factory.prediction_to_petab_measurement_df(pred) # to create new measurement df\n",
    "        _measurement_df = apply_noise_to_data(_measurement_df, true_params, field='measurement',\n",
    "                                              pypesto_problem=pypesto_problem, petab_problem=_petab_problem)\n",
    "        _petab_problem.measurement_df = _measurement_df\n",
    "    _importer = pypesto.petab.PetabImporter(_petab_problem, simulator_type=\"amici\")\n",
    "    _factory = _importer.create_objective_creator()\n",
    "    _model = _factory.create_model(verbose=False)\n",
    "\n",
    "    _pypesto_problem = _importer.create_problem(\n",
    "        startpoint_kwargs={\"check_fval\": True, \"check_grad\": True}\n",
    "    )\n",
    "\n",
    "    if isinstance(_pypesto_problem.objective, pypesto.objective.AggregatedObjective):\n",
    "        _pypesto_problem.objective._objectives[0].amici_solver.setAbsoluteTolerance(1e-8)\n",
    "        #_pypesto_problem.objective._objectives[0].amici_solver.setSensitivityMethod(amici.SensitivityMethod.adjoint)\n",
    "    else:\n",
    "        _pypesto_problem.objective.amici_solver.setAbsoluteTolerance(1e-8)\n",
    "        #_pypesto_problem.objective.amici_solver.setSensitivityMethod(amici.SensitivityMethod.adjoint)\n",
    "\n",
    "    if n_optimization_starts == 0:\n",
    "        print(\"Skipping optimization, sample start points for chains from prior\")\n",
    "        _result = None\n",
    "        x0 = [_pypesto_problem.get_reduced_vector(prior()['amici_params']) for _ in range(n_chains)]\n",
    "    else:\n",
    "        # do the optimization\n",
    "        _result = optimize.minimize(\n",
    "            problem=_pypesto_problem,\n",
    "            optimizer=optimize.FidesOptimizer(verbose=0),\n",
    "            #optimizer=optimize.ScipyOptimizer(method='L-BFGS-B'),\n",
    "            n_starts=n_optimization_starts,\n",
    "            engine=pypesto.engine.MultiProcessEngine(n_procs=n_procs) if n_procs > 1 else None,\n",
    "            progress_bar=verbose\n",
    "        )\n",
    "        x0 = [_pypesto_problem.get_reduced_vector(_result.optimize_result.x[i_c]) for i_c in range(n_optimization_starts)]\n",
    "        x0 = x0[:n_chains]  # use only as many as we have chains\n",
    "        if n_optimization_starts < n_chains:\n",
    "            x0 += [_pypesto_problem.get_reduced_vector(prior()['amici_params']) for _ in range(n_chains - n_optimization_starts)]\n",
    "        else:\n",
    "            # add at least one prior sample\n",
    "            x0[-1] = _pypesto_problem.get_reduced_vector(prior()['amici_params'])\n",
    "        # check each x0 if nan\n",
    "        for x_i in range(len(x0)):\n",
    "            if x0[x_i] is None:\n",
    "                print(\"Warning: x0 contains nan, replace with prior sample\")\n",
    "                x0[x_i] = _pypesto_problem.get_reduced_vector(prior()['amici_params'])\n",
    "\n",
    "    _sampler = sample.AdaptiveParallelTemperingSampler(\n",
    "        internal_sampler=sample.AdaptiveMetropolisSampler(),\n",
    "        #internal_sampler=sample.MetropolisSampler(),\n",
    "        n_chains=n_chains,\n",
    "        options=dict(show_progress=verbose)\n",
    "    )\n",
    "    #_sampler = sample.AdaptiveMetropolisSampler()\n",
    "    #x0 = x0[0]\n",
    "\n",
    "    _result = sample.sample(\n",
    "        problem=_pypesto_problem,\n",
    "        n_samples=n_samples,\n",
    "        sampler=_sampler,\n",
    "        result=_result,\n",
    "        x0=x0\n",
    "    )\n",
    "    sample.geweke_test(_result)\n",
    "\n",
    "    if true_params is None:\n",
    "        return _result\n",
    "    return _result, _petab_problem, _pypesto_problem"
   ],
   "id": "39067b9bf1fc3e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_mcmc_posterior_samples(res):\n",
    "    burn_in = sample.geweke_test(res)\n",
    "    if burn_in == res.sample_result.trace_x.shape[1]:\n",
    "        print(\"Warning: All samples are considered burn-in.\")\n",
    "        _samples = res.sample_result.trace_x[0]  # only use first chain\n",
    "    else:\n",
    "        _samples = res.sample_result.trace_x[0, burn_in:]  # only use first chain\n",
    "    #_samples = pypesto_problem.get_full_vector(_samples)\n",
    "    #scales = petab_problem.parameter_df.loc[res.problem.x_names, 'parameterScale'].values\n",
    "    #_samples = values_to_linear_scale(_samples, scales)\n",
    "    return _samples"
   ],
   "id": "74f951d5b884ee3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_optimization_starts = 10\n",
    "test_params = prior()\n",
    "#print(test_params)\n",
    "new_result, new_petab_problem, new_pypesto_problem = run_mcmc(\n",
    "    petab_problem=petab_problem,\n",
    "    pypesto_problem=pypesto_problem,\n",
    "    true_params=test_params['amici_params'],\n",
    "    n_optimization_starts=n_optimization_starts,\n",
    "    n_samples=1e3,\n",
    "    n_procs=n_cpus,\n",
    "    n_chains=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if n_optimization_starts > 0:\n",
    "    visualize.waterfall(new_result, size=(6, 4))\n",
    "    ax = visualize.parameters(new_result, size=(6, 25))\n",
    "    visualize_optimized_model_fit(petab_problem=new_petab_problem, result=new_result, pypesto_problem=new_pypesto_problem);\n",
    "\n",
    "#print(test_params['amici_params']-new_result.optimize_result.x[0])"
   ],
   "id": "62ba1a556342abed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_optimization_starts = 20\n",
    "result = run_mcmc(\n",
    "    petab_problem=petab_problem,\n",
    "    pypesto_problem=pypesto_problem,\n",
    "    n_optimization_starts=n_optimization_starts,\n",
    "    n_samples=1e3\n",
    ")\n",
    "\n",
    "if n_optimization_starts > 0:\n",
    "    visualize.waterfall(result, size=(6, 4))\n",
    "    visualize.parameters(result, size=(6, 25))\n",
    "    visualize_optimized_model_fit(petab_problem=petab_problem, result=result, pypesto_problem=pypesto_problem);"
   ],
   "id": "82b4a81a42d5475c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#ax = visualize.sampling_parameter_traces(result, size=(20, 20), full_trace=False, use_problem_bounds=False);\n",
    "#visualize.sampling_scatter(result, size=(13, 6));"
   ],
   "id": "8e7d158c84562a1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BayesFlow workflow",
   "id": "2471c34692a6d37c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "simulator = bf.make_simulator([prior, simulator_amici])\n",
    "simulator.sample(2).keys(), simulator.sample(2)['sim_data'].shape"
   ],
   "id": "79a7fafc4a732243",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_training_batches = 512\n",
    "num_validation_sets = 100\n",
    "batch_size = 64\n",
    "epochs = 100"
   ],
   "id": "35657d5e8c718c77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@delayed\n",
    "def sample_and_simulate():\n",
    "    \"\"\"Single iteration of sampling and simulation\"\"\"\n",
    "    prior_sample = prior()\n",
    "    test = simulator_amici(prior_sample['amici_params'])\n",
    "\n",
    "    # Combine both dictionaries\n",
    "    result = {**prior_sample, **test}\n",
    "    return result\n",
    "\n",
    "def simulate_parallel(n_samples):\n",
    "    \"\"\"Parallel sampling and simulation\"\"\"\n",
    "    results = Parallel(n_jobs=n_cpus, verbose=100)(\n",
    "        sample_and_simulate() for _ in range(n_samples)\n",
    "    )\n",
    "    results_dict = defaultdict(list)\n",
    "\n",
    "    for r in results:\n",
    "        for key, value in r.items():\n",
    "            results_dict[key].append(value)\n",
    "    for key, value_list in results_dict.items():\n",
    "        results_dict[key] = np.array(value_list)\n",
    "    return results_dict"
   ],
   "id": "817a213c704ef666",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.exists(f\"{storage}validation_data_petab_{problem_name}.pkl\"):\n",
    "    with open(f'{storage}validation_data_petab_{problem_name}.pkl', 'rb') as f:\n",
    "        validation_data = pickle.load(f)\n",
    "    try:\n",
    "        with open(f'{storage}training_data_petab_{problem_name}.pkl', 'rb') as f:\n",
    "            training_data = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        training_data = None\n",
    "        print(\"Training data not found\")\n",
    "else:\n",
    "    training_data = simulate_parallel(num_training_batches * batch_size)\n",
    "    validation_data = simulate_parallel(num_validation_sets)\n",
    "\n",
    "    with open(f'{storage}training_data_petab_{problem_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(training_data, f)\n",
    "    with open(f'{storage}validation_data_petab_{problem_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(validation_data, f)\n",
    "\n",
    "# remove failed simulations\n",
    "train_mask = ~training_data['sim_failed']\n",
    "for key in training_data.keys():\n",
    "    training_data[key] = training_data[key][train_mask]\n",
    "val_mask = ~validation_data['sim_failed']\n",
    "for key in validation_data.keys():\n",
    "    validation_data[key] = validation_data[key][val_mask]\n",
    "print(f\"Failed Training data: {np.sum(~train_mask)} / {len(train_mask)}, \"\n",
    "      f\"Failed Validation data: {np.sum(~val_mask)} / {len(val_mask)}\")\n",
    "\n",
    "test_mean = np.nanmean(np.log(validation_data['sim_data']+1), axis=(0,1), keepdims=True)\n",
    "test_std = np.nanstd(np.log(validation_data['sim_data']+1), axis=(0,1), keepdims=True)\n",
    "print(validation_data['sim_data'].shape)"
   ],
   "id": "db8aaf92772d55a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param_names = [name for i, name in enumerate(pypesto_problem.x_names) if i in pypesto_problem.x_free_indices]\n",
    "lbs = np.array([lb for i, lb in enumerate(petab_problem.lb_scaled) if i in pypesto_problem.x_free_indices])\n",
    "ubs = np.array([ub for i, ub in enumerate(petab_problem.ub_scaled) if i in pypesto_problem.x_free_indices])\n",
    "\n",
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .drop('amici_params')  # only used for simulation\n",
    "    .to_array()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .concatenate(param_names, into=\"inference_variables\")\n",
    "    .constrain(\"inference_variables\", lower=lbs, upper=ubs, inclusive='both')  # after concatenate such that we can apply an array as constraint\n",
    "\n",
    "    .as_time_series(\"sim_data\")\n",
    "    .log(\"sim_data\", p1=True)\n",
    "    .standardize(\"sim_data\", mean=test_mean, std=test_std)\n",
    "    .nan_to_num(\"sim_data\", default_value=-3.0)\n",
    "    .rename(\"sim_data\", \"summary_variables\")\n",
    ")"
   ],
   "id": "84f74d0e7c75651a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check how the distributions look like\n",
    "test_params = adapter.forward(validation_data)['inference_variables']\n",
    "\n",
    "n_rows = len(param_names) // 2\n",
    "n_cols = int(np.ceil(len(param_names) / n_rows))\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize=(2*n_rows, 2*n_cols), layout='constrained')\n",
    "ax = ax.flatten()\n",
    "for i, name in enumerate(param_names):\n",
    "    samples = test_params[:, i]\n",
    "    ax[i].hist(samples, density=True)\n",
    "    ax[i].set_title(name)\n",
    "plt.show()"
   ],
   "id": "6d79fc2eac8185eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check how the data distribution looks like (disable nan_to_num in adapter to see nans)\n",
    "test_data = adapter.forward(validation_data)['summary_variables']\n",
    "n_features = test_data.shape[-1]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=int(np.ceil(n_features / 3)), figsize=(20, 20))\n",
    "ax = ax.flatten()\n",
    "for i in range(n_features):\n",
    "    ax[i].hist(test_data[:, :, i].flatten(), density=True)\n",
    "plt.show()"
   ],
   "id": "d13f79cb2d51a9d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "workflow = bf.BasicWorkflow(\n",
    "    simulator=simulator,\n",
    "    adapter=adapter,\n",
    "    summary_network=bf.networks.FusionTransformer(summary_dim=len(param_names)*2),  # FusionTransformer\n",
    "    #inference_network=bf.networks.CouplingFlow()\n",
    "    inference_network=bf.networks.DiffusionModel(),\n",
    ")"
   ],
   "id": "8567adc22e0fbc09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = f'{storage}petab_benchmark_diffusion_model_{problem_name}.keras'\n",
    "if not os.path.exists(model_path):\n",
    "    history = workflow.fit_offline(\n",
    "        training_data,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=validation_data,\n",
    "        verbose=2\n",
    "    )\n",
    "    #workflow.approximator.save(model_path)\n",
    "else:\n",
    "    workflow.approximator = keras.models.load_model(model_path)"
   ],
   "id": "410e6d32cba52fff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "num_samples = 100",
   "id": "f6c46f730b8c1482",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics_plots = workflow.plot_default_diagnostics(test_data=validation_data, num_samples=num_samples,\n",
    "                                                      calibration_ecdf_kwargs={\"difference\": True, 'stacked': True})\n",
    "#for k in diagnostics_plots.keys():\n",
    "#    diagnostics_plots[k].savefig(f\"{storage}petab_benchmark_{problem_name}_{k}.png\")"
   ],
   "id": "3894672c0c7ed821",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#diagnostics = workflow.compute_default_diagnostics(test_data=validation_data, num_samples=num_samples)\n",
    "#diagnostics.median(axis=1)"
   ],
   "id": "8fb68f418664d6c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "416fa380abd57171",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MCMC sampling for comparison",
   "id": "e6f768baf9a64371"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_mcmc_single(petab_prob, pypesto_prob, true_params, n_starts, n_mcmc_samples, n_final_samples, n_chains):\n",
    "    import amici\n",
    "    import logging\n",
    "    amici.swig_wrappers.logger.setLevel(logging.CRITICAL)\n",
    "    pypesto.logging.log(level=logging.ERROR, name=\"pypesto.petab\", console=True)\n",
    "\n",
    "    try:\n",
    "        r, _, _ = run_mcmc(\n",
    "            petab_problem=petab_prob,\n",
    "            pypesto_problem=pypesto_prob,\n",
    "            true_params=true_params,\n",
    "            n_optimization_starts=n_starts,\n",
    "            n_samples=n_mcmc_samples,\n",
    "            n_chains=n_chains,\n",
    "            n_procs=1\n",
    "        )\n",
    "    except np.linalg.LinAlgError as e:\n",
    "        print(\"LinAlgError during MCMC:\", e)\n",
    "        return np.full((n_final_samples, len(pypesto_prob.x_free_indices)), np.nan)\n",
    "\n",
    "    if r is None:\n",
    "        return np.full((n_final_samples, len(pypesto_prob.x_free_indices)), np.nan)\n",
    "\n",
    "    ps = get_mcmc_posterior_samples(r)\n",
    "    # num_samples random samples from posterior\n",
    "    idx = np.random.choice(ps.shape[0], size=n_final_samples)\n",
    "    return ps[idx]"
   ],
   "id": "80cea9fd8aa5d13f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mcmc_path = f'{storage}mcmc_samples_{problem_name}.pkl'\n",
    "if os.path.exists(mcmc_path):\n",
    "    with open(mcmc_path, 'rb') as f:\n",
    "        mcmc_posterior_samples = pickle.load(f)\n",
    "else:\n",
    "    mcmc_posterior_samples = Parallel(n_jobs=n_cpus, verbose=10)(\n",
    "        delayed(run_mcmc_single)(\n",
    "            petab_prob=petab_problem,\n",
    "            pypesto_prob=pypesto_problem,\n",
    "            true_params=params,\n",
    "            n_starts=100,\n",
    "            n_mcmc_samples=1e5,\n",
    "            n_final_samples=num_samples,\n",
    "            n_chains=10\n",
    "        ) for params in validation_data['amici_params']\n",
    "    )\n",
    "    mcmc_posterior_samples = np.array(mcmc_posterior_samples)\n",
    "\n",
    "    with open(mcmc_path, 'wb') as f:\n",
    "        pickle.dump(mcmc_posterior_samples, f)"
   ],
   "id": "3c6844de0597c037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.recovery(\n",
    "    estimates=mcmc_posterior_samples,\n",
    "    targets=pypesto_problem.get_reduced_vector(validation_data['amici_params'].T).T,\n",
    "    variable_names=param_names,\n",
    ")\n",
    "#fig.savefig(f\"{storage}petab_benchmark_{problem_name}_mcmc_recovery.png\")\n",
    "\n",
    "fig = bf.diagnostics.calibration_ecdf(\n",
    "    estimates=mcmc_posterior_samples,\n",
    "    targets=pypesto_problem.get_reduced_vector(validation_data['amici_params'].T).T,\n",
    "    variable_names=param_names,\n",
    "    difference=True,\n",
    "    stacked=True\n",
    ")\n",
    "#fig.savefig(f\"{storage}petab_benchmark_{problem_name}_mcmc_calibration.png\")"
   ],
   "id": "65eb5cdc2255a29a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid_params = pypesto_problem.get_reduced_vector(validation_data['amici_params'].T).T\n",
    "error = (mcmc_posterior_samples-valid_params[:, None])**2 / valid_params[:, None]**2\n",
    "error = np.mean(error, axis=1)\n",
    "good_fits = error.mean(axis=-1) < 1"
   ],
   "id": "7ffd41683285cfb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.recovery(\n",
    "    estimates=mcmc_posterior_samples[good_fits],\n",
    "    targets=pypesto_problem.get_reduced_vector(validation_data['amici_params'].T).T[good_fits],\n",
    "    variable_names=param_names,\n",
    ")\n",
    "\n",
    "fig = bf.diagnostics.calibration_ecdf(\n",
    "    estimates=mcmc_posterior_samples[good_fits],\n",
    "    targets=pypesto_problem.get_reduced_vector(validation_data['amici_params'].T).T[good_fits],\n",
    "    variable_names=param_names,\n",
    "    difference=True,\n",
    "    #stacked=True\n",
    ")"
   ],
   "id": "4c71e39a50fe4a81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "56ab8caec5537f9d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
