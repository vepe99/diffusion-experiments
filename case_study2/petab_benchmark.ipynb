{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# PEtab benchmark model with BayesFlow",
   "id": "f2db3aaca617dd83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pip install git+https://github.com/Benchmarking-Initiative/Benchmark-Models-PEtab.git@master#subdirectory=src/python\n",
    "# pypesto, amici, petab, fides, joblib"
   ],
   "id": "c4c3a5bf38839491",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "else:\n",
    "    print(f\"Using '{os.environ['KERAS_BACKEND']}' backend\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "from typing import Union\n",
    "from collections import defaultdict\n",
    "\n",
    "import benchmark_models_petab as benchmark_models\n",
    "import petab\n",
    "import pypesto.optimize as optimize\n",
    "import pypesto.sample as sample\n",
    "import pypesto.petab\n",
    "import pypesto.visualize as visualize\n",
    "from pypesto.visualize.model_fit import visualize_optimized_model_fit\n",
    "from scipy import stats\n",
    "\n",
    "import bayesflow as bf\n",
    "\n",
    "import amici\n",
    "import logging\n",
    "amici.swig_wrappers.logger.setLevel(logging.CRITICAL)\n",
    "pypesto.logging.log(level=logging.ERROR, name=\"pypesto.petab\", console=True)\n",
    "\n",
    "from petab_helper import scale_values, values_to_linear_scale, amici_pred_to_array, apply_noise_to_data\n",
    "\n",
    "# print all model names\n",
    "print(benchmark_models.MODELS)"
   ],
   "id": "f7d1d9565944bee9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# generate petab problem\n",
    "#job_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0))\n",
    "n_cpus = 10 #int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",
    "problem_name = \"Beer_MolBioSystems2014\" #\"Raimundez_PCB2020\", \"Beer_MolBioSystems2014\", \"Boehm_JProteomeRes2014\"\n",
    "storage = '' # f'plots/{problem_name}/'\n",
    "petab_problem = benchmark_models.get_problem(problem_name)\n",
    "\n",
    "# decrease upper bounds for offset, scaling and noise parameters\n",
    "scale_params_id = [name for name in petab_problem.parameter_df.index.values if name[:6] == 'offset' or name[:5] == 'scale']\n",
    "petab_problem.parameter_df.loc[scale_params_id, 'upperBound'] = 100  # instead of 1000\n",
    "sd_params_id = [name for name in petab_problem.parameter_df.index.values if name[:3] == 'sd_']\n",
    "petab_problem.parameter_df.loc[sd_params_id, 'upperBound'] = 10  # instead of 1000\n",
    "\n",
    "if problem_name == \"Raimundez_PCB2020\":\n",
    "    # Elba added normal priors for the scaling params\n",
    "    scale_params_id = [name for name in petab_problem.parameter_df.index.values if name[:2] == 's_']\n",
    "    petab_problem.parameter_df.loc[scale_params_id, 'objectivePriorType'] = \"normal\"\n",
    "    petab_problem.parameter_df.loc[scale_params_id, 'objectivePriorParameters'] = \"1;10\"\n",
    "    petab_problem.parameter_df.loc[scale_params_id, 'parameterScale'] = \"lin\"\n",
    "\n",
    "# add normal prior (on scale) around real parameters values\n",
    "real_data_params = petab_problem.parameter_df.nominalValue\n",
    "std = 0.5\n",
    "for i in real_data_params.index:\n",
    "    if petab_problem.parameter_df.loc[i, 'estimate'] == 0:\n",
    "        continue\n",
    "    # set prior mean depending on scale\n",
    "    mean = scale_values(real_data_params.loc[i], petab_problem.parameter_df.loc[i, 'parameterScale'])\n",
    "    if not 'objectivePriorType' in petab_problem.parameter_df or pd.isna(petab_problem.parameter_df.loc[i, 'objectivePriorType']):\n",
    "        petab_problem.parameter_df.loc[i, 'objectivePriorType'] = \"parameterScaleNormal\"\n",
    "        petab_problem.parameter_df.loc[i, 'objectivePriorParameters'] = f\"{mean};{std}\"\n",
    "\n",
    "for i, row in petab_problem.parameter_df.iterrows():\n",
    "    if 'objectivePriorType' in row and not pd.isna(row['objectivePriorType']):\n",
    "        if row['estimate'] == 0:\n",
    "            print(f\"Parameter {i} has a {row['objectivePriorType']} prior but is not estimated, setting to nan\")\n",
    "            petab_problem.parameter_df.loc[i, 'objectivePriorType'] = np.nan\n",
    "        # validate petab problem, if scale for parameter is defined, prior must be on the same scale\n",
    "        if row['parameterScale'] != 'lin' and not row['objectivePriorType'].startswith('parameterScale'):\n",
    "            raise ValueError(f\"Parameter {i} has parameterScale {row['parameterScale']} but {row['objectivePriorType']} prior\")\n",
    "\n",
    "\n",
    "importer = pypesto.petab.PetabImporter(petab_problem, simulator_type=\"amici\")\n",
    "factory = importer.create_objective_creator()\n",
    "\n",
    "model = factory.create_model(verbose=False)\n",
    "amici_predictor = factory.create_predictor()\n",
    "amici_predictor.amici_objective.amici_solver.setAbsoluteTolerance(1e-8)\n",
    "\n",
    "# Creating the pypesto problem from PEtab\n",
    "pypesto_problem = importer.create_problem(\n",
    "    startpoint_kwargs={\"check_fval\": True, \"check_grad\": True}\n",
    ")\n",
    "pypesto_problem.print_parameter_summary()"
   ],
   "id": "6bc21d5dc9ed9ad7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prior():\n",
    "    lb = petab_problem.parameter_df['lowerBound'].values\n",
    "    ub = petab_problem.parameter_df['upperBound'].values\n",
    "    param_names_id = petab_problem.parameter_df.index.values\n",
    "    param_scale = petab_problem.parameter_df['parameterScale'].values\n",
    "    if 'objectivePriorType' in petab_problem.parameter_df.columns:\n",
    "        prior_type = petab_problem.parameter_df['objectivePriorType'].values\n",
    "    else:\n",
    "        prior_type = [np.nan] * len(param_names_id)\n",
    "    estimate_param = petab_problem.parameter_df['estimate'].values\n",
    "\n",
    "    prior_dict = {}\n",
    "    for i, name in enumerate(param_names_id):\n",
    "        if estimate_param[i] == 0:\n",
    "            prior_dict[name] = petab_problem.parameter_df['nominalValue'].values[i]  # linear space\n",
    "        elif prior_type[i] == 'uniform':  # linear space\n",
    "            prior_dict[name] = np.random.uniform(low=lb[i], high=ub[i])\n",
    "        elif prior_type[i] == 'parameterScaleUniform' or pd.isna(prior_type[i]):\n",
    "            # scale bounds to scaled space\n",
    "            lb_scaled_i = scale_values(lb[i], param_scale[i])\n",
    "            ub_scaled_i = scale_values(ub[i], param_scale[i])\n",
    "            val = np.random.uniform(low=lb_scaled_i, high=ub_scaled_i)\n",
    "            # scale to linear space\n",
    "            prior_dict[name] = values_to_linear_scale(val, param_scale[i])\n",
    "        elif prior_type[i] == 'parameterScaleNormal':\n",
    "            mean, std = petab_problem.parameter_df['objectivePriorParameters'].values[i].split(';')\n",
    "            lb_scaled_i = scale_values(lb[i], param_scale[i])\n",
    "            ub_scaled_i = scale_values(ub[i], param_scale[i])\n",
    "            a, b = (lb_scaled_i - float(mean)) / float(std), (ub_scaled_i - float(mean)) / float(std)\n",
    "            rv = stats.truncnorm.rvs(loc=float(mean), scale=float(std), a=a, b=b)\n",
    "            # scale to linear space\n",
    "            prior_dict[name] = values_to_linear_scale(rv, param_scale[i])\n",
    "        elif prior_type[i] == 'normal':\n",
    "            mean, std = petab_problem.parameter_df['objectivePriorParameters'].values[i].split(';')\n",
    "            a, b = (lb[i] - float(mean)) / float(std), (ub[i] - float(mean)) / float(std)\n",
    "            rv = stats.truncnorm.rvs(loc=float(mean), scale=float(std), a=a, b=b)\n",
    "            prior_dict[name] = rv\n",
    "        elif prior_type[i] == 'laplace':\n",
    "            loc, scale = petab_problem.parameter_df['objectivePriorParameters'].values[i].split(';')\n",
    "            for t in range(10):\n",
    "                rv = np.random.laplace(loc=float(loc), scale=float(scale))\n",
    "                if lb[i] <= rv <= ub[i]:  # sample from truncated laplace\n",
    "                    break\n",
    "            prior_dict[name] = rv\n",
    "        else:\n",
    "            raise ValueError(\"Unknown prior type:\", prior_type[i])\n",
    "        # scale params and make list\n",
    "        prior_dict[name] = np.array([scale_values(prior_dict[name], param_scale[i])])\n",
    "\n",
    "    # prepare variables for simulation\n",
    "    x = np.array([prior_dict[name][0] for name in pypesto_problem.x_names])\n",
    "    prior_dict['amici_params'] = x  # scaled parameters for amici\n",
    "    return prior_dict\n",
    "\n",
    "def simulator_amici(amici_params):\n",
    "    pred = amici_predictor(amici_params)  # expect amici_params to be scaled\n",
    "    sim, failed = amici_pred_to_array(pred, amici_params,\n",
    "                                      factory=factory, petab_problem=petab_problem, pypesto_problem=pypesto_problem)\n",
    "    return dict(sim_data=sim, sim_failed=failed)"
   ],
   "id": "d478fd3d90b5da5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior_sample = prior()\n",
    "test = simulator_amici(prior_sample['amici_params'])\n",
    "test['sim_data'].shape, prior_sample['amici_params'].shape, np.nansum(test['sim_data'])"
   ],
   "id": "815a2fa50947e5f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # plot prior\n",
    "# n_rows = len(pypesto_problem.x_names) // 6\n",
    "# n_cols = int(np.ceil(len(pypesto_problem.x_names) / n_rows))\n",
    "# fig, axs = plt.subplots(n_rows, n_cols, figsize=(2*n_rows, 2*n_cols), layout='constrained')\n",
    "# axs = axs.flatten()\n",
    "# samples = [prior() for i in range(1000)]\n",
    "# for i, name in enumerate(pypesto_problem.x_names):\n",
    "#     samples_i = np.array([s[name] for s in samples]).flatten()\n",
    "#     axs[i].hist(samples_i, density=True)\n",
    "#     axs[i].set_title(name)\n",
    "#     # axs[i].axvline(scale_values(petab_problem.parameter_df['nominalValue'][i],\n",
    "#     #                              petab_problem.parameter_df['parameterScale'][i]), color='red', linestyle='--')\n",
    "#     # axs[i].axvline(scale_values(petab_problem.parameter_df['lowerBound'][i],\n",
    "#     #                             petab_problem.parameter_df['parameterScale'][i]), color='blue', linestyle='--')\n",
    "#     # axs[i].axvline(scale_values(petab_problem.parameter_df['upperBound'][i],\n",
    "#     #                             petab_problem.parameter_df['parameterScale'][i]), color='blue', linestyle='--')\n",
    "# plt.show()"
   ],
   "id": "8b4598746c13ef4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_mcmc(petab_problem, pypesto_problem, true_params=None, n_optimization_starts=0, n_chains=10, n_samples=10000,\n",
    "             n_procs=10, verbose=False) -> Union[pypesto.result.Result, tuple[pypesto.result.Result, petab.Problem, pypesto.Problem]]:\n",
    "    _petab_problem = deepcopy(petab_problem)\n",
    "    if true_params is None:\n",
    "        # use true data\n",
    "        pass\n",
    "    else:\n",
    "        # this is needed to create a new measurement df and recompile the problem for amici\n",
    "        pred = amici_predictor(true_params)\n",
    "        _, failed = amici_pred_to_array(pred, true_params,\n",
    "                                      factory=factory, petab_problem=petab_problem, pypesto_problem=pypesto_problem)\n",
    "        if failed:\n",
    "            print(\"Simulation failed for true parameters\")\n",
    "            return None, None, None\n",
    "        _measurement_df = factory.prediction_to_petab_measurement_df(pred) # to create new measurement df\n",
    "        _measurement_df = apply_noise_to_data(_measurement_df, true_params, field='measurement',\n",
    "                                              pypesto_problem=pypesto_problem, petab_problem=_petab_problem)\n",
    "        _petab_problem.measurement_df = _measurement_df\n",
    "    _importer = pypesto.petab.PetabImporter(_petab_problem, simulator_type=\"amici\")\n",
    "    _factory = _importer.create_objective_creator()\n",
    "    _model = _factory.create_model(verbose=False)\n",
    "\n",
    "    _pypesto_problem = _importer.create_problem(\n",
    "        startpoint_kwargs={\"check_fval\": True, \"check_grad\": True}\n",
    "    )\n",
    "\n",
    "    if isinstance(_pypesto_problem.objective, pypesto.objective.AggregatedObjective):\n",
    "        _pypesto_problem.objective._objectives[0].amici_solver.setAbsoluteTolerance(1e-8)\n",
    "        #_pypesto_problem.objective._objectives[0].amici_solver.setSensitivityMethod(amici.SensitivityMethod.adjoint)\n",
    "    else:\n",
    "        _pypesto_problem.objective.amici_solver.setAbsoluteTolerance(1e-8)\n",
    "        #_pypesto_problem.objective.amici_solver.setSensitivityMethod(amici.SensitivityMethod.adjoint)\n",
    "\n",
    "    if n_optimization_starts == 0:\n",
    "        print(\"Skipping optimization, sample start points for chains from prior\")\n",
    "        _result = None\n",
    "        x0 = [_pypesto_problem.get_reduced_vector(prior()['amici_params']) for _ in range(n_chains)]\n",
    "    else:\n",
    "        # do the optimization\n",
    "        _result = optimize.minimize(\n",
    "            problem=_pypesto_problem,\n",
    "            optimizer=optimize.FidesOptimizer(verbose=0),\n",
    "            #optimizer=optimize.ScipyOptimizer(method='L-BFGS-B'),\n",
    "            n_starts=n_optimization_starts,\n",
    "            engine=pypesto.engine.MultiProcessEngine(n_procs=n_procs) if n_procs > 1 else None,\n",
    "            progress_bar=verbose\n",
    "        )\n",
    "        x0 = [_pypesto_problem.get_reduced_vector(_result.optimize_result.x[0])]\n",
    "        if x0[0] is None:\n",
    "            print(\"Warning: x0 contains nan, replace with prior sample\")\n",
    "            x0[0] = _pypesto_problem.get_reduced_vector(prior()['amici_params'])\n",
    "        x0 += [_pypesto_problem.get_reduced_vector(prior()['amici_params']) for _ in range(n_chains - 1)]\n",
    "\n",
    "    _sampler = sample.AdaptiveParallelTemperingSampler(\n",
    "        internal_sampler=sample.AdaptiveMetropolisSampler(\n",
    "            options=dict(decay_constant=0.7, threshold_sample=2000)\n",
    "        ),\n",
    "        n_chains=n_chains,\n",
    "        options=dict(show_progress=verbose)\n",
    "    )\n",
    "\n",
    "    _result = sample.sample(\n",
    "        problem=_pypesto_problem,\n",
    "        n_samples=n_samples,\n",
    "        sampler=_sampler,\n",
    "        result=_result,\n",
    "        x0=x0\n",
    "    )\n",
    "    sample.geweke_test(_result)\n",
    "\n",
    "    if true_params is None:\n",
    "        return _result\n",
    "    return _result, _petab_problem, _pypesto_problem"
   ],
   "id": "39067b9bf1fc3e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_mcmc_posterior_samples(res):\n",
    "    burn_in = sample.geweke_test(res)\n",
    "    if burn_in == res.sample_result.trace_x.shape[1]:\n",
    "        print(\"Warning: All samples are considered burn-in.\")\n",
    "        _samples = res.sample_result.trace_x[0]  # only use first chain\n",
    "    else:\n",
    "        _samples = res.sample_result.trace_x[0, burn_in:]  # only use first chain\n",
    "    #_samples = pypesto_problem.get_full_vector(_samples)\n",
    "    #scales = petab_problem.parameter_df.loc[res.problem.x_names, 'parameterScale'].values\n",
    "    #_samples = values_to_linear_scale(_samples, scales)\n",
    "    return _samples"
   ],
   "id": "74f951d5b884ee3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_samples_from_dict(samples_dict):\n",
    "    samples = np.stack([samples_dict[name][..., 0] for name in pypesto_problem.x_names], axis=-1)\n",
    "    return samples"
   ],
   "id": "a07fb5dfd265b1ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_optimization_starts = 0\n",
    "test_params = prior()\n",
    "#print(test_params)\n",
    "new_result, new_petab_problem, new_pypesto_problem = run_mcmc(\n",
    "    petab_problem=petab_problem,\n",
    "    pypesto_problem=pypesto_problem,\n",
    "    true_params=test_params['amici_params'],\n",
    "    n_optimization_starts=n_optimization_starts,\n",
    "    n_samples=1e3,\n",
    "    n_procs=n_cpus,\n",
    "    n_chains=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if n_optimization_starts > 0:\n",
    "    visualize.waterfall(new_result, size=(6, 4))\n",
    "    ax = visualize.parameters(new_result, size=(6, 25))\n",
    "    visualize_optimized_model_fit(petab_problem=new_petab_problem, result=new_result, pypesto_problem=new_pypesto_problem);\n",
    "\n",
    "#print(test_params['amici_params']-new_result.optimize_result.x[0])"
   ],
   "id": "62ba1a556342abed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# n_optimization_starts = 20\n",
    "# result = run_mcmc(\n",
    "#     petab_problem=petab_problem,\n",
    "#     pypesto_problem=pypesto_problem,\n",
    "#     n_optimization_starts=n_optimization_starts,\n",
    "#     n_samples=1e3\n",
    "# )\n",
    "#\n",
    "# if n_optimization_starts > 0:\n",
    "#     visualize.waterfall(result, size=(6, 4))\n",
    "#     visualize.parameters(result, size=(6, 25))\n",
    "#     visualize_optimized_model_fit(petab_problem=petab_problem, result=result, pypesto_problem=pypesto_problem);"
   ],
   "id": "82b4a81a42d5475c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#ax = visualize.sampling_parameter_traces(result, size=(20, 20), full_trace=False, use_problem_bounds=False);\n",
    "#visualize.sampling_scatter(result, size=(13, 6));"
   ],
   "id": "8e7d158c84562a1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate training and validation data",
   "id": "9f8137a572fafeaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from model_settings import MODELS, NUM_SAMPLES_INFERENCE, load_model\n",
    "\n",
    "num_training_sets = 512 * 64\n",
    "num_validation_sets = 100"
   ],
   "id": "2b3638846c2c2d4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@delayed\n",
    "def sample_and_simulate():\n",
    "    \"\"\"Single iteration of sampling and simulation\"\"\"\n",
    "    prior_sample = prior()\n",
    "    test = simulator_amici(prior_sample['amici_params'])\n",
    "\n",
    "    # Combine both dictionaries\n",
    "    result = {**prior_sample, **test}\n",
    "    return result\n",
    "\n",
    "\n",
    "def simulate_parallel(n_samples):\n",
    "    \"\"\"Parallel sampling and simulation\"\"\"\n",
    "    results = Parallel(n_jobs=n_cpus, verbose=100)(\n",
    "        sample_and_simulate() for _ in range(n_samples)\n",
    "    )\n",
    "    results_dict = defaultdict(list)\n",
    "\n",
    "    for r in results:\n",
    "        for key, value in r.items():\n",
    "            results_dict[key].append(value)\n",
    "    for key, value_list in results_dict.items():\n",
    "        results_dict[key] = np.array(value_list)\n",
    "    return results_dict"
   ],
   "id": "f83743725aca6688",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.exists(f\"{storage}validation_data_petab_{problem_name}.pkl\"):\n",
    "    with open(f'{storage}validation_data_petab_{problem_name}.pkl', 'rb') as f:\n",
    "        validation_data = pickle.load(f)\n",
    "    try:\n",
    "        with open(f'{storage}training_data_petab_{problem_name}.pkl', 'rb') as f:\n",
    "            training_data = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        training_data = None\n",
    "        print(\"Training data not found\")\n",
    "else:\n",
    "    training_data = simulate_parallel(num_training_sets)\n",
    "    validation_data = simulate_parallel(num_validation_sets)\n",
    "\n",
    "    with open(f'{storage}training_data_petab_{problem_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(training_data, f)\n",
    "    with open(f'{storage}validation_data_petab_{problem_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(validation_data, f)\n",
    "\n",
    "# remove failed simulations\n",
    "if not training_data is None:\n",
    "    train_mask = ~training_data['sim_failed']\n",
    "    for key in training_data.keys():\n",
    "        training_data[key] = training_data[key][train_mask]\n",
    "    print(f\"Failed Training data: {np.sum(~train_mask)} / {len(train_mask)}\")\n",
    "val_mask = ~validation_data['sim_failed']\n",
    "for key in validation_data.keys():\n",
    "    validation_data[key] = validation_data[key][val_mask]\n",
    "print(f\"Failed Validation data: {np.sum(~val_mask)} / {len(val_mask)}\")\n",
    "\n",
    "test_mean = np.nanmean(np.log(validation_data['sim_data'] + 1), axis=(0, 1), keepdims=True)\n",
    "test_std = np.nanstd(np.log(validation_data['sim_data'] + 1), axis=(0, 1), keepdims=True)\n",
    "print(validation_data['sim_data'].shape)\n",
    "\n",
    "param_names = [name for i, name in enumerate(pypesto_problem.x_names) if i in pypesto_problem.x_free_indices]\n",
    "lbs = np.array([lb for i, lb in enumerate(petab_problem.lb_scaled) if i in pypesto_problem.x_free_indices])\n",
    "ubs = np.array([ub for i, ub in enumerate(petab_problem.ub_scaled) if i in pypesto_problem.x_free_indices])"
   ],
   "id": "b8d8e6928df45ef9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MCMC workflow",
   "id": "d5a018bb92514dd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_mcmc_single(petab_prob, pypesto_prob, true_params, n_starts, n_mcmc_samples, n_final_samples, n_chains):\n",
    "    import amici\n",
    "    import logging\n",
    "    amici.swig_wrappers.logger.setLevel(logging.CRITICAL)\n",
    "    pypesto.logging.log(level=logging.ERROR, name=\"pypesto.petab\", console=True)\n",
    "\n",
    "    try:\n",
    "        r, _, _ = run_mcmc(\n",
    "            petab_problem=petab_prob,\n",
    "            pypesto_problem=pypesto_prob,\n",
    "            true_params=true_params,\n",
    "            n_optimization_starts=n_starts,\n",
    "            n_samples=n_mcmc_samples,\n",
    "            n_chains=n_chains,\n",
    "            n_procs=1\n",
    "        )\n",
    "    except np.linalg.LinAlgError as e:\n",
    "        print(\"LinAlgError during MCMC:\", e)\n",
    "        return np.full((n_final_samples, len(pypesto_prob.x_free_indices)), np.nan)\n",
    "\n",
    "    if r is None:\n",
    "        return np.full((n_final_samples, len(pypesto_prob.x_free_indices)), np.nan)\n",
    "\n",
    "    ps = get_mcmc_posterior_samples(r)\n",
    "    # num_samples random samples from posterior\n",
    "    idx = np.random.choice(ps.shape[0], size=n_final_samples)\n",
    "    return ps[idx]"
   ],
   "id": "3dceab2c5591867",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mcmc_path = f'{storage}mcmc_samples_{problem_name}.pkl'\n",
    "if os.path.exists(mcmc_path):\n",
    "    with open(mcmc_path, 'rb') as f:\n",
    "        mcmc_posterior_samples = pickle.load(f)\n",
    "else:\n",
    "    mcmc_posterior_samples = Parallel(n_jobs=n_cpus, verbose=10)(\n",
    "        delayed(run_mcmc_single)(\n",
    "            petab_prob=petab_problem,\n",
    "            pypesto_prob=pypesto_problem,\n",
    "            true_params=params,\n",
    "            n_starts=10,\n",
    "            n_mcmc_samples=1e5,\n",
    "            n_final_samples=1000,\n",
    "            n_chains=10\n",
    "        ) for params in validation_data['amici_params']\n",
    "    )\n",
    "    mcmc_posterior_samples = np.array(mcmc_posterior_samples)\n",
    "\n",
    "    with open(mcmc_path, 'wb') as f:\n",
    "        pickle.dump(mcmc_posterior_samples, f)\n",
    "mcmc_mask = ~np.isnan(mcmc_posterior_samples.sum(axis=(1, 2)))\n",
    "mcmc_posterior_samples_test = mcmc_posterior_samples[mcmc_mask * val_mask]"
   ],
   "id": "4b1c5b1e5a1797a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.recovery(\n",
    "    estimates=mcmc_posterior_samples_test,\n",
    "    targets=pypesto_problem.get_reduced_vector(validation_data['amici_params'].T).T[mcmc_mask],\n",
    "    variable_names=param_names,\n",
    ")\n",
    "#fig.savefig(f\"{storage}petab_benchmark_{problem_name}_mcmc_recovery.png\")\n",
    "\n",
    "fig = bf.diagnostics.calibration_ecdf(\n",
    "    estimates=mcmc_posterior_samples_test,\n",
    "    targets=pypesto_problem.get_reduced_vector(validation_data['amici_params'].T).T[mcmc_mask],\n",
    "    variable_names=param_names,\n",
    "    difference=True,\n",
    "    stacked=True\n",
    ")\n",
    "#fig.savefig(f\"{storage}petab_benchmark_{problem_name}_mcmc_calibration.png\")"
   ],
   "id": "4ec482955602ce23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BayesFlow workflow",
   "id": "2471c34692a6d37c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .drop('amici_params')  # only used for simulation\n",
    "    .to_array()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .concatenate(param_names, into=\"inference_variables\")\n",
    "    .constrain(\"inference_variables\", lower=lbs, upper=ubs, inclusive='both')  # after concatenate such that we can apply an array as constraint\n",
    "\n",
    "    .as_time_series(\"sim_data\")\n",
    "    .log(\"sim_data\", p1=True)\n",
    "    #.standardize(\"sim_data\", mean=test_mean, std=test_std)\n",
    "    #.nan_to_num(\"sim_data\", default_value=-3.0)\n",
    "    .rename(\"sim_data\", \"summary_variables\")\n",
    ")"
   ],
   "id": "84f74d0e7c75651a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # check how the distributions look like\n",
    "# test_params = adapter.forward(validation_data)['inference_variables']\n",
    "#\n",
    "# n_rows = len(param_names) // 6\n",
    "# n_cols = int(np.ceil(len(param_names) / n_rows))\n",
    "# fig, ax = plt.subplots(n_rows, n_cols, figsize=(2*n_rows, 2*n_cols), layout='constrained')\n",
    "# ax = ax.flatten()\n",
    "# for i, name in enumerate(param_names):\n",
    "#     samples = test_params[:, i]\n",
    "#     ax[i].hist(samples, density=True)\n",
    "#     ax[i].set_title(name)\n",
    "# plt.show()"
   ],
   "id": "6d79fc2eac8185eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check how the data distribution looks like (disable nan_to_num in adapter to see nans)\n",
    "_test_data = adapter.forward(validation_data)['summary_variables']\n",
    "# n_features = _test_data.shape[-1]\n",
    "#\n",
    "# n_rows = n_features // 5\n",
    "# n_cols = int(np.ceil(n_features / n_rows))\n",
    "# fig, ax = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(2*n_rows, 2*n_cols))\n",
    "# ax = ax.flatten()\n",
    "# for i in range(n_features):\n",
    "#     ax[i].hist(_test_data[:, :, i].flatten(), density=True)\n",
    "# plt.show()"
   ],
   "id": "d13f79cb2d51a9d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print some stats about the data\n",
    "print('Minimum', np.min(_test_data))\n",
    "print('Maximum', np.max(_test_data))\n",
    "print('Mean', np.mean(_test_data))\n",
    "print('Standard Deviation', np.std(_test_data))\n",
    "print('Nan Values', np.isnan(_test_data).sum())"
   ],
   "id": "db94aa20d5ed36c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_name = list(MODELS.keys())[-1]\n",
    "conf_tuple = MODELS[model_name]\n",
    "print(model_name)"
   ],
   "id": "6a0293b5f73232bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "workflow = load_model(adapter=adapter, conf_tuple=conf_tuple, param_names=param_names,\n",
    "                      training_data=validation_data, #todo: training_data,\n",
    "                      validation_data=validation_data, storage=storage+'models/', problem_name=problem_name, model_name=model_name)"
   ],
   "id": "8567adc22e0fbc09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics_plots = workflow.plot_default_diagnostics(test_data=validation_data, num_samples=NUM_SAMPLES_INFERENCE,\n",
    "                                                      calibration_ecdf_kwargs={\"difference\": True, 'stacked': True})\n",
    "#for k in diagnostics_plots.keys():\n",
    "#    diagnostics_plots[k].savefig(f\"{storage}petab_benchmark_{problem_name}_{model_name}_{k}.png\")"
   ],
   "id": "3894672c0c7ed821",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compare MCMC and Diffusion models",
   "id": "ebb1651065f52dac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from bayesflow.diagnostics.metrics import root_mean_squared_error, posterior_contraction, calibration_error\n",
    "from visualize_results import plot_model_comparison_radar"
   ],
   "id": "823a2e0f8a48fbe4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_data = {}\n",
    "for keys, values in validation_data.items():\n",
    "    test_data[keys] = values[mcmc_mask * val_mask]"
   ],
   "id": "29eec708cd99c486",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "fusion_transformer_summary = False",
   "id": "917c77af225455e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics = []\n",
    "for i in range(len(MODELS)):\n",
    "    if fusion_transformer_summary and not 'ft' in list(MODELS.keys())[i]:\n",
    "        continue\n",
    "    elif not fusion_transformer_summary and 'ft' in list(MODELS.keys())[i]:\n",
    "        continue\n",
    "    if os.path.exists(f'{storage}models/petab_benchmark_{problem_name}_metrics_{i}.pkl'):\n",
    "        with open(f'{storage}models/petab_benchmark_{problem_name}_metrics_{i}.pkl', 'rb') as f:\n",
    "            metric = pickle.load(f)\n",
    "        metrics += metric\n",
    "    else:\n",
    "        print(f\"Metrics for model {list(MODELS.keys())[i]} not found\")\n",
    "\n",
    "# df, all columns to float beside model and sampler\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "for col in ['nrmse', 'posterior_contraction', 'posterior_calibration_error', 'c2st']:\n",
    "    metrics_df[col] = metrics_df[col].astype(float)\n",
    "    metrics_df.loc[metrics_df[col].isna(), col] = 1\n",
    "\n",
    "# add mcmc as baseline\n",
    "test_targets = get_samples_from_dict(test_data)\n",
    "mcmc_df = pd.DataFrame([{\n",
    "    'model': 'MCMC',\n",
    "    'sampler': 'MCMC',\n",
    "    'nrmse': root_mean_squared_error(mcmc_posterior_samples_test, test_targets)['values'].mean(),\n",
    "    'posterior_contraction': posterior_contraction(mcmc_posterior_samples_test, test_targets)['values'].mean(),\n",
    "    'posterior_calibration_error': calibration_error(mcmc_posterior_samples_test, test_targets)['values'].mean(),\n",
    "    #'c2st': None\n",
    "}])\n",
    "metrics_df = pd.concat([metrics_df, mcmc_df], ignore_index=True)\n",
    "metrics_df"
   ],
   "id": "316dd05f86c816ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compare all models with all samplers overlaid\n",
    "plot_model_comparison_radar(metrics_df, group_by_sampler=True, save_path=f'{storage}models/')"
   ],
   "id": "f7dac60ca021a570",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_model_comparison_radar(metrics_df, group_by_sampler=True,\n",
    "                            group_by_method='diffusion', save_path=f'{storage}models/')"
   ],
   "id": "8b7fa9d1aa413052",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ac15359e85358d9b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
