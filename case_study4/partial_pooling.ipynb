{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# Case Study: Compositional Inference for a Drift Diffusion Model",
   "id": "f0f59c3bce4dc698"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Consider a decision task in which participants are presented with sequences of letters and asked\n",
    "to differentiate between words and non-words (i.e., a lexical decision task). The Diffusion Decision Model (DDM;\n",
    "e.g., Ratcliff et al., 2016) simulataneously models this binary decision and the response time via a continuous evidence\n",
    "accumulation process: After an initial non-decision time t0, evidence accumulates following a noisy diffusion process\n",
    "with a certain drift rate ν, starting from a point β, until one of two decision thresholds {0,α}corresponding to the two\n",
    "choices is hit.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_\\nu \\sim\\mathcal{N}(0.5,0.3) \\\\\n",
    "\\mu_\\alpha \\sim\\mathcal{N}(0,0.05) \\\\\n",
    "\\mu_{t_0}\\sim \\mathcal{N}(−1,0.3) \\\\\n",
    "\\log \\sigma_\\nu \\sim\\mathcal{N}(−1,1) \\\\\n",
    "\\log \\sigma_\\alpha \\sim\\mathcal{N}(−3,1) \\\\\n",
    "\\log \\sigma_{t_0}\\sim\\mathcal{N}(−1,0.3) \\\\\n",
    "\\nu_j\\sim\\mathcal{N}(\\mu_\\nu,\\sigma_\\nu) \\\\\n",
    "\\log \\alpha_j\\sim\\mathcal{N}(\\mu_\\alpha,\\sigma_\\alpha) \\\\\n",
    "\\log t_{0,j}\\sim\\mathcal{N}(\\mu_{t_0} ,\\sigma_{t_0} ) \\\\\n",
    "\\beta\\sim\\operatorname{Beta}(a=50,b=50) \\\\\n",
    "y_j\\sim \\operatorname{DDM}(\\nu_j,\\alpha_j,t_{0,j},\\beta)\n",
    "\\end{align*}\n",
    "\n",
    "$\\beta_\\text{raw}$ is a transformed unbounded variable that is transformed to $\\beta$ via the beta inverse CDF."
   ],
   "id": "87ef9510c29662c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "else:\n",
    "    print(f\"Using '{os.environ['KERAS_BACKEND']}' backend\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy.stats import median_abs_deviation\n",
    "\n",
    "import keras\n",
    "import bayesflow as bf\n",
    "\n",
    "from ddm_simulator import simulate_ddm, beta_from_normal\n",
    "\n",
    "problem_name = \"compositional_case_study\"\n",
    "storage = '' #f'plots/{problem_name}/'\n",
    "n_jobs = 10 #int(os.environ.get('SLURM_CPUS_PER_TASK', 1))"
   ],
   "id": "86cd4e0b9f89165b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param_names_global = ['mu_nu', 'mu_log_alpha', 'mu_log_t0',\n",
    "                      'log_sigma_nu', 'log_sigma_log_alpha', 'log_sigma_log_t0',\n",
    "                      'beta_raw']\n",
    "pretty_param_names_global = [r'$\\mu_\\nu$', r'$\\mu_{\\log \\alpha}$', r'$\\mu_{\\log t_0}$',\n",
    "                              r'$\\log \\sigma_\\nu$', r'$\\log \\sigma_{\\log \\alpha}$', r'$\\log \\sigma_{\\log t_0}$',\n",
    "                              r'$\\beta$']\n",
    "param_names_local = ['nu', 'alpha', 't0']\n",
    "pretty_param_names_local = [r'$\\nu_p$', r'$\\alpha_p$', r'$t_{0,p}$']\n",
    "pretty_param_names_local_log = [r'$\\nu_p$', r'$\\log\\alpha_p$', r'$\\log t_{0,p}$']\n",
    "num_training_batches = 256\n",
    "num_validation_sets = 300\n",
    "batch_size = 128\n",
    "epochs = 1000"
   ],
   "id": "e5b8ce91e7a0c2d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Priors\n",
    "# ---------------------------\n",
    "n_trials = 30\n",
    "def sample_priors(n_subjects=1):\n",
    "    \"\"\"\n",
    "    Hierarchical draws as in your specification.\n",
    "    Returns a dict with group params and per subject params.\n",
    "    \"\"\"\n",
    "    # Group level\n",
    "    mu_nu = np.random.normal(0.5, 0.3)\n",
    "    mu_log_alpha = np.random.normal(0.0, 0.05)\n",
    "    mu_log_t0 = np.random.normal(-1.0, 0.3)\n",
    "\n",
    "    log_sigma_nu = np.random.normal(-1.0, 1.0)\n",
    "    log_sigma_log_alpha = np.random.normal(-3.0, 1.0)\n",
    "    log_sigma_log_t0 = np.random.normal(-1.0, 0.3)\n",
    "\n",
    "    sigma_nu = np.exp(log_sigma_nu)\n",
    "    sigma_log_alpha = np.exp(log_sigma_log_alpha)\n",
    "    sigma_log_t0 = np.exp(log_sigma_log_t0)\n",
    "\n",
    "    beta_raw = np.random.normal(0.0, 1.0)\n",
    "    beta = beta_from_normal(beta_raw, a=50, b=50)\n",
    "\n",
    "    # Subject level\n",
    "    nu = np.random.normal(mu_nu, sigma_nu, size=n_subjects)\n",
    "    log_alpha = np.random.normal(mu_log_alpha, sigma_log_alpha, size=n_subjects)\n",
    "    log_t0 = np.random.normal(mu_log_t0, sigma_log_t0, size=n_subjects)\n",
    "    alpha = np.exp(log_alpha)\n",
    "    t0 = np.exp(log_t0)\n",
    "\n",
    "    return {\n",
    "        # group\n",
    "        \"mu_nu\": mu_nu,\n",
    "        \"mu_log_alpha\": mu_log_alpha,\n",
    "        \"mu_log_t0\": mu_log_t0,\n",
    "        \"log_sigma_nu\": log_sigma_nu,\n",
    "        \"log_sigma_log_alpha\": log_sigma_log_alpha,\n",
    "        \"log_sigma_log_t0\": log_sigma_log_t0,\n",
    "        \"beta_raw\": beta_raw,\n",
    "        \"beta\": beta,\n",
    "        # subjects\n",
    "        \"nu\": nu,\n",
    "        \"alpha\": alpha,\n",
    "        \"t0\": t0,\n",
    "        # meta\n",
    "        \"n_subjects\": n_subjects,\n",
    "        \"n_trials\": n_trials,\n",
    "    }\n",
    "\n",
    "\n",
    "def score_log_norm(x, m, s):\n",
    "    return -(x-m) / s**2\n",
    "\n",
    "def prior_score(x: dict):\n",
    "    mu_nu = x[\"mu_nu\"]\n",
    "    mu_log_alpha = x[\"mu_log_alpha\"]\n",
    "    mu_log_t0 = x[\"mu_log_t0\"]\n",
    "    log_sigma_nu = x[\"log_sigma_nu\"]\n",
    "    log_sigma_log_alpha = x[\"log_sigma_log_alpha\"]\n",
    "    log_sigma_log_t0 = x[\"log_sigma_log_t0\"]\n",
    "    beta_raw = x[\"beta_raw\"]\n",
    "\n",
    "    parts = {\n",
    "        \"mu_nu\": score_log_norm(mu_nu, m=0.5, s=0.3),\n",
    "        \"mu_log_alpha\": score_log_norm(mu_log_alpha, m=0.0, s=0.05),\n",
    "        \"mu_log_t0\": score_log_norm(mu_log_t0, m=-1.0, s=0.3),\n",
    "        \"log_sigma_nu\": score_log_norm(log_sigma_nu, m=-1.0, s=1.0),\n",
    "        \"log_sigma_log_alpha\": score_log_norm(log_sigma_log_alpha, m=-3.0, s=1.0),\n",
    "        \"log_sigma_log_t0\": score_log_norm(log_sigma_log_t0, m=-1.0, s=0.3),\n",
    "        \"beta_raw\": score_log_norm(beta_raw, m=0.0, s=1.0),\n",
    "    }\n",
    "    return parts\n",
    "\n",
    "simulator = bf.make_simulator([sample_priors, simulate_ddm])\n",
    "print(simulator.sample(1)['sim_data'].shape)"
   ],
   "id": "66179c1fa0cbd4e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_data = None #simulator.sample_parallel((num_training_batches * batch_size))\n",
    "validation_data = simulator.sample_parallel(num_validation_sets)"
   ],
   "id": "584ce273cb384ce9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .to_array()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .concatenate(param_names_global, into=\"inference_variables\")\n",
    "    .rename(\"sim_data\", \"summary_variables\")\n",
    ")"
   ],
   "id": "7b877a3b3d13718e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check how the distributions look like\n",
    "test_params = adapter.forward(validation_data)['inference_variables']\n",
    "\n",
    "n_rows = len(param_names_global) // 4\n",
    "n_cols = int(np.ceil(len(param_names_global) / n_rows))\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize=(2*n_cols, 2*n_rows), layout='constrained')\n",
    "ax = ax.flatten()\n",
    "for i, name in enumerate(pretty_param_names_global):\n",
    "    samples = test_params[:, i]\n",
    "    ax[i].hist(samples, density=True)\n",
    "    ax[i].set_title(name)\n",
    "plt.show()"
   ],
   "id": "590b7586c13b1a0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check how the data distribution looks like (disable nan_to_num in adapter to see nans)\n",
    "test_data = adapter.forward(validation_data)['summary_variables']\n",
    "n_features = test_data.shape[-1]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=int(np.ceil(n_features / 2)), figsize=(8, 4), layout='constrained')\n",
    "ax = ax.flatten()\n",
    "for i in range(n_features):\n",
    "    ax[i].hist(test_data[..., i].flatten(), density=True)\n",
    "plt.show()\n",
    "\n",
    "print(test_data.shape)"
   ],
   "id": "e4ba6641adbf0209",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scores = prior_score(validation_data)\n",
    "\n",
    "# check how the distributions look like\n",
    "n_rows = len(param_names_global) // 4\n",
    "n_cols = int(np.ceil(len(param_names_global) / n_rows))\n",
    "fig, ax = plt.subplots(n_rows, n_cols, figsize=(2*n_cols, 2*n_rows), layout='constrained')\n",
    "ax = ax.flatten()\n",
    "for i, name in enumerate(param_names_global):\n",
    "    ax[i].hist(scores[name], density=True)\n",
    "    ax[i].set_title(r'$\\nabla \\log p($' + pretty_param_names_global[i] + '$)$')\n",
    "plt.show()"
   ],
   "id": "f81d1d775ed92c43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Non-Compositional inference",
   "id": "aea98f328685db4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "workflow_global = bf.BasicWorkflow(\n",
    "    adapter=adapter,\n",
    "    summary_network=bf.networks.SetTransformer(summary_dim=16, dropout=0.1),\n",
    "    inference_network=bf.networks.CompositionalDiffusionModel(),\n",
    ")"
   ],
   "id": "71267ef96c8e940b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = f'{storage}models/partial_pooling_global.keras'\n",
    "if not os.path.exists(model_path):\n",
    "    history = workflow_global.fit_offline(\n",
    "        training_data,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=validation_data,\n",
    "        verbose=2,\n",
    "    )\n",
    "    workflow_global.approximator.save(model_path)\n",
    "else:\n",
    "    workflow_global.approximator = keras.models.load_model(model_path)"
   ],
   "id": "a161f6777e95c1d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics_plots = workflow_global.plot_default_diagnostics(test_data=validation_data, num_samples=100,\n",
    "                                                             calibration_ecdf_kwargs={\"difference\": True},\n",
    "                                                             variable_names=pretty_param_names_global)\n",
    "#for k in diagnostics_plots.keys():\n",
    "#    diagnostics_plots[k].savefig(f\"{storage}plots/partial_pooling_single_subject_{k}.png\")"
   ],
   "id": "906e9b9704f52108",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Partial Pooling: Global Compositional inference",
   "id": "a7f6461466032a2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_data_comp_subjects = simulator.sample_parallel(100, n_subjects=30, n_trials=n_trials)\n",
    "validation_data['sim_data'].shape, test_data_comp_subjects['sim_data'].shape"
   ],
   "id": "5a81962ce22b0d91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_samples = 100\n",
    "test_posterior_comp_subjects = workflow_global.compositional_sample(\n",
    "    num_samples=num_samples,\n",
    "    conditions={'sim_data': test_data_comp_subjects['sim_data']},\n",
    "    compute_prior_score=prior_score,\n",
    "    #steps=300,\n",
    "    #corrector_steps=2,\n",
    "    compositional_bridge_d1=0.05,\n",
    "    mini_batch_size=1\n",
    ")\n",
    "ps = test_posterior_comp_subjects.copy()\n",
    "ps['beta'] = beta_from_normal(ps['beta_raw'], a=50, b=50)\n",
    "ps.pop('beta_raw');"
   ],
   "id": "3c52aa33af040e47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.recovery(\n",
    "    estimates=ps,\n",
    "    targets=test_data_comp_subjects,\n",
    "    variable_names=pretty_param_names_global\n",
    ")\n",
    "#fig.savefig(f\"{storage}plots/partial_pooling_compositional_subjects_recovery.png\")\n",
    "\n",
    "fig = bf.diagnostics.calibration_ecdf(\n",
    "    estimates=ps,\n",
    "    targets=test_data_comp_subjects,\n",
    "    difference=True,\n",
    "    variable_names=pretty_param_names_global\n",
    ")\n",
    "# fig.savefig(f\"{storage}plots/partial_pooling_compositional_subjects_calibration.png\")"
   ],
   "id": "ed1c6008b19684a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics = {\n",
    "    'NRMSE': bf.diagnostics.metrics.root_mean_squared_error(ps, test_data_comp_subjects)['values'],\n",
    "    'NRMSE-mad': bf.diagnostics.metrics.root_mean_squared_error(ps, test_data_comp_subjects, aggregation=median_abs_deviation)['values'],\n",
    "    'calibration_error': bf.diagnostics.metrics.calibration_error(ps, test_data_comp_subjects)['values'],\n",
    "    'calibration_error-mad': bf.diagnostics.metrics.calibration_error(ps, test_data_comp_subjects,\n",
    "                                                                      aggregation=median_abs_deviation)['values'],\n",
    "}\n",
    "\n",
    "with open(f'partial_pooling_global_metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(metrics, f)\n",
    "metrics"
   ],
   "id": "68943d898aa6224b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training a non-compositional local model",
   "id": "d8a841c2c2554cfa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "adapter_subjects = (\n",
    "    bf.adapters.Adapter()\n",
    "    .to_array()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .log([\"alpha\", \"t0\"])  # log-transform alpha and t0 to make them unbounded\n",
    "    .concatenate(param_names_local, into=\"inference_variables\")\n",
    "    .concatenate(param_names_global, into=\"inference_conditions\")\n",
    "    .rename(\"sim_data\", \"summary_variables\")\n",
    ")"
   ],
   "id": "1bc5c46b1e84126e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "workflow_local = bf.BasicWorkflow(\n",
    "    adapter=adapter_subjects,\n",
    "    summary_network=bf.networks.SetTransformer(summary_dim=16, dropout=0.1),\n",
    "    inference_network=bf.networks.FlowMatching(),\n",
    ")"
   ],
   "id": "e04f6f0401f93b42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = f'{storage}models/partial_pooling_local.keras'\n",
    "if not os.path.exists(model_path):\n",
    "    history = workflow_local.fit_offline(\n",
    "        training_data,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=validation_data,\n",
    "        verbose=2,\n",
    "    )\n",
    "    workflow_local.approximator.save(model_path)\n",
    "else:\n",
    "    workflow_local.approximator = keras.models.load_model(model_path)"
   ],
   "id": "83766218cc71c8b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diagnostics_plots = workflow_local.plot_default_diagnostics(test_data=validation_data, num_samples=100,\n",
    "                                                             calibration_ecdf_kwargs={\"difference\": True},\n",
    "                                                             variable_names=pretty_param_names_local)"
   ],
   "id": "d204df89a0479c76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# one global sample per subject\n",
    "subject_id = np.random.choice(test_data_comp_subjects['sim_data'].shape[1])\n",
    "print(subject_id)\n",
    "test_data_local_subject = {\n",
    "    'sim_data': np.tile(test_data_comp_subjects['sim_data'][:, subject_id][..., None], num_samples).transpose((0,3,1,2)).reshape(-1, n_trials, 2),\n",
    "    'mu_nu': test_posterior_comp_subjects['mu_nu'].reshape(-1, 1),\n",
    "    'mu_log_alpha': test_posterior_comp_subjects['mu_log_alpha'].reshape(-1, 1),\n",
    "    'mu_log_t0': test_posterior_comp_subjects['mu_log_t0'].reshape(-1, 1),\n",
    "    'log_sigma_nu': test_posterior_comp_subjects['log_sigma_nu'].reshape(-1, 1),\n",
    "    'log_sigma_log_alpha': test_posterior_comp_subjects['log_sigma_log_alpha'].reshape(-1, 1),\n",
    "    'log_sigma_log_t0': test_posterior_comp_subjects['log_sigma_log_t0'].reshape(-1, 1),\n",
    "    'beta_raw': test_posterior_comp_subjects['beta_raw'].reshape(-1, 1),\n",
    "\n",
    "    # true local params for evaluation\n",
    "    'nu': test_data_comp_subjects['nu'][:, subject_id].reshape(-1, 1),\n",
    "    'alpha': test_data_comp_subjects['alpha'][:, subject_id].reshape(-1, 1),\n",
    "    't0': test_data_comp_subjects['t0'][:, subject_id].reshape(-1, 1),\n",
    "}\n",
    "\n",
    "test_local_samples_subject = workflow_local.sample(conditions=test_data_local_subject, num_samples=1)\n",
    "for key in test_local_samples_subject.keys():\n",
    "    test_local_samples_subject[key] = test_local_samples_subject[key].reshape(*test_posterior_comp_subjects['mu_nu'].shape)"
   ],
   "id": "69510b04f7d4129b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_subjects = test_data_comp_subjects['sim_data'].shape[1]\n",
    "\n",
    "all_samples = None\n",
    "combined = {}\n",
    "\n",
    "# preallocate containers per key\n",
    "keys_to_collect = None\n",
    "\n",
    "for s in range(n_subjects):\n",
    "    print(s)\n",
    "    test_data_local_subject = {\n",
    "        'sim_data': np.tile(\n",
    "            test_data_comp_subjects['sim_data'][:, s][..., None],\n",
    "            num_samples\n",
    "        ).transpose((0, 3, 1, 2)).reshape(-1, n_trials, 2),\n",
    "\n",
    "        'mu_nu': test_posterior_comp_subjects['mu_nu'].reshape(-1, 1),\n",
    "        'mu_log_alpha': test_posterior_comp_subjects['mu_log_alpha'].reshape(-1, 1),\n",
    "        'mu_log_t0': test_posterior_comp_subjects['mu_log_t0'].reshape(-1, 1),\n",
    "        'log_sigma_nu': test_posterior_comp_subjects['log_sigma_nu'].reshape(-1, 1),\n",
    "        'log_sigma_log_alpha': test_posterior_comp_subjects['log_sigma_log_alpha'].reshape(-1, 1),\n",
    "        'log_sigma_log_t0': test_posterior_comp_subjects['log_sigma_log_t0'].reshape(-1, 1),\n",
    "        'beta_raw': test_posterior_comp_subjects['beta_raw'].reshape(-1, 1),\n",
    "\n",
    "        # true local params for evaluation\n",
    "        #'nu':   test_data_comp_subjects['nu'][:, s].reshape(-1, 1),\n",
    "        #'alpha': test_data_comp_subjects['alpha'][:, s].reshape(-1, 1),\n",
    "        #'t0':   test_data_comp_subjects['t0'][:, s].reshape(-1, 1),\n",
    "    }\n",
    "\n",
    "    samples_s = workflow_local.sample(conditions=test_data_local_subject, num_samples=1)\n",
    "\n",
    "    # on first pass create storage\n",
    "    if keys_to_collect is None:\n",
    "        keys_to_collect = list(samples_s.keys())\n",
    "        for k in keys_to_collect:\n",
    "            combined[k] = []\n",
    "\n",
    "    # match your original reshape\n",
    "    for k in keys_to_collect:\n",
    "        combined[k].append(\n",
    "            samples_s[k].reshape(*test_posterior_comp_subjects['mu_nu'].shape)\n",
    "        )\n",
    "\n",
    "test_combined = {}\n",
    "for p in param_names_local:\n",
    "    combined[p] = np.array(combined[p]).transpose(1,0,2,3)\n",
    "    combined[p] = combined[p].reshape(-1, num_samples, 1)\n",
    "    test_combined[p] = test_data_comp_subjects[p].reshape(-1, 1)"
   ],
   "id": "8edf4b2f6f49ee2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_combined['log_alpha'] = np.log(test_combined['alpha'])\n",
    "test_combined['log_t0'] = np.log(test_combined['t0'])\n",
    "combined['log_alpha'] = np.log(combined['alpha'])\n",
    "combined['log_t0'] = np.log(combined['t0'])"
   ],
   "id": "2133eabcded929f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.recovery(\n",
    "    estimates=combined, #test_local_samples_subject,\n",
    "    targets=test_combined, #test_data_local_subject,\n",
    "    variable_names=pretty_param_names_local_log,\n",
    "    variable_keys=['nu', 'alpha', 't0']\n",
    ")\n",
    "#fig.savefig(f\"{storage}plots/partial_pooling_local_subject_recovery.png\")"
   ],
   "id": "e53569e8735a32ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metrics = {\n",
    "    'NRMSE': bf.diagnostics.metrics.root_mean_squared_error(combined, test_combined)['values'],\n",
    "    'NRMSE-mad': bf.diagnostics.metrics.root_mean_squared_error(combined, test_combined,\n",
    "                                                                aggregation=median_abs_deviation)['values'],\n",
    "    'calibration_error': bf.diagnostics.metrics.calibration_error(combined, test_combined)['values'],\n",
    "    'calibration_error-mad': bf.diagnostics.metrics.calibration_error(combined, test_combined,\n",
    "                                                                      aggregation=median_abs_deviation)['values'],\n",
    "}\n",
    "\n",
    "with open(f'partial_pooling_local_metrics.pkl', 'wb') as f:\n",
    "    pickle.dump(metrics, f)\n",
    "metrics"
   ],
   "id": "fb7c9662f7c3cf0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8e719f5e671fd5ab",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
