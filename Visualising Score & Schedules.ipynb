{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009b6adf",
   "metadata": {},
   "source": "# Visualising Score & Schedules of the Diffusion Model\n"
  },
  {
   "cell_type": "code",
   "id": "d5f88a59",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "else:\n",
    "    print(f\"Using '{os.environ['KERAS_BACKEND']}' backend\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import timeit\n",
    "\n",
    "import keras\n",
    "import bayesflow as bf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import norm\n",
    "sech = lambda x: 1 / np.cosh(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simulator<a class=\"anchor\" id=\"simulator\"></a>",
   "id": "c63b26ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def theta_prior():\n",
    "    theta = np.random.uniform(-1, 1, 2)\n",
    "    return dict(theta=theta)\n",
    "\n",
    "def forward_model(theta):\n",
    "    alpha = np.random.uniform(-np.pi / 2, np.pi / 2)\n",
    "    r = np.random.normal(0.1, 0.01)\n",
    "    x1 = -np.abs(theta[0] + theta[1]) / np.sqrt(2) + r * np.cos(alpha) + 0.25\n",
    "    x2 = (-theta[0] + theta[1]) / np.sqrt(2) + r * np.sin(alpha)\n",
    "    return dict(x=np.array([x1, x2]))"
   ],
   "id": "f761b142a0e1da66",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4b89c861527c13b8",
   "metadata": {},
   "source": [
    "simulator = bf.make_simulator([theta_prior, forward_model])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c9c2dc70f53d103",
   "metadata": {},
   "source": [
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .to_array()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .rename(\"theta\", \"inference_variables\")\n",
    "    .rename(\"x\", \"inference_conditions\")\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "35291b6d847c2fb1"
  },
  {
   "cell_type": "code",
   "id": "39cb5a1c9824246f",
   "metadata": {},
   "source": [
    "num_training_batches = 256\n",
    "num_validation_sets = 300\n",
    "batch_size = 128\n",
    "epochs = 1000"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9dee7252ef99affa",
   "metadata": {},
   "source": [
    "training_data = simulator.sample(num_training_batches * batch_size)\n",
    "validation_data = simulator.sample(num_validation_sets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from bayesflow.networks.diffusion_model import EDMNoiseSchedule",
   "id": "cbe1f1358932d520",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import math\n",
    "# from typing import Literal\n",
    "#\n",
    "# from keras import ops\n",
    "#\n",
    "# from bayesflow.types import Tensor\n",
    "# from bayesflow.utils.serialization import deserialize, serializable\n",
    "# from bayesflow.networks.diffusion_model.schedules import NoiseSchedule\n",
    "#\n",
    "# # disable module check, use potential module after moving from experimental\n",
    "# @serializable(\"bayesflow.networks\", disable_module_check=True)\n",
    "# class TrigFlowNoiseSchedule(NoiseSchedule):\n",
    "#     \"\"\"TrigFlow noise schedule for diffusion models. This schedule is based on [1].\n",
    "#\n",
    "#     A continuous-time noise schedule that uses trigonometric functions to define the log signal-to-noise.\n",
    "#\n",
    "#     [1] Lu, C., & Song, Y. (2025). Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models\n",
    "#     \"\"\"\n",
    "#\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         target_std: float = 1.0,\n",
    "#         weighting: Literal[\"sigmoid\", \"likelihood_weighting\"] = \"sigmoid\",\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Initialize the cosine noise schedule.\n",
    "#\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         target_std : float, optional\n",
    "#             The target standard deviation of the noise at the end of the diffusion process. Default is\n",
    "#             1.0, should be the same as the standard deviation of the target at t=0.\n",
    "#         weighting : Literal[\"sigmoid\", \"likelihood_weighting\"], optional\n",
    "#             The type of weighting function to use for the noise schedule. Default is \"sigmoid\".\n",
    "#         \"\"\"\n",
    "#         super().__init__(name=\"trig_flow_noise_schedule\", variance_type=\"preserving\", weighting=weighting)\n",
    "#         self.target_std = target_std\n",
    "#         self._weighting = weighting\n",
    "#\n",
    "#         self._t_min = 0 + 1e-5\n",
    "#         self._t_max = math.pi / 2\n",
    "#\n",
    "#         self.log_snr_max = self.get_log_snr(ops.convert_to_tensor(0.0), training=False)\n",
    "#         self.log_snr_min = self.get_log_snr(ops.convert_to_tensor(1.0), training=False)\n",
    "#\n",
    "#     def _truncated_t(self, t: Tensor) -> Tensor:\n",
    "#         return self._t_min + (self._t_max - self._t_min) * t\n",
    "#\n",
    "#     def get_alpha_sigma(self, log_snr_t: Tensor) -> tuple[Tensor, Tensor]:\n",
    "#         \"\"\"Get alpha and sigma for a given log signal-to-noise ratio (lambda).\"\"\"\n",
    "#         t = self.get_t_from_log_snr(log_snr_t=log_snr_t, training=False)\n",
    "#\n",
    "#         alpha = ops.cos(t)\n",
    "#         sigma = ops.sin(t) * self.target_std\n",
    "#         return alpha, sigma\n",
    "#\n",
    "#     def get_log_snr(self, t: Tensor | float, training: bool) -> Tensor:\n",
    "#         \"\"\"Get the log signal-to-noise ratio (lambda) for a given diffusion time.\"\"\"\n",
    "#         t_trunc = self._truncated_t(t)\n",
    "#         return ops.log(ops.cos(t_trunc) ** 2 / (ops.sin(t_trunc) ** 2 * self.target_std))\n",
    "#\n",
    "#     def get_t_from_log_snr(self, log_snr_t: Tensor | float, training: bool) -> Tensor:\n",
    "#         \"\"\"Get the diffusion time (t) from the log signal-to-noise ratio (lambda).\"\"\"\n",
    "#         return ops.arcsin(1 / ops.sqrt(1 + self.target_std * ops.exp(log_snr_t)))\n",
    "#\n",
    "#     def derivative_log_snr(self, log_snr_t: Tensor, training: bool) -> Tensor:\n",
    "#         \"\"\"Compute d/dt log(1 + e^(-snr(t))), which is used for the reverse SDE.\"\"\"\n",
    "#         t = self.get_t_from_log_snr(log_snr_t=log_snr_t, training=training)\n",
    "#\n",
    "#         # Compute the truncated time t_trunc\n",
    "#         t_trunc = self._truncated_t(t)\n",
    "#         dsnr_dx = -2 / (ops.sin(t_trunc) * ops.cos(t_trunc))\n",
    "#\n",
    "#         # Using the chain rule on f(t) = log(1 + e^(-snr(t))):\n",
    "#         # f'(t) = - (e^{-snr(t)} / (1 + e^{-snr(t)})) * dsnr_dt\n",
    "#         dsnr_dt = dsnr_dx * (self._t_max - self._t_min)\n",
    "#         factor = ops.exp(-log_snr_t) / (1 + ops.exp(-log_snr_t))\n",
    "#         return -factor * dsnr_dt\n",
    "#\n",
    "#     # def get_weights_for_snr(self, log_snr_t: Tensor) -> Tensor:\n",
    "#     #     alpha, sigma = self.get_alpha_sigma(log_snr_t)\n",
    "#     #     velocity_weighting = 1 / (alpha** 2 * (ops.exp(-log_snr_t) + 1)**2)\n",
    "#     #\n",
    "#     #     additional_weights = super().get_weights_for_snr(log_snr_t=log_snr_t)\n",
    "#     #     return velocity_weighting * additional_weights\n",
    "#\n",
    "#     def get_config(self):\n",
    "#         config = {\n",
    "#             \"target_std\": self.target_std,\n",
    "#             \"weighting\": self._weighting,\n",
    "#         }\n",
    "#         return config\n",
    "#\n",
    "#     @classmethod\n",
    "#     def from_config(cls, config, custom_objects=None):\n",
    "#         return cls(**deserialize(config, custom_objects=custom_objects))\n"
   ],
   "id": "dddb55fad1aa38e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96ca6ffa",
   "metadata": {},
   "source": [
    "workflows = {}\n",
    "\n",
    "for noise_schedule in ['cosine', 'edm', 'edm_ve']:\n",
    "    diffusion_model_workflow = bf.BasicWorkflow(\n",
    "        simulator=simulator,\n",
    "        adapter=adapter,\n",
    "        inference_network = bf.networks.DiffusionModel(\n",
    "            noise_schedule=noise_schedule if noise_schedule != 'edm_ve' else EDMNoiseSchedule(variance_type='exploding'),\n",
    "            prediction_type='velocity' if noise_schedule == 'cosine' else 'F',\n",
    "        ),\n",
    "    )\n",
    "    workflows['diffusion_model_'+noise_schedule] = diffusion_model_workflow\n",
    "\n",
    "for ot in [False, True]:\n",
    "    name = \"flow_matching_ot\" if ot else \"flow_matching\"\n",
    "    for time_sampling_alpha in [0, -0.6]:\n",
    "        if time_sampling_alpha != 0:\n",
    "            continue\n",
    "            name += '_pl'\n",
    "        flow_matching_workflow = bf.BasicWorkflow(\n",
    "            simulator=simulator,\n",
    "            adapter=adapter,\n",
    "            inference_network = bf.networks.FlowMatching(\n",
    "                use_optimal_transport=ot,\n",
    "                time_power_law_alpha=-0.6\n",
    "            ),\n",
    "        )\n",
    "        workflows[name] = flow_matching_workflow"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f496bda",
   "metadata": {},
   "source": [
    "for name, workflow in workflows.items():\n",
    "    if os.path.exists('models/two_moons_'+name+'.keras'):\n",
    "        print(f\"Loading {name} from disk\")\n",
    "        workflow.fit_offline(  # otherwise some building steps are not executed\n",
    "            training_data,\n",
    "            epochs=1,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=validation_data,\n",
    "            verbose=0\n",
    "        )\n",
    "        workflow.approximator = keras.saving.load_model('models/two_moons_'+name+'.keras')\n",
    "        continue\n",
    "\n",
    "    print(f\"Training {name}...\")\n",
    "    start = timeit.default_timer()\n",
    "    workflow.fit_offline(\n",
    "        training_data,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=validation_data,\n",
    "        verbose=2\n",
    "    )\n",
    "    workflow.approximator.save('models/two_moons_'+name+'.keras')\n",
    "    end = timeit.default_timer()\n",
    "    print(f\"Trained {name} in {end - start:.2f} seconds\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2fbe42f-b6e8-45f3-a53a-4015fb84e78f",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(ncols=len(workflows), figsize=(14, 5), layout='constrained', sharey=True, sharex=True)\n",
    "if len(workflows) == 1:\n",
    "    ax = [ax]\n",
    "for i, (name, workflow) in enumerate(workflows.items()):\n",
    "    samples = workflow.sample(num_samples=1000, conditions={\"x\":np.array([[0.0, 0.0]], dtype=np.float32)})\n",
    "    ax[i].scatter(samples[\"theta\"][0, :, 0], samples[\"theta\"][0, :, 1], alpha=0.75, s=0.5, label=\"ODE\")\n",
    "\n",
    "    if name.split('_')[0] == 'diffusion':\n",
    "        samples = workflow.sample(num_samples=1000, method=\"euler_maruyama\",\n",
    "                                  conditions={\"x\":np.array([[0.0, 0.0]], dtype=np.float32)})\n",
    "        ax[i].scatter(samples[\"theta\"][0, :, 0], samples[\"theta\"][0, :, 1], alpha=0.75, s=0.5, label=\"Euler Maruyama\")\n",
    "        ax[i].legend(loc=\"upper right\")\n",
    "\n",
    "    ax[i].set_title(name)\n",
    "    ax[i].set_aspect('equal')\n",
    "ax[-1].set_xlim([-0.5, 0.5])\n",
    "ax[-1].set_ylim([-0.5, 0.5])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualizing the Trajectory",
   "id": "6ba22a899ba3a5e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def euler_backward_like(workflow, conditions, x0=None, steps=200, stochastic_solver=False):\n",
    "    num_samples = 1  # not sure if the code would work with more, but not needed\n",
    "    # conditions must always have shape (batch_size, ..., dims)\n",
    "    conditions_prep = workflow.approximator._prepare_data(conditions)['inference_conditions']\n",
    "    batch_size = keras.ops.shape(conditions_prep)[0]\n",
    "    inference_conditions = keras.ops.expand_dims(conditions_prep, axis=1)\n",
    "    inference_conditions = keras.ops.broadcast_to(\n",
    "                    inference_conditions, (batch_size, num_samples, *keras.ops.shape(inference_conditions)[2:])\n",
    "    )\n",
    "\n",
    "    if workflow.approximator.inference_network.name.split('_')[0] == 'flow':\n",
    "        t_start, t_end = 0.0, 1.0\n",
    "    elif workflow.approximator.inference_network.name.split('_')[0] == 'diffusion':\n",
    "        t_start, t_end = 1.0, 0.0\n",
    "    else:\n",
    "        raise ValueError(\"Unknown inference network type\")\n",
    "\n",
    "    dt = (t_end - t_start) / steps  # negative if integrating toward 0\n",
    "    if x0 is not None:\n",
    "        x = x0\n",
    "    else:\n",
    "        # sample from the base distribution\n",
    "        x = workflow.approximator.inference_network.base_distribution.sample((1, num_samples))\n",
    "    t = float(t_start)\n",
    "\n",
    "    traj = []\n",
    "    vels = []\n",
    "    for k in range(steps):\n",
    "        traj.append(keras.ops.convert_to_numpy(x))\n",
    "        if workflow.inference_network.name.split('_')[0] != 'diffusion':\n",
    "            v_curr = workflow.approximator.inference_network.velocity(\n",
    "                xz=x, time=t, conditions=inference_conditions, training=False\n",
    "            )\n",
    "        else:\n",
    "            # for diffusion models, we can use a stochastic solver\n",
    "            v_curr = workflow.approximator.inference_network.velocity(\n",
    "                xz=x, time=t, conditions=inference_conditions, stochastic_solver=stochastic_solver, training=False\n",
    "            )\n",
    "            if stochastic_solver:\n",
    "                diff_curr = workflow.approximator.inference_network.diffusion_term(\n",
    "                    xz=x, time=t, training=False\n",
    "                )\n",
    "                noise = keras.random.normal(keras.ops.shape(x), dtype=keras.ops.dtype(x)) * np.sqrt(np.abs(dt))\n",
    "                x = x + diff_curr * noise\n",
    "\n",
    "        x = x + dt * v_curr\n",
    "        t = t + dt\n",
    "        vels.append(keras.ops.convert_to_numpy(v_curr))\n",
    "\n",
    "    traj = np.stack(traj, axis=0)      # shape [steps+1, batch, num_samples, dims]\n",
    "    vels = np.stack(vels, axis=0)      # shape [steps,   batch, num_samples, dims]\n",
    "    times = np.linspace(t_start, t_end, steps+1, dtype=np.float32)\n",
    "\n",
    "    traj =  workflow.approximator.standardize_layers[\"inference_variables\"](traj, forward=False)\n",
    "    return traj, vels, times"
   ],
   "id": "5252221790198aee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x0 = workflows[list(workflows.keys())[0]].approximator.inference_network.base_distribution.sample((1, 1)) * 0\n",
    "conditions = {\"x\":np.array([[0.0, 0.0]], dtype=np.float32)}"
   ],
   "id": "adf1e8072985cba4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=len(workflows), figsize=(3*len(workflows), 4),\n",
    "                       layout='constrained', sharey='row', sharex='row')\n",
    "for i, (name, workflow) in enumerate(workflows.items()):\n",
    "    traj, vels, times = euler_backward_like(\n",
    "        workflow, x0=x0, conditions=conditions, steps=200, stochastic_solver=False\n",
    "    )\n",
    "    traj, vels = traj[:, 0, 0], vels[:, 0, 0]  # take first batch item\n",
    "    vel_norm = np.linalg.norm(vels, axis=-1)\n",
    "    ax[0, i].plot(traj[:, 0], traj[:, 1], color='black', label=\"Trajectory\" if i == 0 else None)\n",
    "    ax[0, i].scatter(traj[0, 0], traj[0, 1], s=30, marker='o', label='start' if i == 0 else None, color='black') # start\n",
    "    #ax[0, i].scatter(traj[-1, 0], traj[-1, 1], s=30, marker='x', label='end' if i == 0 else None, color='black') # end\n",
    "\n",
    "    samples = workflow.sample(num_samples=1000, conditions=conditions)\n",
    "    ax[0, i].scatter(samples[\"theta\"][0, :, 0], samples[\"theta\"][0, :, 1], alpha=0.75, s=0.75,\n",
    "                     color='darkviolet', label=\"Posterior Samples\" if i == 0 else None)\n",
    "\n",
    "    ax[1, i].plot(times[:-1], vel_norm)\n",
    "    #ax[1, i].set_title(\"Velocity norm over time\")\n",
    "    ax[1, i].set_xlabel(\"time\")\n",
    "\n",
    "    ax[0, i].set_title(name)\n",
    "    ax[0, i].set_aspect('equal')\n",
    "    ax[0, i].set_xlabel(\"x\")\n",
    "fig.legend(bbox_to_anchor=(1.1, 0.85))\n",
    "ax[0, 0].set_ylabel(\"y\")\n",
    "ax[0, -1].set_xlim([-0.5, 0.5])\n",
    "ax[0, -1].set_ylim([-0.5, 0.5])\n",
    "ax[1, 0].set_ylabel(\"||v||\")\n",
    "plt.show()"
   ],
   "id": "9097a167e6044ebb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def moving_average(x, w=10):\n",
    "    # convolution with same length\n",
    "    y = np.convolve(x, np.ones(w)/w, mode=\"same\")\n",
    "    # force first and last values equal to input edges\n",
    "    y[0] = x[0]\n",
    "    y[-1] = x[-1]\n",
    "    return y"
   ],
   "id": "693846c2802a9237",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(3*2, 4), layout='constrained', sharey='row', sharex='row')\n",
    "for i, (name, workflow) in enumerate(workflows.items()):\n",
    "    if name.split('_')[0] != 'diffusion':\n",
    "        continue  # no stochastic solver\n",
    "\n",
    "    traj, vels, times = euler_backward_like(\n",
    "        workflow, x0=x0, conditions=conditions, steps=200, stochastic_solver=True\n",
    "    )\n",
    "    traj, vels = traj[:, 0, 0], vels[:, 0, 0]  # take first batch item\n",
    "    vel_norm = np.linalg.norm(vels, axis=-1)\n",
    "    x_smooth = moving_average(traj[:,0])\n",
    "    y_smooth = moving_average(traj[:,1])\n",
    "    ax[0, i].plot(x_smooth, y_smooth, color='black', label=\"Trajectory\" if i == 0 else None)\n",
    "    ax[0, i].scatter(x_smooth[0], y_smooth[0], s=30, marker='o', label='start' if i == 0 else None, color='black')\n",
    "    #ax[0, i].scatter(x_smooth[-1], y_smooth[-1], s=30, marker='x', label='end' if i == 0 else None, color='black')\n",
    "\n",
    "    samples = workflow.sample(num_samples=1000, conditions=conditions, method=\"euler_maruyama\")\n",
    "    ax[0, i].scatter(samples[\"theta\"][0, :, 0], samples[\"theta\"][0, :, 1], alpha=0.75, s=0.75,\n",
    "                     color='darkviolet', label=\"Posterior Samples\" if i == 0 else None)\n",
    "\n",
    "    ax[1, i].plot(times[:-1], vel_norm, linewidth=2)\n",
    "    #ax[1, i].set_title(\"Velocity norm over time\")\n",
    "    ax[1, i].set_xlabel(\"time\")\n",
    "\n",
    "    ax[0, i].set_title(name)\n",
    "    ax[0, i].set_aspect('equal')\n",
    "    ax[0, i].set_xlabel(\"x\")\n",
    "fig.legend(bbox_to_anchor=(1.2, 0.85))\n",
    "ax[0, 0].set_ylabel(\"y\")\n",
    "ax[0, -1].set_xlim([-1, 1])\n",
    "ax[0, -1].set_ylim([-1, 1])\n",
    "ax[1, 0].set_ylabel(\"||v||\")\n",
    "plt.show()"
   ],
   "id": "30dc7e1c4e6ab894",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_trajectories = 20\n",
    "colors = [\n",
    "    \"#0072B2\",  # blue (EDM)\n",
    "    \"#E69F00\",  # orange (Cosine)\n",
    "    \"#009E73\",  # green (Flow Matching)\n",
    "    \"#D55E00\",  # red (Flow Matching power law)\n",
    "]"
   ],
   "id": "e49dee10b80ece2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(ncols=len(workflows)+3, figsize=(10, 3),\n",
    "                       layout='constrained', sharey='row', sharex='row')\n",
    "for j in tqdm(range(n_trajectories)):\n",
    "    x0_i = workflows[list(workflows.keys())[0]].approximator.inference_network.base_distribution.sample((1, 1))\n",
    "    for i, (name, workflow) in enumerate(workflows.items()):\n",
    "        if name.split('_')[0] == 'diffusion':\n",
    "            if name.split('diffusion_model_')[1] =='edm':\n",
    "                 ax[i].set_title('EDM VP')\n",
    "                 _color = colors[0]\n",
    "            elif name.split('diffusion_model_')[1] =='edm_ve':\n",
    "                ax[i].set_title('EDM VE')\n",
    "                _color = colors[0]\n",
    "            else:\n",
    "                ax[i].set_title(name.split('diffusion_model_')[1].title())\n",
    "                _color = colors[1]\n",
    "        else:\n",
    "            if name == 'flow_matching_ot':\n",
    "                ax[i].set_title('Flow Matching OT')\n",
    "                _color = colors[2]\n",
    "            else:\n",
    "                ax[i].set_title('Flow Matching')\n",
    "                _color = colors[2]\n",
    "\n",
    "        traj, vels, times = euler_backward_like(\n",
    "            workflow, x0=x0_i, conditions=conditions, steps=200, stochastic_solver=False\n",
    "        )\n",
    "        traj, vels = traj[:, 0, 0], vels[:, 0, 0]  # take first batch item\n",
    "        ax[i].plot(traj[:, 0], traj[:, 1], color=_color, alpha=0.75,\n",
    "                   label=\"Trajectory\" if i == 0 and j == 0 else None)\n",
    "        ax[i].scatter(traj[0, 0], traj[0, 1], s=10, marker='o',\n",
    "                      label='start' if i == 0 and j == 0 else None, color=_color)\n",
    "\n",
    "        if j == (n_trajectories-1):\n",
    "            samples = workflow.sample(num_samples=1000, conditions=conditions)\n",
    "            ax[i].scatter(samples[\"theta\"][0, :, 0], samples[\"theta\"][0, :, 1], alpha=0.75, s=0.6,\n",
    "                          color='gray', label=\"Posterior Samples\" if i == 0 else None, zorder=4)\n",
    "        ax[i].set_aspect('equal')\n",
    "        ax[i].set_xlabel(r\"$\\theta_1$\")\n",
    "\n",
    "n_trajectories_sde = int(n_trajectories*0.25)\n",
    "for j in tqdm(range(n_trajectories_sde)):\n",
    "    x0_i = workflows[list(workflows.keys())[0]].approximator.inference_network.base_distribution.sample((1, 1))\n",
    "    new_i = 0\n",
    "    for i, (name, workflow) in enumerate(workflows.items()):\n",
    "        if name.split('_')[0] != 'diffusion':\n",
    "            continue  # no stochastic solver\n",
    "        if name.split('diffusion_model_')[1] =='edm':\n",
    "             ax[len(workflows)+new_i].set_title('EDM VP' + ' (SDE)')\n",
    "             _color = colors[0]\n",
    "        elif name.split('diffusion_model_')[1] =='edm_ve':\n",
    "            ax[len(workflows)+new_i].set_title('EDM VE' + ' (SDE)')\n",
    "            _color = colors[0]\n",
    "        else:\n",
    "            ax[len(workflows)+new_i].set_title(name.split('diffusion_model_')[1].title() + ' (SDE)')\n",
    "            _color = colors[1]\n",
    "\n",
    "        traj, vels, times = euler_backward_like(\n",
    "            workflow, x0=x0_i, conditions=conditions, steps=200, stochastic_solver=True\n",
    "        )\n",
    "        traj, vels = traj[:, 0, 0], vels[:, 0, 0]  # take first batch item\n",
    "        x_smooth = moving_average(traj[:,0])\n",
    "        y_smooth = moving_average(traj[:,1])\n",
    "        ax[len(workflows)+new_i].plot(x_smooth, y_smooth, color=_color, alpha=0.75)\n",
    "        ax[len(workflows)+new_i].scatter(x_smooth[0], y_smooth[0], s=10, marker='o', color=_color)\n",
    "\n",
    "        if j == (n_trajectories_sde-1):\n",
    "            samples = workflow.sample(num_samples=1000, conditions=conditions, method=\"euler_maruyama\")\n",
    "            ax[len(workflows)+new_i].scatter(samples[\"theta\"][0, :, 0], samples[\"theta\"][0, :, 1], alpha=0.75, s=0.6,\n",
    "                          color='gray', zorder=4)\n",
    "\n",
    "        ax[len(workflows)+new_i].set_aspect('equal')\n",
    "        ax[len(workflows)+new_i].set_xlabel(r\"$\\theta_1$\")\n",
    "        new_i += 1\n",
    "\n",
    "handles = [\n",
    "    plt.scatter([], [], color='black', marker='o', s=30, label='Initial Point (t=1)'),\n",
    "    Line2D([0], [0], color='black', label='Trajectory'),\n",
    "    plt.scatter([], [], color='gray', marker='x', alpha=0.75, label='Posterior Samples (t=0)'),\n",
    "]\n",
    "fig.legend(handles=handles, bbox_to_anchor=(0.5, 0), loc='lower center', ncols=3)\n",
    "ax[0].set_ylabel(r\"$\\theta_2$\")\n",
    "ax[-1].set_xlim([-1.5, 1.5])\n",
    "ax[-1].set_ylim([-1.5, 1.5])\n",
    "for a in ax:\n",
    "    for item in ([a.title, a.xaxis.label, a.yaxis.label] + a.get_xticklabels() + a.get_yticklabels()):\n",
    "        item.set_fontsize(10)\n",
    "plt.savefig('trajectories.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "db2d0954ae27b194",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(3*3, 4),\n",
    "                       layout='constrained', sharey='row', sharex='row')\n",
    "for j in tqdm(range(int(n_trajectories*0.1))):\n",
    "    x0_i = workflows[list(workflows.keys())[0]].approximator.inference_network.base_distribution.sample((1, 1))\n",
    "    for i, (name, workflow) in enumerate(workflows.items()):\n",
    "        if name.split('_')[0] != 'diffusion':\n",
    "            continue  # no stochastic solver\n",
    "\n",
    "        traj, vels, times = euler_backward_like(\n",
    "            workflow, x0=x0_i, conditions=conditions, steps=200, stochastic_solver=True\n",
    "        )\n",
    "        traj, vels = traj[:, 0, 0], vels[:, 0, 0]  # take first batch item\n",
    "        x_smooth = moving_average(traj[:,0])\n",
    "        y_smooth = moving_average(traj[:,1])\n",
    "        ax[i].plot(x_smooth, y_smooth, color='black', alpha=0.75,\n",
    "                   label=\"Trajectory\" if i == 0 and j == 0 else None)\n",
    "        ax[i].scatter(x_smooth[0], y_smooth[0], s=30, marker='o',\n",
    "                      label='start' if i == 0 and j == 0 else None, color='black')\n",
    "        #ax[i].scatter(x_smooth[-1], y_smooth[-1], s=30, marker='x',\n",
    "        #              label='end' if i == 0 and j == 0 else None, color='black')\n",
    "\n",
    "        if j == 0:\n",
    "            samples = workflow.sample(num_samples=1000, conditions=conditions, method=\"euler_maruyama\")\n",
    "            ax[i].scatter(samples[\"theta\"][0, :, 0], samples[\"theta\"][0, :, 1], alpha=0.75, s=0.4,\n",
    "                          color='darkviolet', label=\"Posterior Samples\" if i == 0 else None)\n",
    "\n",
    "        ax[i].set_title(name)\n",
    "        ax[i].set_aspect('equal')\n",
    "        ax[i].set_xlabel(\"x\")\n",
    "fig.legend(bbox_to_anchor=(1.2, 0.6))\n",
    "ax[0].set_ylabel(\"y\")\n",
    "ax[-1].set_xlim([-1.5, 1.5])\n",
    "ax[-1].set_ylim([-1.5, 1.5])\n",
    "plt.show()"
   ],
   "id": "e8fc20e1f0f1a0f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def velocity_field_plot(workflow, conditions, times, stochastic_solver=False,\n",
    "                        traj=None, grid_limits=(-3,3), grid_points=20, name=None):\n",
    "    # grid\n",
    "    x = np.linspace(grid_limits[0], grid_limits[1], grid_points)\n",
    "    y = np.linspace(grid_limits[0], grid_limits[1], grid_points)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    grid = np.stack([X, Y], axis=-1)  # [grid_points, grid_points, 2]\n",
    "    grid = grid.reshape(-1, 2)[None]  # [1, grid_points*grid_points, 2]\n",
    "\n",
    "    grid_transf = workflow.approximator.standardize_layers[\"inference_variables\"](grid.reshape(-1, 2), forward=False)\n",
    "    grid_transf = keras.ops.convert_to_numpy(grid_transf)\n",
    "    XY_transf = grid_transf.reshape(grid_points, grid_points, 2) # [G, G, 2]\n",
    "    X_transf = XY_transf[..., 0]\n",
    "    Y_transf = XY_transf[..., 1]\n",
    "\n",
    "    # conditions must always have shape (batch_size, ..., dims)\n",
    "    conditions_prep = workflow.approximator._prepare_data(conditions)['inference_conditions']\n",
    "    batch_size = keras.ops.shape(conditions_prep)[0]\n",
    "    inference_conditions = keras.ops.expand_dims(conditions_prep, axis=1)\n",
    "    inference_conditions = keras.ops.broadcast_to(\n",
    "                    inference_conditions, (batch_size, grid_points*grid_points, *keras.ops.shape(inference_conditions)[2:])\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(times), figsize=(5*len(times), 5), layout='constrained',\n",
    "                             sharex=True, sharey=True, squeeze=False)\n",
    "\n",
    "    for i, t in enumerate(times):\n",
    "        # expand to shape [batch, num_samples, dim]\n",
    "        if workflow.inference_network.name.split('_')[0] != 'diffusion':\n",
    "            v = workflow.inference_network.velocity(\n",
    "                xz=grid, time=float(t), conditions=inference_conditions, training=False\n",
    "            )\n",
    "        else:\n",
    "            # for diffusion models, we can use a stochastic solver\n",
    "            v = workflow.inference_network.velocity(\n",
    "                xz=grid, time=float(t), conditions=inference_conditions, stochastic_solver=stochastic_solver, training=False\n",
    "            )\n",
    "        v = keras.ops.convert_to_numpy(v)\n",
    "        v = v[0, :, :2]  # [num_points, 2]\n",
    "\n",
    "        U = v[:,0].reshape(grid_points, grid_points)\n",
    "        V = v[:,1].reshape(grid_points, grid_points)\n",
    "\n",
    "        ax = axes[0, i]\n",
    "        ax.quiver(X_transf, Y_transf, U, -V, angles=\"xy\", label=\"velocity field\", alpha=0.5)\n",
    "        if traj is not None:\n",
    "            ax.plot(traj[:,0], traj[:,1], color=\"red\", linewidth=2, label=\"trajectory\")\n",
    "            ax.scatter(traj[0,0], traj[0,1], color=\"green\", s=60, label=\"start\")\n",
    "            ax.scatter(traj[-1,0], traj[-1,1], color=\"black\", s=60, label=\"end\")\n",
    "\n",
    "        if name is not None:\n",
    "            ax.set_title(f\"{name} time={t:.2f}\")\n",
    "        else:\n",
    "            ax.set_title(f\"time={t:.2f}\")\n",
    "        ax.set_xlim((X_transf.min(), X_transf.max()))\n",
    "        ax.set_ylim((Y_transf.min(), Y_transf.max()))\n",
    "        ax.legend()\n",
    "    plt.show()"
   ],
   "id": "4a9a5b4a34ec9c29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_times = [1.0, 0.5, 0.0]\n",
    "for name, workflow in workflows.items():\n",
    "    velocity_field_plot(\n",
    "        workflow,\n",
    "        conditions=conditions,\n",
    "        times=plot_times if name.split('_')[0] == 'diffusion' else plot_times[::-1],\n",
    "        stochastic_solver=False,\n",
    "        name=name\n",
    "    )"
   ],
   "id": "99f4443291310b3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualizing the Schedules",
   "id": "fa85e1cb9eef5b94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from bayesflow.networks.diffusion_model.schedules import EDMNoiseSchedule, CosineNoiseSchedule, NoiseSchedule",
   "id": "ef6ddb2da13bf8b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FlowMatching(NoiseSchedule):\n",
    "    def __init__(self, name=\"Flow Matching Schedule\"):\n",
    "        super().__init__(name=name, variance_type=\"preserving\", weighting=None)\n",
    "\n",
    "    def get_log_snr(self, t, training):\n",
    "        \"\"\"Get the log signal-to-noise ratio (lambda) for a given diffusion time.\"\"\"\n",
    "        return 2 * keras.ops.log((1-t)/t)\n",
    "\n",
    "    def get_alpha_sigma(self, t):\n",
    "        alpha_t = 1 - t\n",
    "        sigma_t = 1 - alpha_t\n",
    "        return alpha_t, sigma_t\n",
    "\n",
    "    def get_t_from_log_snr(self, log_snr_t, training: bool):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def derivative_log_snr(self, log_snr_t, training):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_weights_for_snr(self, log_snr_t):\n",
    "        return 1 + keras.ops.exp(-log_snr_t) + 2*keras.ops.exp(-log_snr_t / 2)"
   ],
   "id": "2428482878b57571",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sample_powerlaw(alpha, size=1):\n",
    "    \"\"\"\n",
    "    Sample from distribution with CDF p(t) ∝ t^(1/(1+α)) on [0,1]\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    alpha : float\n",
    "        Shape parameter. α = 0 gives uniform distribution\n",
    "    size : int or tuple\n",
    "        Number of samples to generate\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    samples : ndarray\n",
    "        Samples from the distribution\n",
    "    \"\"\"\n",
    "    # Generate uniform random samples\n",
    "    u = np.random.uniform(0, 1, size)\n",
    "\n",
    "    # Since p(t) ∝ t^(1/(1+α)) is the CDF, we need the inverse\n",
    "    # Normalized CDF: F(t) = t^(1/(1+α))\n",
    "    # Inverse CDF: F^(-1)(u) = u^(1+α)\n",
    "    return 1 - u ** (1 + alpha)"
   ],
   "id": "cf3867346f3efe58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "edm = EDMNoiseSchedule()\n",
    "edm_ve = EDMNoiseSchedule(variance_type='exploding')\n",
    "cosine = CosineNoiseSchedule(weighting=None)\n",
    "edm.name = \"EDM Schedule\"# VP\"\n",
    "edm_ve.name = \"EDM Schedule VE\"\n",
    "cosine.name = \"Cosine Schedule\"\n",
    "fm = FlowMatching()\n",
    "fm_pl = FlowMatching(r\"Flow Matching Schedule $\\rho=-0.6$\")\n",
    "\n",
    "time = keras.ops.linspace(0.0, 1, 10000)\n",
    "# gebhard: t^(1/4) sim U(0,1)\n",
    "#fm_time = (1-time) ** 4\n",
    "# power law with alpha=-0.6\n",
    "fm_time = sample_powerlaw(alpha=-0.6, size=time.shape[0])\n",
    "fm_time = np.sort(fm_time)\n",
    "fm_time = keras.ops.convert_to_tensor(fm_time)\n",
    "colors = [\n",
    "    \"#0072B2\",  # blue (EDM)\n",
    "    \"#E69F00\",  # orange (Cosine)\n",
    "    \"#009E73\",  # green (Flow Matching)\n",
    "    \"#D55E00\",  # red (Flow Matching power law)\n",
    "]\n",
    "schedules = [edm, #edm_ve,\n",
    "             cosine, fm, fm_pl]"
   ],
   "id": "fc4614be04485301",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axis = plt.subplots(1, 2, figsize=(10,3), layout='constrained', gridspec_kw={'width_ratios': [1.75, 1]})\n",
    "ax = axis[0]\n",
    "for i, schedule in enumerate(schedules):\n",
    "    training_schedule = keras.ops.convert_to_numpy(schedule.get_log_snr(time, training=True))\n",
    "    inference_schedule = keras.ops.convert_to_numpy(schedule.get_log_snr(time, training=False))\n",
    "\n",
    "    if (training_schedule != inference_schedule).all():\n",
    "        ax.plot(time, training_schedule, label=f\"{schedule.name} Training\", color=colors[i])\n",
    "        ax.plot(time, inference_schedule, label=f\"{schedule.name} Inference\", linestyle=\"--\", color=colors[i])\n",
    "    else:\n",
    "        if schedule.name == r'Flow Matching Schedule $\\rho=-0.6$':\n",
    "            ax.plot(time, training_schedule, label=f\"{schedule.name}\", color=colors[i], linestyle=\"--\")\n",
    "        else:\n",
    "            ax.plot(time, training_schedule, label=f\"{schedule.name}\", color=colors[i])\n",
    "ax.legend(loc=\"lower left\")\n",
    "ax.set_ylabel(r\"log SNR $\\lambda_t$\")\n",
    "ax.set_xlabel(\"time\")\n",
    "# increase font size\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(12)\n",
    "#plt.savefig('schedules.pdf', bbox_inches='tight')\n",
    "#plt.show()\n",
    "\n",
    "#fig, ax = plt.subplots(ncols=1, figsize=(4,3), layout='constrained', sharey=True)\n",
    "ax = axis[1]\n",
    "for i, schedule in enumerate(schedules):\n",
    "    if schedule.name == r'Flow Matching Schedule $\\rho=-0.6$':\n",
    "        training_schedule = schedule.get_log_snr(fm_time[1:-1], training=True)\n",
    "    else:\n",
    "        training_schedule = schedule.get_log_snr(time[1:-1], training=True)\n",
    "    training_weights = keras.ops.convert_to_numpy(schedule.get_weights_for_snr(training_schedule))\n",
    "    log_snr = np.random.choice(training_schedule, p=training_weights / training_weights.sum(), replace=True, size=10000)\n",
    "    ax.hist(log_snr, density=True, color=colors[i], label=f\"{schedule.name}\", alpha=0.7, bins=20)\n",
    "\n",
    "ax.set_xlabel(r\"$\\lambda$ (log SNR)\")\n",
    "ax.set_ylabel(\"Weighting Density\")\n",
    "#ax.set_xlim([-8.5, 12])\n",
    "#ax.set_ylim([-0, 0.6])\n",
    "ax.legend(labels=[s.name for s in schedules], loc=\"upper right\")\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(12)\n",
    "plt.savefig('schedules.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "e348df0a230f5b38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "91083c40b43c8eec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "from typing import Literal\n",
    "from keras import ops\n",
    "from bayesflow.types import Tensor\n",
    "\n",
    "class EDMNoiseScheduleNoWeight(NoiseSchedule):\n",
    "    \"\"\"EDM noise schedule for diffusion models. This schedule is based on the EDM paper [1].\n",
    "    This should be used with the F-prediction type in the diffusion model.\n",
    "\n",
    "    [1] Karras, T., Aittala, M., Aila, T., & Laine, S. (2022). Elucidating the design space of diffusion-based\n",
    "    generative models. Advances in Neural Information Processing Systems, 35, 26565-26577.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        weighting,\n",
    "        sigma_data: float = 1.0,\n",
    "        sigma_min: float = 1e-4,\n",
    "        sigma_max: float = 80.0,\n",
    "        variance_type: Literal[\"preserving\", \"exploding\"] = \"preserving\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the EDM noise schedule.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sigma_data : float, optional\n",
    "            The standard deviation of the output distribution. Input of the network is scaled by this factor and\n",
    "            the weighting function is scaled by this factor as well. Default is 1.0.\n",
    "        sigma_min : float, optional\n",
    "            The minimum noise level. Only relevant for sampling. Default is 1e-4.\n",
    "        sigma_max : float, optional\n",
    "            The maximum noise level. Only relevant for sampling. Default is 80.0.\n",
    "        variance_type : Literal[\"preserving\", \"exploding\"], optional\n",
    "            The type of variance to use. Default is \"preserving\". Original EDM paper uses \"exploding\".\n",
    "        \"\"\"\n",
    "        super().__init__(name=\"edm_noise_schedule\", variance_type=variance_type, weighting=weighting)\n",
    "        self.sigma_data = sigma_data\n",
    "        # training settings\n",
    "        self.p_mean = -1.2\n",
    "        self.p_std = 1.2\n",
    "        # sampling settings\n",
    "        self.sigma_max = sigma_max\n",
    "        self.sigma_min = sigma_min\n",
    "        self.rho = 7\n",
    "\n",
    "        # convert EDM parameters to signal-to-noise ratio formulation\n",
    "        self.log_snr_min = -2 * ops.log(sigma_max)\n",
    "        self.log_snr_max = -2 * ops.log(sigma_min)\n",
    "        # t is not truncated for EDM by definition of the sampling schedule\n",
    "        # training bounds should be set to avoid numerical issues\n",
    "        self._log_snr_min_training = self.log_snr_min - 1  # one is never sampler during training\n",
    "        self._log_snr_max_training = self.log_snr_max + 1  # 0 is almost surely never sampled during training\n",
    "\n",
    "    def get_log_snr(self, t: float | Tensor, training: bool) -> Tensor:\n",
    "        \"\"\"Get the log signal-to-noise ratio (lambda) for a given diffusion time.\"\"\"\n",
    "        if training:\n",
    "            # SNR = dist.icdf(1-t)  # Kingma paper wrote -F(t) but this seems to be wrong\n",
    "            loc = -2 * self.p_mean\n",
    "            scale = 2 * self.p_std\n",
    "            snr = loc + scale * ops.erfinv(2 * (1 - t) - 1) * math.sqrt(2)\n",
    "            snr = ops.clip(snr, x_min=self._log_snr_min_training, x_max=self._log_snr_max_training)\n",
    "        else:\n",
    "            sigma_min_rho = self.sigma_min ** (1 / self.rho)\n",
    "            sigma_max_rho = self.sigma_max ** (1 / self.rho)\n",
    "            snr = -2 * self.rho * ops.log(sigma_max_rho + (1 - t) * (sigma_min_rho - sigma_max_rho))\n",
    "        return snr\n",
    "\n",
    "    def get_t_from_log_snr(self, log_snr_t: float | Tensor, training: bool) -> Tensor:\n",
    "        \"\"\"Get the diffusion time (t) from the log signal-to-noise ratio (lambda).\"\"\"\n",
    "        if training:\n",
    "            # SNR = dist.icdf(1-t) => t = 1-dist.cdf(snr)  # Kingma paper wrote -F(t) but this seems to be wrong\n",
    "            loc = -2 * self.p_mean\n",
    "            scale = 2 * self.p_std\n",
    "            x = log_snr_t\n",
    "            t = 1 - 0.5 * (1 + ops.erf((x - loc) / (scale * math.sqrt(2.0))))\n",
    "        else:  # sampling\n",
    "            # SNR = -2 * rho * log(sigma_max ** (1/rho) + (1 - t) * (sigma_min ** (1/rho) - sigma_max ** (1/rho)))\n",
    "            # => t = 1 - ((exp(-snr/(2*rho)) - sigma_max ** (1/rho)) / (sigma_min ** (1/rho) - sigma_max ** (1/rho)))\n",
    "            sigma_min_rho = self.sigma_min ** (1 / self.rho)\n",
    "            sigma_max_rho = self.sigma_max ** (1 / self.rho)\n",
    "            t = 1 - ((ops.exp(-log_snr_t / (2 * self.rho)) - sigma_max_rho) / (sigma_min_rho - sigma_max_rho))\n",
    "        return t\n",
    "\n",
    "    def derivative_log_snr(self, log_snr_t: Tensor, training: bool = False) -> Tensor:\n",
    "        \"\"\"Compute d/dt log(1 + e^(-snr(t))), which is used for the reverse SDE.\"\"\"\n",
    "        if training:\n",
    "            raise NotImplementedError(\"Derivative of log SNR is not implemented for training mode.\")\n",
    "        # sampling mode\n",
    "        t = self.get_t_from_log_snr(log_snr_t=log_snr_t, training=training)\n",
    "\n",
    "        # SNR = -2*rho*log(s_max + (1 - x)*(s_min - s_max))\n",
    "        s_max = self.sigma_max ** (1 / self.rho)\n",
    "        s_min = self.sigma_min ** (1 / self.rho)\n",
    "        u = s_max + (1 - t) * (s_min - s_max)\n",
    "        # d/dx snr = 2*rho*(s_min - s_max) / u\n",
    "        dsnr_dx = 2 * self.rho * (s_min - s_max) / u\n",
    "\n",
    "        # Using the chain rule on f(t) = log(1 + e^(-snr(t))):\n",
    "        # f'(t) = - (e^{-snr(t)} / (1 + e^{-snr(t)})) * dsnr_dt\n",
    "        factor = ops.exp(-log_snr_t) / (1 + ops.exp(-log_snr_t))\n",
    "        return -factor * dsnr_dx\n",
    "\n",
    "    # def get_weights_for_snr(self, log_snr_t: Tensor) -> Tensor:\n",
    "    #     \"\"\"Get weights for the signal-to-noise ratio (snr) for a given log signal-to-noise ratio (lambda).\"\"\"\n",
    "    #     # for F-loss: w = (ops.exp(-log_snr_t) + sigma_data^2) / (ops.exp(-log_snr_t)*sigma_data^2)\n",
    "    #     return 1 + ops.exp(-log_snr_t) / ops.square(self.sigma_data)"
   ],
   "id": "af81958eed8f0908",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot:\n",
    "# - EDM schedule with different weighting functions, F parameterization\n",
    "# - EDM schedule with different weighting functions, e parameterization\n",
    "# - Cosine schedule with different weighting functions, e parameterization\n",
    "edm = EDMNoiseSchedule()\n",
    "cosine = CosineNoiseSchedule(weighting=None)\n",
    "schedules = [edm, cosine]\n",
    "time = keras.ops.linspace(0.0, 1, 10000)\n",
    "\n",
    "fig, axis = plt.subplots(3, 3, figsize=(10,6), sharey='row', sharex=True, layout='constrained')\n",
    "ax = axis[0]\n",
    "max_w = 700\n",
    "for i, schedule in enumerate(schedules):\n",
    "    training_schedule = schedule.get_log_snr(time, training=True)\n",
    "    no_weight_schedule = CosineNoiseSchedule(weighting=None)\n",
    "    training_weights = keras.ops.convert_to_numpy(no_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    ax[0].plot(training_schedule, training_weights/ training_weights.max(), color=colors[i], label=f\"{schedule.name}\", alpha=0.7)\n",
    "    ax[0].set_xlabel(r\"$\\lambda$ (log SNR)\")\n",
    "    #ax[0].legend(labels=[s.name for s in schedules], loc=\"upper right\")\n",
    "    ax[0].set_title('No weighting')\n",
    "\n",
    "    # training_schedule = schedule.get_log_snr(time[0:-1], training=True)\n",
    "    # edm_weight_schedule = EDMNoiseSchedule()\n",
    "    # training_weights = keras.ops.convert_to_numpy(edm_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    # ax[1].plot(training_schedule, training_weights/max_w, color=colors[i], label=f\"{schedule.name}\", alpha=0.7)\n",
    "    # max_w = training_weights.max()\n",
    "    # ax[1].set_xlabel(r\"$\\lambda$ (log SNR)\")\n",
    "    # #ax[1].legend(labels=[s.name for s in schedules], loc=\"upper right\")\n",
    "    # ax[1].set_title('EDM weighting')\n",
    "    # ax[1].set_ylim(0, 1)\n",
    "\n",
    "    training_schedule = schedule.get_log_snr(time, training=True)\n",
    "    sigmoid_weight_schedule = CosineNoiseSchedule(weighting='sigmoid')\n",
    "    training_weights = keras.ops.convert_to_numpy(sigmoid_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    ax[1].plot(training_schedule, training_weights, color=colors[i], label=f\"{schedule.name}\", alpha=0.7)\n",
    "    ax[1].set_xlabel(r\"$\\lambda$ (log SNR)\")\n",
    "    #ax[2].legend(labels=[s.name for s in schedules], loc=\"upper right\")\n",
    "    ax[1].set_title('Sigmoid weighting')\n",
    "\n",
    "    training_schedule = schedule.get_log_snr(time, training=True)\n",
    "    if i == 0:\n",
    "        llh_weight_schedule = EDMNoiseScheduleNoWeight(weighting='likelihood_weighting')\n",
    "    else:\n",
    "        llh_weight_schedule = CosineNoiseSchedule(weighting='likelihood_weighting')\n",
    "    training_weights = keras.ops.convert_to_numpy(llh_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    ax[2].plot(training_schedule, training_weights/ training_weights.max(), color=colors[i], label=f\"{schedule.name}\", alpha=0.7)\n",
    "    ax[2].set_xlabel(r\"$\\lambda$ (log SNR)\")\n",
    "    ax[2].set_title('Likelihood weighting')\n",
    "ax[0].set_ylabel(\"Weighting Function\\n\"+r\"($\\boldsymbol{\\epsilon}$-loss)\")\n",
    "ax[0].set_ylim(-0.1, 1.1)\n",
    "\n",
    "ax = axis[1]\n",
    "for i, schedule in enumerate(schedules):\n",
    "    training_schedule = schedule.get_log_snr(time, training=True)\n",
    "    no_weight_schedule = CosineNoiseSchedule(weighting=None)\n",
    "    training_weights = keras.ops.convert_to_numpy(no_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    log_snr = np.random.choice(training_schedule, p=training_weights / training_weights.sum(), replace=True, size=10000)\n",
    "    ax[0].hist(log_snr, density=True, color=colors[i], label=f\"{schedule.name}\", alpha=0.7, bins=20)\n",
    "    ax[0].set_xlabel(r\"$\\lambda$ (log SNR)\")\n",
    "    #ax[0].legend(labels=[s.name for s in schedules], loc=\"upper right\")\n",
    "\n",
    "    # training_schedule = schedule.get_log_snr(time[0:-1], training=True)\n",
    "    # edm_weight_schedule = EDMNoiseSchedule()\n",
    "    # training_weights = keras.ops.convert_to_numpy(edm_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    # log_snr = np.random.choice(training_schedule, p=training_weights / training_weights.sum(), replace=True, size=10000)\n",
    "    # ax[1].hist(log_snr, density=True, color=colors[i], label=f\"{schedule.name}\", alpha=0.7, bins=20)\n",
    "    # ax[1].set_xlabel(r\"$\\lambda$ (log SNR)\")\n",
    "    # #ax[1].legend(labels=[s.name for s in schedules], loc=\"upper right\")\n",
    "\n",
    "    training_schedule = schedule.get_log_snr(time, training=True)\n",
    "    sigmoid_weight_schedule = CosineNoiseSchedule(weighting='sigmoid')\n",
    "    training_weights = keras.ops.convert_to_numpy(sigmoid_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    log_snr = np.random.choice(training_schedule, p=training_weights / training_weights.sum(), replace=True, size=10000)\n",
    "    ax[1].hist(log_snr, density=True, color=colors[i], label=f\"{schedule.name}\", alpha=0.7, bins=20)\n",
    "    ax[1].set_xlabel(r\"$\\lambda$ (log SNR)\")\n",
    "    #ax[2].legend(labels=[s.name for s in schedules], loc=\"upper right\")\n",
    "\n",
    "    training_schedule = schedule.get_log_snr(time, training=True)\n",
    "    log_snr = np.random.uniform(np.min(training_schedule), np.max(training_schedule), size=training_schedule.shape)\n",
    "    ax[2].hist(log_snr, density=True, color=colors[i], label=f\"{schedule.name}\", alpha=0.7, bins=20)\n",
    "    ax[2].set_xlabel(r\"$\\lambda$ (log SNR)\")\n",
    "ax[0].set_ylabel(\"Effective Density\\n\"+r\"($\\boldsymbol{\\epsilon}$-loss)\")\n",
    "\n",
    "ax = axis[2]\n",
    "for i, schedule in enumerate(schedules):\n",
    "    training_schedule = schedule.get_log_snr(time[1:-1], training=True)\n",
    "    no_weight_schedule = CosineNoiseSchedule(weighting=None)\n",
    "    training_weights = keras.ops.convert_to_numpy(no_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    edm_weight_schedule = EDMNoiseSchedule()\n",
    "    edm_weights = keras.ops.convert_to_numpy(edm_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    training_weights = training_weights * edm_weights\n",
    "    log_snr = np.random.choice(training_schedule, p=training_weights / training_weights.sum(), replace=True, size=10000)\n",
    "    ax[0].hist(log_snr, density=True, color=colors[i], label=f\"{schedule.name}\", alpha=0.7, bins=20)\n",
    "    ax[0].set_xlabel(r\"$\\lambda$ (log SNR)\")\n",
    "    #ax[0].legend(labels=[s.name for s in schedules], loc=\"upper right\")\n",
    "\n",
    "    training_schedule = schedule.get_log_snr(time[1:-1], training=True)\n",
    "    sigmoid_weight_schedule = CosineNoiseSchedule(weighting='sigmoid')\n",
    "    training_weights = keras.ops.convert_to_numpy(sigmoid_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    edm_weight_schedule = EDMNoiseSchedule()\n",
    "    edm_weights = keras.ops.convert_to_numpy(edm_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    training_weights = training_weights * edm_weights\n",
    "    log_snr = np.random.choice(training_schedule, p=training_weights / training_weights.sum(), replace=True, size=10000)\n",
    "    ax[1].hist(log_snr, density=True, color=colors[i], label=f\"{schedule.name}\", alpha=0.7, bins=20)\n",
    "    ax[1].set_xlabel(r\"$\\lambda$ (log SNR)\")\n",
    "    #ax[2].legend(labels=[s.name for s in schedules], loc=\"upper right\")\n",
    "\n",
    "    training_schedule = schedule.get_log_snr(time[1:-1], training=True)\n",
    "    training_schedule = np.random.uniform(np.min(training_schedule), np.max(training_schedule), size=training_schedule.shape)\n",
    "    edm_weight_schedule = EDMNoiseSchedule()\n",
    "    edm_weights = keras.ops.convert_to_numpy(edm_weight_schedule.get_weights_for_snr(training_schedule))\n",
    "    training_weights = edm_weights\n",
    "    log_snr = np.random.choice(training_schedule, p=training_weights / training_weights.sum(), replace=True, size=10000)\n",
    "    ax[2].hist(log_snr, density=True, color=colors[i], label=f\"{schedule.name}\", alpha=0.7, bins=20)\n",
    "    ax[2].set_xlabel(r\"$\\lambda$ (log SNR)\")\n",
    "\n",
    "fig.legend(labels=['EDM', 'Cosine'], loc=\"upper right\", bbox_to_anchor=(1.11, 0.57))\n",
    "ax[0].set_ylabel(\"Effective Density\\n\"+r\"($\\boldsymbol{F}$-loss)\")\n",
    "for ax in axis.flatten():\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(12)\n",
    "plt.savefig('weightings.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "af05f0d7fccad660",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(12,3), layout='constrained') #, sharey=True)\n",
    "for i, schedule in enumerate(schedules):\n",
    "    if schedule.name == 'Flow Matching Schedule':\n",
    "        alpha_t_training, sigma_t_training = schedule.get_alpha_sigma(time)\n",
    "        alpha_t_inference, sigma_t_inference = alpha_t_training, sigma_t_training\n",
    "    else:\n",
    "        training_schedule = schedule.get_log_snr(time, training=True)\n",
    "        inference_schedule = schedule.get_log_snr(time, training=False)\n",
    "        alpha_t_training, sigma_t_training = schedule.get_alpha_sigma(training_schedule)\n",
    "        alpha_t_inference, sigma_t_inference = schedule.get_alpha_sigma(inference_schedule)\n",
    "    alpha_t_training = keras.ops.convert_to_numpy(alpha_t_training)\n",
    "    sigma_t_training = keras.ops.convert_to_numpy(sigma_t_training)\n",
    "    alpha_t_inference = keras.ops.convert_to_numpy(alpha_t_inference)\n",
    "    sigma_t_inference = keras.ops.convert_to_numpy(sigma_t_inference)\n",
    "\n",
    "    if (training_schedule != inference_schedule).all():\n",
    "        ax[0].plot(time, alpha_t_training, label=f\"{schedule.name} Training\", color=colors[i])\n",
    "        ax[0].plot(time, alpha_t_inference, label=f\"{schedule.name} Inference\", linestyle=\"--\", color=colors[i])\n",
    "        ax[1].plot(time, sigma_t_training, label=f\"{schedule.name} Training\", color=colors[i])\n",
    "        ax[1].plot(time, sigma_t_inference, label=f\"{schedule.name} Inference\", linestyle=\"--\", color=colors[i])\n",
    "        ax[2].plot(time, alpha_t_training**2+sigma_t_training**2, label=f\"{schedule.name} Training\", color=colors[i])\n",
    "        ax[2].plot(time, alpha_t_inference**2+sigma_t_inference**2, label=f\"{schedule.name} Inference\", linestyle=\"--\", color=colors[i])\n",
    "    else:\n",
    "        ax[0].plot(time, alpha_t_training, label=f\"{schedule.name} Training & Inference\", color=colors[i])\n",
    "        ax[1].plot(time, sigma_t_training, label=f\"{schedule.name} Training & Inference\", color=colors[i])\n",
    "        ax[2].plot(time, alpha_t_training**2+sigma_t_training**2, label=f\"{schedule.name} Training & Inference\", color=colors[i])\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"time\")\n",
    "ax[0].set_ylabel(r\"$\\alpha_t$\")\n",
    "ax[1].set_ylabel(r\"$\\sigma_t$\")\n",
    "ax[1].set_yscale(\"log\")\n",
    "ax[2].set_yscale(\"log\")\n",
    "ax[2].set_ylabel(r\"$\\alpha_t^2+\\sigma_t^2$\")\n",
    "fig.legend(loc=\"lower center\", ncol=3, bbox_to_anchor=(0.5, -0.35))\n",
    "plt.show()"
   ],
   "id": "37884c5f6fe8578c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# theoretical densities from Kingma Paper\n",
    "lambda_t = np.linspace(-15, 15, 100)\n",
    "edm_w = norm.pdf(lambda_t, loc=2.4, scale=2.4) * (np.exp(-lambda_t) + 1**2)\n",
    "cosine_w = sech(lambda_t / 2)\n",
    "fm_w =  np.exp(-lambda_t/2)\n",
    "\n",
    "plt.plot(lambda_t, edm_w / sum(edm_w), label='edm', color=colors[0])\n",
    "plt.plot(lambda_t, cosine_w / sum(cosine_w), label='cosine', color=colors[1])\n",
    "plt.plot(lambda_t, fm_w / sum(fm_w), label='flow matching', color=colors[2])\n",
    "plt.xlabel(r\"log SNR $\\lambda$\")\n",
    "plt.ylabel(\"Implied Weighting Function\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "e829c47f0990389f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Variance Types\n",
    "\n",
    "- Variance preserving (VP): $\\alpha_t = \\sqrt{1 - \\sigma_t^2}$. The total variance remains constant over $t$. This setting maintains a balance between signal and noise at each step.\n",
    "- Variance exploding (VE): $\\alpha_t = 1$ with $\\sigma_t$ growing large. Here the signal is constant but the noise variance increases.\n",
    "- Sub-variance preserving (sub-VP)}: $\\alpha_t = \\sqrt{1 - \\sigma_t}$.\n",
    "- Flow matching: $\\alpha_t = 1 - \\sigma_t$."
   ],
   "id": "96dc58cfce298946"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "t = np.linspace(1,0, 1000)\n",
    "\n",
    "vp_sigma_t = t\n",
    "vp_alpha_t = np.sqrt(1 - vp_sigma_t ** 2)\n",
    "\n",
    "ve_sigma_t = t\n",
    "ve_alpha_t = np.ones_like(ve_sigma_t)\n",
    "\n",
    "sub_vp_sigma_t = t\n",
    "sub_vp_alpha_t = np.sqrt(1 - sub_vp_sigma_t)\n",
    "\n",
    "fm_sigma_t = t\n",
    "fm_alpha_t = 1 - t"
   ],
   "id": "fe5116ebff5165f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = plt.figure(figsize=(5,3), layout='constrained')\n",
    "plt.plot(t, vp_sigma_t**2+vp_alpha_t**2, label='Variance Preserving')\n",
    "plt.plot(t, ve_sigma_t**2+ve_alpha_t**2, label='Variance Exploding')\n",
    "plt.plot(t, sub_vp_sigma_t**2+sub_vp_alpha_t**2, label='Sub-Variance Preserving')\n",
    "plt.plot(t, fm_sigma_t**2+fm_alpha_t**2, label='Flow Matching')\n",
    "plt.ylabel('Variance')\n",
    "plt.xlabel('time')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "6d61f9ff414c9f21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "57d098cee7ad0a8e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
