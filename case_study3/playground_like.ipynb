{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import numpy as np\n",
    "import bayesflow as bf\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from FyeldGenerator import generate_field\n",
    "import colorcet as cc\n",
    "from tqdm.notebook import tqdm\n",
    "from resnet import ResNetSummary"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_power_spectrum(alpha, scale):\n",
    "    def power_spectrum(k):\n",
    "        base = np.power(k, -alpha) * scale**2\n",
    "        return base\n",
    "\n",
    "    return power_spectrum\n",
    "\n",
    "\n",
    "def distribution(shape=(32, 32)):\n",
    "    a = np.random.normal(loc=0, scale=1., size=shape)\n",
    "    b = np.random.normal(loc=0, scale=1., size=shape)\n",
    "    return a + 1j * b"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "shape = (32, 32)\n",
    "n_examples = 5\n",
    "alphas = np.linspace(2, 5, n_examples)\n",
    "spectra = [generate_power_spectrum(alpha, 1) for alpha in alphas]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def plot_distribution(shape=shape):\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    a = rng.normal(loc=0, scale=1., size=shape)\n",
    "    b = rng.normal(loc=0, scale=1., size=shape)\n",
    "    return a + 1j * b\n",
    "fig, axs = plt.subplots(1, n_examples, figsize=(n_examples * 3, 4))\n",
    "\n",
    "for power_spectrum, alpha, ax in zip(spectra, alphas, axs):\n",
    "    \n",
    "    field = generate_field(plot_distribution, power_spectrum, shape)\n",
    "    max_magnitude = np.max(np.abs(field))\n",
    "    ax.imshow(field, cmap=cc.cm.coolwarm, vmin=-max_magnitude, vmax=max_magnitude)\n",
    "    ax.set_title(f\"$\\\\alpha={alpha:.2f}$\")\n",
    "    ax.set_axis_off()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "rng = np.random.default_rng()\n",
    "\n",
    "\n",
    "def prior():\n",
    "    log_std = rng.normal(scale=0.3)\n",
    "    alpha = rng.normal(loc=3, scale=0.5)\n",
    "    params_expanded = np.array([log_std, alpha])\n",
    "    params_expanded = np.ones(shape + (2,)) * params_expanded[None, None, :]\n",
    "    return {\n",
    "        \"log_std\": log_std,\n",
    "        \"alpha\": alpha,\n",
    "        \"params_expanded\": params_expanded\n",
    "    }\n",
    "\n",
    "\n",
    "def likelihood(log_std, alpha):\n",
    "    field = generate_field(\n",
    "        distribution, generate_power_spectrum(alpha, np.exp(log_std)), shape\n",
    "    )\n",
    "\n",
    "    return {\"field\": field[..., None] / 50.}\n",
    "\n",
    "\n",
    "simulator = bf.make_simulator([prior, likelihood])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "@bf.utils.serialization.serializable(\"custom\")\n",
    "class ResNetSubnet(bf.networks.SummaryNetwork):\n",
    "    def __init__(\n",
    "        self,\n",
    "        widths=(8, 16, 32),\n",
    "        activation=\"mish\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        layers = [keras.layers.Conv2D(width, kernel_size=3, activation=activation, padding='SAME') for width in widths]\n",
    "        self.net = bf.networks.Sequential(layers)\n",
    "\n",
    "    def build(self, x_shape, t_shape, conditions_shape):\n",
    "        self.net.build(x_shape[:-1] + (4,))\n",
    "\n",
    "    def call(self, x, t, conditions, training=False):\n",
    "        t = keras.ops.broadcast_to(t, keras.ops.shape(x)[:-1] + (1,))\n",
    "        return self.net(keras.ops.concatenate((x, t, conditions), axis=-1), training=training)\n",
    "    \n",
    "    def compute_output_shape(self, x_shape, t_shape, conditions_shape):\n",
    "        return x_shape[:-1] + (32,)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "training_data = simulator.sample(5000)\n",
    "validation_data = simulator.sample(50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .convert_dtype(\"float64\", \"float32\")\n",
    "    .rename(\"params_expanded\", \"inference_conditions\")\n",
    "    .rename(\"field\", \"inference_variables\")\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test Training"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inference_network = bf.networks.DiffusionModel(subnet=ResNetSubnet, concatenate_subnet_input=False)\n",
    "\n",
    "workflow = bf.workflows.BasicWorkflow(\n",
    "    simulator=simulator,\n",
    "    inference_network=inference_network,\n",
    "    adapter=adapter,\n",
    "    standardize=None,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "history = workflow.fit_offline(\n",
    "    data=training_data,\n",
    "    epochs=100,\n",
    "    validation_data=validation_data,\n",
    "    batch_size=16,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "workflow.approximator.inference_network.base_distribution.dims"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "inference_network is workflow.approximator.inference_network"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "validation_data = simulator.sample(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "z = keras.random.normal((1, 32, 32, 1))\n",
    "conditions = keras.ops.convert_to_tensor(validation_data[\"params_expanded\"], dtype=\"float32\")\n",
    "\n",
    "\n",
    "sample = inference_network(z, conditions=conditions, inverse=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sample = keras.ops.convert_to_numpy(sample)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.imshow(sample[0], cmap=\"seismic\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "f = bf.diagnostics.plots.loss(history)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "small_training_data = {k: v[:100] for k,v in training_data.items()}\n",
    "\n",
    "workflow.plot_custom_diagnostics(\n",
    "    test_data=test_data,\n",
    "    plot_fns={\n",
    "        \"recovery\": bf.diagnostics.recovery,\n",
    "        \"calibration\": bf.diagnostics.calibration_ecdf,\n",
    "    },\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluations"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target = \"NLE\"\n",
    "models = [\n",
    "    \"consistency_model\",\n",
    "    \"diffusion_edm_vp\",\n",
    "    \"flow_matching\",\n",
    "]\n",
    "scales = [2**n for n in range(3, 9)]\n",
    "model_configs = [\"8_16\", \"32_64_128_256\"]\n",
    "checkpoint_paths = [\n",
    "    f\"{model}/{target}/checkpoints/{scale}_shape_config_{model_configs[0] if str(scale) in model_configs[0] else model_configs[1]}.keras\"\n",
    "    for model in models\n",
    "    for scale in scales\n",
    "]\n",
    "print(checkpoint_paths)\n",
    "checkpoint_path = checkpoint_paths[7]\n",
    "print(checkpoint_path)\n",
    "current_shape = int(checkpoint_path.split(\"/\")[-1].split(\"_\")[0])\n",
    "current_config = checkpoint_path.split(\"_shape_config_\")[-1].split(\".keras\")[0]\n",
    "approximator = keras.saving.load_model(checkpoint_path)\n",
    "approximator.summary()\n",
    "approximator.inference_network.integrate_kwargs = {\n",
    "    \"method\": \"rk45\",\n",
    "    \"steps\": 500,\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rng = np.random.default_rng()\n",
    "shape = (current_shape, current_shape)\n",
    "\n",
    "def generate_power_spectrum(alpha, scale):\n",
    "    def power_spectrum(k):\n",
    "        base = np.power(k, -alpha) * scale**2\n",
    "        return base\n",
    "    return power_spectrum\n",
    "\n",
    "def distribution(shape):\n",
    "    a = np.random.normal(loc=0, scale=1., size=shape)\n",
    "    b = np.random.normal(loc=0, scale=1., size=shape)\n",
    "    return a + 1j * b\n",
    "\n",
    "def prior():\n",
    "    log_std = rng.normal(scale=0.3)\n",
    "    alpha = rng.normal(loc=3, scale=0.5)\n",
    "    params_expanded = np.array([log_std, alpha])\n",
    "    params_expanded = np.ones(shape + (2,)) * params_expanded[None, None, :]\n",
    "    return {\n",
    "        \"log_std\": log_std,\n",
    "        \"alpha\": alpha,\n",
    "        \"params_expanded\": params_expanded\n",
    "    }\n",
    "\n",
    "\n",
    "def likelihood(log_std, alpha):\n",
    "    field = generate_field(\n",
    "        distribution, generate_power_spectrum(alpha, np.exp(log_std)), shape\n",
    "    )\n",
    "\n",
    "    return {\"field\": field[..., None] / 50.}\n",
    "\n",
    "\n",
    "simulator = bf.make_simulator([prior, likelihood])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "validation_data = simulator.sample(1)\n",
    "z = keras.random.normal((1, current_shape, current_shape, 1))\n",
    "conditions = keras.ops.convert_to_tensor(validation_data[\"params_expanded\"], dtype=\"float32\")\n",
    "sample = approximator.inference_network(z, conditions=conditions, inverse=True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(validation_data[\"field\"][0, :, :, 0], cmap=cc.cm.coolwarm)\n",
    "ax[0].set_title(\"Simulated Field\")\n",
    "ax[1].imshow(keras.ops.convert_to_numpy(sample)[0, :, :, 0], cmap=cc.cm.coolwarm)\n",
    "ax[1].set_title(\"Generated Field\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 100\n",
    "def generate_classifier_data(approximator, simulator, n_samples):\n",
    "    data = simulator.sample(n_samples)\n",
    "    z = keras.random.normal((n_samples, current_shape, current_shape, 1))\n",
    "    conditions = keras.ops.convert_to_tensor(data[\"params_expanded\"], dtype=\"float32\")\n",
    "    for b in tqdm(range(n_samples // batch_size)):\n",
    "        z_batch = z[b*batch_size:(b+1)*batch_size]\n",
    "        conditions_batch = conditions[b*batch_size:(b+1)*batch_size]\n",
    "        samples_batch = approximator.inference_network(z_batch, conditions=conditions_batch, inverse=True)\n",
    "        if b == 0:\n",
    "            samples = samples_batch\n",
    "        else:\n",
    "            samples = keras.ops.concatenate([samples, samples_batch], axis=0)\n",
    "    return {\n",
    "        \"simulated\": data[\"field\"],\n",
    "        \"generated\": keras.ops.convert_to_numpy(samples),\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "classifier_kwargs = {\n",
    "        \"shape_config_32_64_128_256\": {\n",
    "            \"summary_kwargs\": {\n",
    "                \"summary_dim\": 1,\n",
    "                \"widths\": [16, 16],\n",
    "                \"use_batchnorm\": False,\n",
    "                \"dropout\": 0.0,\n",
    "            },\n",
    "        },\n",
    "        \"shape_config_8_16\": {\n",
    "            \"summary_kwargs\": {\n",
    "                \"summary_dim\": 1,\n",
    "                \"widths\": 2*(8,),\n",
    "                \"use_batchnorm\": False,\n",
    "                \"dropout\": 0.0,\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "\n",
    "def make_classifier(current_shape, current_config):\n",
    "    inputs = keras.Input(current_shape)\n",
    "    outputs = ResNetSummary(**classifier_kwargs[f\"shape_config_{current_config}\"][\"summary_kwargs\"])(inputs)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "classifier = make_classifier((current_shape, current_shape, 1), current_config)\n",
    "classifier.summary()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "traindata = generate_classifier_data(approximator, simulator, n_samples=10000)\n",
    "x = np.concatenate(\n",
    "    [traindata[\"simulated\"], traindata[\"generated\"]], axis=0\n",
    ")\n",
    "y = np.concatenate(\n",
    "    [np.ones((traindata[\"simulated\"].shape[0], 1)), np.zeros((traindata[\"generated\"].shape[0], 1))],\n",
    "    axis=0,\n",
    ")\n",
    "validation_data = generate_classifier_data(approximator, simulator, n_samples=100)\n",
    "x_val = np.concatenate(\n",
    "    [validation_data[\"simulated\"], validation_data[\"generated\"]], axis=0\n",
    ")\n",
    "y_val = np.concatenate(\n",
    "    [np.ones((validation_data[\"simulated\"].shape[0], 1)), np.zeros((validation_data[\"generated\"].shape[0], 1))],\n",
    "    axis=0,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure()\n",
    "plt.hist(traindata[\"simulated\"].flatten(), bins=100, alpha=0.5, label=\"simulated\")\n",
    "plt.hist(traindata[\"generated\"].flatten(), bins=100, alpha=0.5, label=\"generated\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "epochs = 100\n",
    "classifier.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n",
    ")\n",
    "history=classifier.fit(\n",
    "    x,\n",
    "    y,\n",
    "    epochs=epochs,\n",
    "    batch_size=16,\n",
    "    validation_data=(x_val, y_val),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val loss\")\n",
    "plt.plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
